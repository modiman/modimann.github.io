#  基于百度地图的物流管理系统

## 系统应用景

* 整个系统是为管理员设计的，并非面向司机

智能调度
=======

**任务目标**

- 理解地图数据收集与存储方案
- 理解Reactor线程模型
- 掌握通过Netty编写服务端应用
- 掌握Kafka集群的部署
- 掌握地图数据的收集方案的实现
- 理解MongoDB分片式集群
- 掌握接收消息存储到MongoDB
- 掌握压力测试的方法



## 为什么使用MongoDB

* 因为涉及到海量数据存储的问题，MongoDB集群分布更合适
* 与关系数据库相比，性能调整轻而易举。
* 非常容易扩展。
* 因为它是一个 NOSQL 数据库，它本质上是安全的，因为它不能执行 SQL 注入。
* MongoDB 支持的文档查询语言在支持动态查询方面起着至关重要的作用。
* MongoDB 不需要使用虚拟机。
* 由于它将数据存储在内部存储器中，因此可以更快地访问数据。
* 不需要将应用程序对象与数据对象相关联。
* MongoDB 也可以用作文件系统，这使得负载平衡更加容易。
* 有大量可访问的文档。



## 3、为什么选择MongoDB？

> **参考回答：**

之所以选择MongoDB是因为它天生就支持海量数据的存储，可以很好的水平扩容，虽然它不能像传统的关系型数据库那样进行多表联合查询，但对我们的业务而言单表操作足够了。

在MongoDB中，我们会存储好友关系、用户发表的动态、推荐的结果等数据。

对于MongoDB的集群而且，我们采用了它的分片式集群，这种集群方式有很好的扩展性，使用层面是直接连接到mongos服务即可完成集群的操作，非常的方便。

> 说明：如果要继续答分片式集群的话请参考课件中的知识模块。

作者：海拥
链接：https://juejin.cn/post/6994337229292175374
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 5、项目中短信是怎么发送的？如果发送失败怎么处理？

> **参考回答：**

项目中的短信是通过阿里云的短信服务发送的，之所以选择阿里云是由于它相对比较稳定，由于我们发送量并不是很大，所以我们只接入了这一个通道，如果后期量大的了话会考虑接入多个通道来确保短信服务的稳定性。

如果短信发送失败的话，我们会这样处理：

- 首先，我们的发送短信的代码是做了重试功能的，这样基本可以确保网络故障的场景下能恢复发送
- 如果重试3次后依然出错的话，应该是通道或账号出现了问题，我们会将错误记录到日志中，会有后续的日志监控进行监控，及时的发现问题并解决问题。

高并发接受用户数据 Netty Kafka 
应用背景解决车辆线路相关的问题

* 车辆线路规划
* 车辆运行时的电子围栏（线性电子围栏） 车辆进入/出离区域会发出报警信息 车辆驶出规定路线给出报警（线性围

栏）

* 车辆运行轨迹的纠偏
* 车辆运行记录的线路补偿 车辆运行中有路线缺失，给出补偿

## 功能介绍

美团单车

* 智能锁 2g/4g信号 物联卡，自带发送消息功能
* GPS+北斗双定位

### 轨迹上报

* 运动过程中每隔15s上报一次位置信息，运动多个点组成运动轨迹
* 运动过程中只是把位置上报给百度地图，结束后统一查询存储到MongoDB
* 系统中的位置上报信息是通过 函数模拟得到的，暂时没有具体的汽车上报方案，这里只考虑后台处理逻辑

### 订单管理（面向后台管理）

根据用户填表生成订单，负责这些订单的管理



接口列表

| 地址 | 接口     | 参数 | 返回值 |
| ---- | -------- | ---- | ------ |
|      | 订单列表 |      |        |
| {id} | 订单详情 | id   |        |

* 订单是用户在物流系统中下的订单，比如：寄一件衣服从上海到北京
  订单管理列表  建立订单表
  * 序号
  * 订单编号 
  * 运单编号 
  * 下单时间 
  * 订单状态 
  * 发件人姓名 
  * 发件人电话 
  * 操作
* 根据订单规划路径
  在订单详情中，通过百度地图显示出订单的路径规划

### 运单管理（面向用户）

接口列表

根据订单生成合适的路线，主要功能是查看规定路线

| 地址 | 接口     | 参数 | 返回值 |
| ---- | -------- | ---- | ------ |
|      | 运单轨迹 |      |        |
| {id} | 运单详情 | id   |        |

* 运单是物流系统为每个订单生成的运单号，一般是告知用户的，用户根据订单号查询物流状态信息
* 运单管理表
  * 序号 
  * 运单编号 
  * 运单状态 
  * 调度状态 
  * 订单编号 
  * 下单时间 
  * 发件人姓名 
  * 发件人手机 
  * 操作（查看详情）

#### 电子围栏

接口列表

```java
@RequestMapping("electronicFence")
```

| 地址                        | 接口         | 参数                | 返回值 |
| --------------------------- | ------------ | ------------------- | ------ |
| {id}                        | 更新电子围栏 | id                  |        |
|                             | 新增围栏     |                     |        |
| @DeleteMapping("{id}")      | 删除围栏     | id、electronicFence |        |
| @GetMapping("page")         | 围栏分页展示 |                     | Object |
| @GetMapping("details/{id}") | 围栏详情     | id                  | Object |

通过设置多边形的电子围栏对车辆进行监控，如果超出范围就会告警

#### 车辆管理

接口列表

| 地址         | 接口               | 参数   | 返回值         |
| ------------ | ------------------ | ------ | -------------- |
| /            | 查询车辆列表       |        |                |
| details/{id} | 根据id查询车辆数据 | 车辆id | Object车辆详情 |
| {id}         | 根据id删除车辆数据 | id     |                |
|              | 新增车辆           |        |                |

在车辆管理的界面中，如果有车辆超出了电子围栏就会有红色告警信息显示

* 车辆管理
  * 序号 
  * 车辆编号 
  * 车辆类型 
  * 车辆号码 
  * 电子围栏警告 
  * 车辆当前位置 
  * 操作（查看详情 删除 ） 
  * 新增车辆
  * 超出范围给司机打电话  发消息等



## 实现

前端使用VUE

### 电子围栏

点击新增，调用百度地图API，以鼠标点击拖动的形式绘制各种形式的围栏，围栏创建后有围栏名称，这样新增车辆后可以进行关联

```
db.createUser({
	user:"jinyun",pwd:'ddd',roles| {role:"readWrite",db:"jinyun"}|}
});
```

### 后端
MongoDB+百度地图



## 轨迹坐标数据收集与存储

轨迹收集与

1. 客户端不停地向服务端以Http请求的形式发送位置信息
2. 问题在于多用户同时发送，服务端如何处理 
3. 引入Netty,Netty接收数据但并不直接访问MongoDB存储
4. Netty将数据发送给KafKa集群
5. Kafka将数据写进MongoDB，至此服务结束



困难

1. 高并发上报地理位置的请求如何解决
2. 海量坐标数据如何存储

![image-20220810190403330](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220810190403330.png)

​	l关于上图的说明如下：

l客户端可能会有多种，可以是移动端APP、微信小程序、浏览器等。

l客户端一般会发送http请求，为了高效的处理用户请求，我们采用Netty来处理用户的请求。

* Netty是一个高性能的、异步的、基于事件驱动的网络应用框架。
* Netty服务在接收到请求后，自己并不完成数据存储的工作，而是将数据发送给Kafka消息服务器。
* 这样做的好处，Netty服务可以更快速给用户响应，自己的变得更加轻量化。
* Kafka 是一个开源消息系统，是Apache 软件基金会开发的一个开源消息系统项目。
* 该项目的目标是为处理实时数据提供一个统一、高通量、低延迟的平台。

![image-20220810190557727](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220810190557727.png)





 ## 分表

* [掘金](https://juejin.cn/post/6995504951480811550)

业务背景：随着时间的推移数据量疯狂增长

策略：按月分表

- **协议服务** 接收设备上传数据，解码，封装，发生带数据处理服务
- **数据处理** 对数据进行业务处理，添加时间字段（如分钟数据填加hour、day...）
- **存储服务** 按照数据类型对数据进行存储前业务处理，数据目标表，分表实现

### 分片集群

* 分片（sharding）是MongoDB用来将大型集合分割到不同服务器（或者说一个集群）上所采用的方法。

* 使用分片减少了每个分片需要处理的请求数，通过水平扩展，集群可以提高自己的存储容量和吞吐量。例如，如果数据库1tb的数据集，并有4个分片，然后每个分片可能仅持有256 GB的数据。如果有40个分片，那么每个切分可能只有25GB的数据。

### 优势

* MongoDB自带了一个叫做mongos的专有路由进程。mongos就是掌握统一路口的路由器，其会将客户端发来的请求准确无误的路由到集群中的一个或者一组服务器上，同时会把接收到的响应拼装起来发回到客户端
* 保证集群总是可读写。将MongoDB的分片和复制功能结合使用，在确保数据分片到多台服务器的同时，也确保了每分数据都有相应的备份，这样就可以确保有服务器坏掉时，其他的从库可以立即接替坏掉的部分继续工作。
* 集群易于扩展。当系统需要更多的空间和资源的时候，MongoDB使我们可以按需方便的扩充系统容量。

### 架构

* Mongos

提供对外应用访问，所有操作均通过mongos执行。

* Config Server

l存储集群所有节点、分片数据路由信息，默认需要配置3个Config Server节点。

* Shard

l数据分片，存储应用数据记录，一般有多个MongoDB服务节点，达到数据分片目的。

### **集群中的数据分布**

![image-20220811142152501](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142152501.png)

![image-20220811142158035](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142158035.png)

* 在一个shard server内部，MongoDB会把数据分为chunks，每个chunk代表这个shard server内部一部分数据。chunk的产生，会有以下两个用途：

  * **Splitting**：当一个chunk的大小超过配置中的chunk size（默认64M）时，MongoDB的后台进程会把这个chunk切分成更小的chunk，从而避免chunk过大的情况。

  * **Balancing**：balancer是一个后台进程，负责chunk的迁移，从而均衡各个shard server的负载，系统初始1个chunk，mongoDB会自动拆分和迁移chunks。

### 搭建集群

下面我们搭建一个分片式的集群，包括：1个Mongos服务，3个Config Server，2个Shard，每个Shard中包含3个MongoDB实例，其为副本集，保障数据的安全。具体方法参考《搭建MongoDB分片式集群.md》文档。·

![image-20220811142703138](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142703138.png)

4e918eacee07cce15f1a553f40ba01a96e3d2e15
