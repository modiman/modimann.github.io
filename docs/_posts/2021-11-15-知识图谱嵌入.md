    随着知识图谱研究的深入，研究人员发现知识图谱在各种应用中存在以下质 量问题： 

​          第一个问题是知识图谱的不完备性，即知识图谱中的关系缺失或者属性缺失， 例如人物的职业信息缺失。这个问题可能是因为构建知识图谱的数据本身就是不完备的，也可能是信息抽取算法无法识别到一些关系或者抽取到属性值。

​         第二个问题是知识图谱中存在错误的关系，如人物知识图谱中可能包含错误 的人物关系。这个问题可能是因为构建知识图谱的数据有错误，也可能是因为知 识图谱构建时采用了统计方法，而统计方法很难保证学习的知识是绝对正确的。

​     知识图谱之所以被认为是实现人工智能的一个重要研究方向，是因为知识图谱上的推理使之能够支撑人工智能的很多应用，而这也是知识图谱区别于传统关系数据模型的关键所在。

# 研究内容和关键科学问题

* 知识图谱的表示
* 基于符号的并行知识推理
* 实体关系学习方法
* 模式归纳方法



##  实体关系学习方法

### 基于表示学习

​    知识图谱表示学习旨在于将知识图谱中的实体与关系统一映射至低维连续 向量空间，以刻画它们的潜在语义特征。

模型

* TransE
* RESCAL  双线性匹配
* 。。。

### 基于图特征

基于图特征的方法借助从知识图谱中抽取出的图特征来预测两个实体间可 能存在的不同类型的边（关系）。





随着预训练+微调这一技术路线的流行，将多种模态的信息纳入知识推理的考虑范围是否比单纯考虑三元组的嵌入效果更好

**传统推理**

h + r ≈ t

考虑多种模态数据

h1 + h2 + h2 + r1 + r2 ≈ t1 + t2

1 2 3指各种模态的数据

包括 

* 三元组
* 图片
* 图文对
* 文本（嵌入）



# 知识图谱嵌入

## 损失函数

### pair-wise

如果正例三元组的打分小于负例三元组打分，给一个惩罚

![image-20211223181629347](https://github.com/modiman/modiman.github.io/tree/gh-pages/docs/_posts/imgs/image-20211223181629347.png?raw=true)

## 生成负例

# 超参数

【1】超参数的“学院派”定义： 

> 在机器学习的过程中，
> 超参= 在开始机器学习之前，就人为设置好的参数。
> 模型参数=通过训练得到的参数数据。
> 通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果



【2】怎么决定超参数？

> 1. 定义关于模型的更高层次的概念，如复杂性或学习能力。
> 2. 能直接从[标准模型](https://www.zhihu.com/search?q=标准模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"78137628"})培训过程中的数据中学习，需要预先定义。、
> 3. 可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定



【3】超参数的“通俗”定义：

> 超参数也是一种参数，它具有参数的特性，比如未知，也就是它不是一个已知常量。
> 是一种手工可配置的设置，需要为它根据已有或现有的经验，指定“正确”的值，也就是人为为它设定一个值，它不是通过系统学习得到的。



【4】超参数的一些示例：

> 1. 聚类中类的个数
> 2. 话题模型中话题的数量
> 3. 模型的学习率
> 4. [深层神经网络](https://www.zhihu.com/search?q=深层神经网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"78137628"})隐藏层数  
> 5. 树的数量或树的深度
> 6. 矩阵分解中潜在因素的数量
> 7. k[均值聚类](https://www.zhihu.com/search?q=均值聚类&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"78137628"})中的簇数

# 时空知识图谱文章   2021-12-28

## Context-Aware Temporal Knowledge Graph Embedding

### 背景   

   现有的方法将相关事实视为时不变的，忽略了相应的有效时段。实际上，许多关系是随着时间的推移而变化和涉及的，也就是说，它们只在一定的时间内有效。例如，关系实例  **配偶(“布拉德皮特”，“安吉丽娜朱莉”)**仅在时间间隔“[2014,2016]”内成立。

### 模型

​     在这项工作中，我们提出了一个时间感知的KGE模型，该模型可以衡量一个事实的两个方面:事实的可信性和时间的一致性。我们的主要思想

### 定义四元组

（h,r,t, τ）

其中τ = （τ<sub>s</sub>,τ<sub>e</sub>） 比如（2003,2006）

### 评估方法

遮罩法打乱四元组，





### 实验

数据集

**YAGO11k and Wikidata12k**

### baseline

* TransE
* HoIE
* ComplEx
* pRotatE 
* t-TransE
* HyTE

 后两个是时间知识图谱嵌入模型

## HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding



# 结构描述文章2021-2-11

## Representation Learning of Knowledge Graphs with Entity  Descriptions

### 摘要

针对问题：事实上，在大多数知识图中，通常存在对实体的简明描述，这是现有方法无法很好地利用的。在本文中，我们提出了一种新的基于实体描述的知识图RL方法。

* 提出两种编码器：连续词袋和CNN用来编码实体描述
* 在知识补全和实体分类两个任务上评估模型
* 可以处理零样本问题，即可以根据实体描述表示知识图谱中没有出现过的实体

### 介绍

写一些问题背景，即随着知识图谱规模的不断增大，由于计算效率和数据稀疏的限制，以往的知识图谱应用方法不再可行，因此提出了知识图谱表示学习。

### 相关工作

* 基于翻译的模型
* 考虑文本信息的模型-----NTN，

### 问题公式化

### 定义一：基于结构的表示

使用TransE模型得到hs和ts

### 定义二：基于描述的表示

根据头实体和尾实体的描述得到hd和td

### 方法

为了利用事实三元组和实体描述，并能够处理零样本场景，提出两种实体表示

基于结构的表示捕获三元组信息，基于描述的的表示捕获实体描述的信息。

将两种表示学习到同一连续向量空间，能量函数定义为

E=E<sub>S</sub>+E<sub>D</sub> （2）

其中

 E<sub>D</sub>= E<sub>DD</sub>+E<sub>DS</sub>+E<sub>SD</sub>  （3）

E<sub>DD</sub>=|hd+r-td|

E<sub>DS</sub>=|hd+r-ts|

E<sub>SD</sub>=|hd+r-td|

能量函数将两种类型的实体表示投射到同一个向量空间中，四种能量函数共享关系表示，从而使两种类型的表示相互促进。

本文提出了两种编码器构建基于描述的表示

### 连续词袋模型

​       从每个简短的描述中，我们可以生成一组关键字，这些关键字通常能够捕捉实体的主要思想。我们假设相似的实体应该有相似的描述，相应的有相似的关键字。那些不能通过结构信息直接检测到的关系，可以在关键词的内部联系中找到

​    首先利用TF—IDF等方法给实体描述中的词语排序，选取前n个词语作为编码器输入

**ed = x1+x2+......+xk,**（4）

![image-20220211155713070](https://github.com/modiman/modiman.github.io/blob/gh-pages/docs/_posts/imgs/image-20220211155713070.png?raw=true)

### CNN编码器

### 总体框架

![image-20220211160038645](https://github.com/modiman/modiman.github.io/blob/gh-pages/docs/_posts/imgs/image-20220211160038645.png?raw=true)

### 预处理和词表示

使用Word2Vec作为卷积层的输入

### 训练

参数集设置为θ=（X，W1，W2，E，R）

X，E，R分别是词向量，实体、关系嵌入

W1,W2是不同层的卷积核

### 初始化

W1，W2随机初始化

### 优化

随机梯度下降

### 实验

### 数据集

* FB15K，
* 针对零样本，自建立FB20K



# How Does Knowledge Graph Embedding Extrapolate
# to Unseen Data: a Semantic Evidence View 

### 摘要

现有的工作主要关注如何构建更精妙的三元组建模函数，这只能得到已观察到的三元组的概率，难以解释为什么能泛化到未知数据，也无法得出哪些是帮助KGE推理的重要因素。

因此，这篇工作有关KGE推理的两个问题

1.   KGE如何推理未知数据
2. 如何设计具有更强推理能力的KGE

### introduction

对于问题1，从（h,r,?） 到t的匹配必须获得一些语义相似度，本文从三种级别进行关联

1. **关系级别**   r和t的关系
2. **实体级别**：h和t的关系
3. **三元组级别**：（h,r,?）和t的关系

将这三种因素命名为Se-mantic Evidence (SE)，并分别使用三种相关矩阵来量化这三个因素

| 关系级别               | 实体级别         | 三元组级别           |
| ---------------------- | ---------------- | -------------------- |
| 训练集中r和t的共现次数 | 从h到t的路径连接 | （h,r,?）和t的相似度 |

![]()

![](./imgs/SE.png)

对于问题2,提出了一种新的基于KGE的图神经网络，叫做SemanticEvidence awareGraph NeuralNetwork (SE-GNN)

总的来说，本文贡献如下

* 率先探索KGE外推问题
* 提出一种新的基于GNN的KGE方法
* 在FB15k-237 和WN18RR上的拓展试验验证了模型

### ralated work

#### KGE 

分为三种模型

* 基于平移距离的模型
* 基于语义匹配的模型DistMult  ComplEx   ConvE   InteractE  
* 基于GNN的模型R-GCN  CompGCN 

#### 外推能力研究

### 3 Knowledge Graph Embedding Extrapolation
#### 3.1 Problem Definition

#### 3.2 Extrapolate with Semantic Evidences

* 关系级别

如果r 和t频繁的共同出现在三元组中，证明r可能包含预测t所需的信息，根据直觉，这可能是一种实体类型信息，比如（hi,born_in）很可能是要预测一个地名类型的实体而非电影

* 实体级别

  h和t的相关程度：如果有从h到t的间接路径，证明两者的语义信息相近。如（h,is mother,e） 和（e,is father,t)可以为预测（h,grandmother of,t）提供一些置信度

* 三元组级别

如果已知（卡梅隆，职业，导演）（卡梅隆，职业，编剧）不难推断出（卡梅隆，职业，制片人）

#### 3.3 Experiment Verification

提出三种相关矩阵去验证每个SE的证据强度

* S<sub>rel</sub>对应关系级别，它是训练集中满足（hi,r,t）的三元组个数
* S<sub>ent</sub>对应实体级别，从h到t的路径个数，限制路径长度小于等于2
* S<sub>tri</sub>对应三元组级别，量化查询（h,r）与实体h'的相似度

![](./imgs/三元组相似性.png)

![](./imgs/sim.png)

### 4 Semantic Evidence aware GNN

为了更好的利用外推知识表示中的语义证据信息，提出了一种SE-GNN模型

#### 4.1 Modeling SEs with Neighbor Pattern

1. 通过聚合所有相连的关系，得到关系级别SE表示，实体ei的表示为

![image-20220307183656194](.\imgs\image-20220307183656194.png)

W<sup>rel</sup>表示线性变换矩阵，rj为与ei连接的关系表示![image-20220307184119614](.\imgs\image-20220307184119614.png)

2. 对于实体级别的SE，实体ei的表示为![image-20220307184321602](.\imgs\image-20220307184321602.png)

![image-20220307184539145](.\imgs\image-20220307184539145.png)

3. 对于三元组级别的SE，实体ei的表示为
4. ![image-20220307184602661](.\imgs\image-20220307184602661.png)

![image-20220307184609667](.\imgs\image-20220307184609667.png)

#### 4.2 Model Architecture

![image-20220307184942435](.\imgs\image-20220307184942435.png)

![image-20220307184950422](.\imgs\image-20220307184950422.png)

![image-20220307184957402](.\imgs\image-20220307184957402.png)![image-20220307185007375](.\imgs\image-20220307185007375.png)![image-20220307185011907](.\imgs\image-20220307185011907.png)![image-20220307185019695](.\imgs\image-20220307185019695.png)![image-20220307185035267](.\imgs\image-20220307185035267.png)

### 5 Experiments

数据集

* FB15k-237
* WN18RR

评估矩阵

* MR
* MRR
* hit@1/3/10

实验结果



![image-20220307185412252](.\imgs\image-20220307185412252.png)
