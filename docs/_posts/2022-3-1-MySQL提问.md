---
typora-root-url: imgs
---

* [InnoDB并发如此高，原因竟然在这？](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651967047&idx=1&sn=b605fe50e6dd74ecad659c464a0e29ee&chksm=bd2d7b9b8a5af28d35c13e469e8e2c6a7082f00e608fd52d999fc0f7a6aec016faf2a8d748fa&scene=178&cur_album_id=1776446719614910465#rd？)



# 数据库管理系统

![image-20220710170540048](/image-20220710170540048.png)





## 查询语句执行

参考文献

* [SQL查询语句是如何被执行的](https://juejin.cn/post/7002578653385080863#heading-6)
* 作者：代码迷途
  链接：https://juejin.cn/post/7002578653385080863
  来源：稀土掘金
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

一条SQL语句执行的流程



**建立连接→查询缓存→分析器→优化器→执行器**

### 1. 建立连接

### 2. 查询缓存

```
查询缓存中存放着，之前执行过的语句及其结果，会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。
```

如果命中查询缓存，则直接将结果返回给客户端，效率非常高。

如果没有命中查询缓存（查询的语句不在查询缓存中），则执行后续操作。**执行完成后的结果会被存入查询缓存中（如果开启了query_cache）**

#### 不推荐使用查询缓存

查询缓存往往弊大于利，查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

而且，任何字符上的不同，例如空格、注释等都会导致缓存的不命中。

**对于更新压力大的数据库来说，查询缓存的命中率会非常低**。

除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

`MySQL` 中可以**将参数 `query_cache_type` 设置成 `DEMAND`**，这样对于默认的 SQL 语句都不使用查询缓存。

对于确定要使用查询缓存的语句，可以用 `SQL_CACHE` 显式指定，如：

```sql
mysql> select SQL_CACHE * from T where ID=10；
```

**默认查询缓存功能是关闭的**。通过在MySQL服务器配置文件中添加`query_cache_type=2`，重启mysql服务启用。

**MySQL 8.0 版本直接去掉了查询缓存的整块功能， 即 8.0 开始就没有了查询缓存功能**。

> 几个查询缓存有关的参数：
>
> 1. `query_cache_type=0`（或者OFF）时表示关闭；1（或者ON）时表示打开；2（或者DEMAND）表示只有 select 中明确指定SQL_CACHE才缓存。
> 2. 查询 `query_cache_type` 变量：`show variables like 'query_cache_type';`。
> 3. have_query_cache 配置参数表示这个mysql版本是否支持查询缓存。 如MySQL8.0下，`show variables like 'have_query_cache';` 返回 `NO`。
> 4. `query_cache_limit` 表示单个结果集所被允许缓存的最大值。
> 5. `query_cache_min_res_unit` 每个被缓存的结果集要占用的最小内存。
> 6. `query_cache_size` 用于查询缓存的内存大小。 `query_cache_size=0`表示不使用缓存，推荐`query_cache_type=0`关闭。
> 7. `Qcache_free_memory` 查询缓存目前剩余空间大小。
> 8. `Qcache_hits` 查询缓存的命中次数。
> 9. `Qcache_inserts` 查询缓存插入的次数。

### 3. 分析器

未命中查询缓存，或者未开启查询缓存，就会进入分析器对SQL语句做解析。

1. 分析器先做“词法分析”。一条 SQL 语句由多个字符串和空格组成，MySQL 需要识别出里面的字符串分别是什么，代表什么。

*比如，从输入的"select"这个关键字，识别出是一个查询语句。同时，还要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”等。*

1. 然后做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。

*如果语句存在语法错误，比如这个语句`elect \* from t where ID=1;`，`select`少了开头的`s`，语法分析就会收到“You have an error in your SQL syntax”的错误。*

> 语法错误会提示第一个出现错误的位置，错误信息的“use near”位置。

1. 语义分析（语义解析），将上面分析的结果（即生成的解析树），进一步分析字符串标识符的语义，检查表、列是否存在，别名是否有歧义等，通过后生成新的解析树。

> 分析器部分可以分为 **解析器和预处理器**。
>
> - 解析器处理语法和解析查询, 生成一课对应的“解析树”。解析器将查询分解成一个个Identifier（词法分析），然后确保查询中的标识都是有效的，检查其中的语法错误（语法分析），比如标识符是否有效、语句是否闭合等。
> - 预处理器进一步检查“解析树”的合法性，解决解析器无法解析的语义。比如: 检查数据表和数据列是否存在、检查名字和别名保证没有歧义等。如果通过则生成新的解析树，再提交给优化器。
>
> 即，在优化器之前还需要进行预处理器的"语义分析"。

一个 Parser 的核心分为**词法分析、语法分析和语义分析**

如果表 T 中没有字段 k，执行语句 `select * from T where k=1;`, 在 分析器 的 预处理器 分析中，就会报“不存在这个列”的错误： `“Unknown column ‘k’ in ‘where clause’”`。

### 4. 优化器

优化器是专门用于对查询进行优化的（包括所有类型的SQL语句：DDL和DML）。

优化器会根据解析树生成执行计划，同时，在表中有多个索引的时候，决定使用哪个索引；一个语句有多表关联(join)时，决定各个表的连接顺序。

优化器执行完，一个语句的执行方案就确定了下来。

### 5. 执行器（查询执行引擎）

MySQL 通过分析器知道SQL语句要做什么，通过优化器知道该怎么去做。剩下的就是进入执行器（或者执行阶段），依照“执行计划”开始执行。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，会返回没有权限的错误。

*权限错误的信息通常为：SELECT command denied to user 'xx'@'localhost' for table 'xx'*

> 权限验证不仅仅在执行器这部分会做。
>
> 在分析器之后，也就是知道了该语句要“干什么”之后，也会先做一次权限验证。叫做 precheck （在工程实现上会这么处理，同时工程实现上，也会在查询缓存命中返回结果时，做权限验证）。
>
> 而precheck是无法对运行时涉及到的表进行权限验证的，比如使用了触发器的情况。因此在执行器这里也要做一次执行时的权限验证。

> 在连接阶段，只是“获得权限信息”； 真正开始查询动作时，才判断“有没有操作这个表的权限”。
>
> 执行器阶段的权限验证，主要是判断一些关联操作，比如更新一行时，由触发器会再更新别的表的情况。

> 实际的执行和调用路径，执行器阶段都在mysql_parse函数里面调用的。

权限OK，就会打开表继续执行。

打开表的时候，执行器就会根据表的引擎定义，使用这个引擎提供的接口。

比如，这个查询语句的例子，T 表的 ID 字段没有索引，则执行流程是这样的：

1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

这样，一条SELECT语句完整的执行逻辑就结束了。

对于有索引的表，（如果能够命中索引）执行的逻辑基本相同，【开启了索引下推，默认都是开启，并且没必要关闭】。执行的逻辑为：第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，由引擎提供这些接口。

无索引时，执行器循环调用引擎接口，先调用“第一行”并过滤数据，之后循环调用“下一行”并过滤数据，一直到最后一行。过滤是在Server层完成的。

有索引时，执行器调用“满足条件的第一行”的引擎接口，之后调用“满足条件的下一行”的接口。有索引时在引擎层完成数据的过滤。

索引下推（`index condition pushdown` —— `ICP`）就是把数据筛选交由存储引擎层处理，减少Server层的负担，提高查询效率。

> 索引下推(`Index Condition Pushdown`) ICP 是 Mysql5.6 之后新增的功能，主要的核心点就在于把数据筛选的过程放在了存储引擎层去处理，而不是像之前一样放到Server层去做过滤。
>
> 在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 。
>
> 索引条件下推优化是非常好的查询优化策略，把本来由Server层做的索引条件检查下推给存储引擎层来做，可以减少存储引擎查询基础表的次数、降低回表，也可以减少MySQL服务器从存储引擎接收数据的次数、减少server层的过滤处理。

> MySQL 慢查询日志中有 rows_examined 的字段，表示这个语句执行过程中“调用”存储引擎取数据行的次数。
>
> rows_examined 会被笼统的看做语句执行过程扫描了多少行。但有时，执行器调用一次，引擎内部可能扫描了多行，因此**引擎扫描行数跟 rows_examined 并不完全相同**。但可以看做：**执行器获取到的数据行数**。

## 更新语句执行

* Mysql实战45讲 

更新与查询的大体流程相似，但引入了两个重要的日志模块**redo log（重做日志）和 binlog（归档日志）。**

### redo log（InnoDB引擎特有）

更新数据的两种方式：

> 1. 即来即更新
> 2. 汇总一段时间的数据统一更新

由于读写磁盘的IO成本太高，一般使用后者进行数据库更新操作。这也叫做WAL技术，全称Writing-Ahead Loging.关键点为先写日志，再写磁盘。

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里 面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作 记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

redo log的大小是固定的，从头开始写，写到末尾又从头开始循环。

![image-20220712170308529](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220712170308529.png)

* write pos是当前记录的位置
* checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文 件。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为**crash-safe**。 要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或 写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。

### binlog

Server层自己的日志，称为binlog（归档日志）。

为什么会有两份日志呢？ 

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有 crash-safe的能力，binlog日志只能用于归档。

而InnoDB是另一个公司以插件形式引入MySQL 的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是 redo log来实现crash-safe能力。

 **这两种日志有以下三点不同**。 

> 1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 
> 2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的 是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 
> 3.  redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件 写到一定大小后会切换到下一个，并不会覆盖以前的日志。 

有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语 句时的内部流程。 

> 1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一 行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然 后再返回。 
> 2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行 数据，再调用引擎接口写入这行新数据。 
> 3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处 于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 4. 
> 4. 执行器生成这个操作的binlog，并把binlog写入磁盘。 5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更 新完成。 这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表 示是在执行器中执行的

![image-20220712170859812](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220712170859812.png)

### 两阶段提交

**redo log的写入拆成了两个步骤：prepare和 commit，这就是"两阶段提交"**

​	为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得 从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承 诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期 做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数 据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备 份恢复到临时库； 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时 刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢 复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法 来进行解释。 由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语 句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？ 1. 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异 常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回 来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份 日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这 个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。 2. 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以 后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日 志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的 状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再 多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用binlog来 实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保 持逻辑上的一致。 小结 今天，我介绍了MySQL里面最重要的两个日志，即物理日志redo log和逻辑日志binlog。 redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候， 表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证 MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建 议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 我还跟你介绍了与MySQL日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑 一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。 文章的最后，我给你留一个思考题吧。前面我说到定期全量备份的周期“取决于系统重要性，有 的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或 者说，它影响了这个数据库系统的哪个指标？ 你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾给出我的答案。 感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读

## 存储引擎的作用

* 存储引擎用于具体的组织、管理物理文件，包括二进制日志（binlog属于Server管理的文件，但也提供了存储引擎使用的接口）、数据文件、错误日志、慢查询日志、全日志、redo/undo 日志等。
* 除此之外，还有引擎程序运行的实例，包括功能结构、内存中数据的管理等（这就是具体引擎自己的功能实现了）。
* 不同的存储引擎有着不同的文件组织、管理的方式。
* 同时存储引擎提供Server层可以访问、存取数据的接口，用于 server 层的执行引擎获取或存入数据。

| 数据库引擎        | 特点                                                         |
| ----------------- | ------------------------------------------------------------ |
| ISAM              | ISAM是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到数据库被查询的次数要远大于更新的次数。因此，ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要**不足之处**在于，它**不支持事务处理**，也**不能够容错**：如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。 |
| MYISAM            | MYISAM是MYSQL的ISAM扩展格式和**缺省的数据库引擎**。除了**提供**ISAM里所没有的**索引和字段管理的**功能，MYISAM还使用一种**表格锁定的机制**，来**优化多个并发的读写操作**。其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。MYISAM还有一些有用的扩展，例如用来修复数据库文件的MYISAMCHK工具和用来恢复浪费空间的MYISAMPACK工具。　<br/>MYISAM强调了快速读取操作，这可能就是为什么MYSQL受到了WEB开发如此青睐的主要原因：在WEB开发中你所进行的大量数据操作都是读取操作。所以，大多数虚拟主机提供商和INTERNET平台提供商只允许使用MYISAM格式。 |
| HEAP              | HEAP**允许只驻留在内存里的临时表格**。驻留在内存里让HEAP要比ISAM和MYISAM都快，但是它所**管理的数据是不稳定的**，而且如果在关机之前没有进行保存，那么所有的数据都会丢失。在数据行被删除的时候，HEAP也不会浪费大量的空间。HEAP表格在你需要使用SELECT表达式来选择和操控数据的时候非常有用。要记住，在用完表格之后就删除表格。 |
| INNODB和BERKLEYDB | INNODB和BERKLEYDB（BDB）数据库引擎都是造就MYSQL灵活性的技术的直接产品，这项技术就是MYSQL++ API。在使用MYSQL的时候，你所面对的每一个挑战几乎都源于ISAM和MYISAM数据库引擎不支持事务处理也不支持外来键。尽管要比ISAM和MYISAM引擎慢很多，但是INNODB和BDB包括了**对事务处理和外来键的支持**，这两点都是前两个引擎所没有的。如前所述，如果你的设计需要这些特性中的一者或者两者，那你就要被迫使用后两个引擎中的一个了。 |



### MyISAM与InnoDB的比较

|                      | MyISAM                                                       | InnoDB                                                       |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 存储结构             | Myisam 创建表后生成的文件有三个，分别为：  frm:创建表的语句 MYD:表里面的数据文件（myisam data） MYI:表里面的索引文件（myisam index） | Innodb 创建表后生成的文件有两个，分别为： frm:创建表的语句 idb:表里面的数据+索引文件 |
| 存储空间             | MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。 | 需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 |
| 可移植性、备份及恢复 | 数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作； MyISAM遇到错误，必须完整扫描后才能重建索引，或修正未写入硬盘的错误； MyISAM的修复时间，与数据量的多少成正比。 | 在数据量很大的时候就相对痛苦； InnoDB可借由事务记录档（Transaction Log）来恢复程序崩溃（crash），或非预期结束所造成的数据错误； InnoDB的修复时间，大略都是固定的。 |
| 索引                 | `非聚集索引`，MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 | `聚集索引`，聚集索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 |
| 事务支持             | 强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是`不提供事务支持`。 | `提供事务支持`，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 |
| 主键自增长           | 可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 | InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 |
| 锁的粒度             | `只支持表级锁`，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 | `支持行级锁`。行锁大幅度提高了多用户并发操作的性能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 |
| 全文索引             | 支持 FULLTEXT类型的全文索引                                  | 不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 |
| 表主键               | 允许没有任何索引和主键的表存在，索引都是保存行的地址。       | 如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 |
| 存储表的具体行数     | 保存有表的总行数，如果select count() from table;会直接取出出该值。 | 没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 |
| 外键                 | 不支持                                                       | 支持，对一个包含外键的 InnoDB 表转为 MYISAM 会失败。         |


作者：YuShiwen
链接：https://juejin.cn/post/7084093849198395399
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

# 索引



### 1.MySQL索引，问了20分钟

* [我以为我对Mysql*索引*很了解，直到我遇到了阿里的面试官](https://zhuanlan.zhihu.com/p/73204847)





​       一张表可以建立**任意多个索引，**每个索引可以是**任意多个字段**的组合。索引**可能会提高查询速度**（如果查询时使用了索引），但**一定会减慢写入速度**，因为每次写入时都需要更新索引，所以索引只应该加在经常需要搜索的列上，不要加在写多读少的列上。

#### 单列索引与复合索引

只包含一个字段的索引叫做**单列索引**，包含两个或以上字段的索引叫做**复合索引**（或组合索引）。

建立复合索引时，字段的顺序极其重要。

下面这个SQL语句在 列X，列Y，列Z 上建立了一个复合索引。

```mysql
CREATE INDEX 索引名 ON 表名(列名X, 列名Y, 列名Z);
```

### 索引建立的动机

* 数据库系统基础教程 p223

当关系变得很大时，通过扫描所有元组找到匹配给定查询条件的元组的代价太高

### 索引的声明

```mysql
create index yearindex on Movie(year)
```

这样SQL查询处理器在处理制定年份的查询时，仅仅对年份为指定值的Movies的元组进行测试，从而使获得查询结果的时间大大缩短



### 聚簇|非聚簇索引

* 找到了索引就找到了需要的数据，那么这个索引就是聚簇索引，所以主键就是聚簇索引，修改聚簇索引其实就是修改主键。
* 索引的存储和数据的存储是分离的，也就是说找到了索引但没找到数据，需要根据索引上的值(主键)再次回表查询,非聚簇索引也叫做[辅助索引](https://www.zhihu.com/search?q=辅助索引&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"142139541"})。

```
clustered index（MySQL官方对聚簇索引的解释）The InnoDB term for a primary key index. InnoDB table storage is organized based on the values of the primary key columns, to speed up queries and sorts involving the primary key columns. For best performance, choose the primary key columns carefully based on the most performance-critical queries. Because modifying the columns of the clustered index is an expensive operation, choose primary columns that are rarely or never updated.

作者：Java路人甲
链接：https://zhuanlan.zhihu.com/p/142139541
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
```



## B+树

* 作者：孤独烟
* 链接：https://zhuanlan.zhihu.com/p/107228878
* 来源：知乎
* 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

#### B+树的两个明显特点

1. 数据只出现在叶子节点
2. 所有叶子节点增加了一个链指针

#### 针对上面的B+树和B树的特点，我们做一个总结

**(1)**B树的树内存储数据，因此查询单条数据的时候，B树的查询效率不固定，最好的情况是O(1)。我们可以认为在做单一数据查询的时候，使用B树平均性能更好。但是，由于B树中各节点之间没有指针相邻，因此B树不适合做一些数据遍历操作。

**(2)**B+树的数据只出现在叶子节点上，因此在查询单条数据的时候，查询速度非常稳定。因此，在做单一数据的查询上，其平均性能并不如B树。但是，B+树的叶子节点上有指针进行相连，因此在做数据遍历的时候，只需要对叶子节点进行遍历即可，这个特性使得B+树非常适合做[范围查询](https://www.zhihu.com/search?q=范围查询&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"107228878"})。

​         因此，我们可以做一个推论:没准是Mysql中数据遍历操作比较多，所以用B+树作为索引结构。而Mongodb是做单一查询比较多，数据遍历操作比较少，所以用B树作为索引结构。

***那么为什么Mysql做数据遍历操作多？而Mongodb做数据遍历操作少呢？***

 因为Mysql是关系型数据库，而Mongodb是非关系型数据。









#### B+ Tree索引和Hash索引区别

* [哈希索引](https://www.zhihu.com/search?q=哈希索引&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"73204847"})适合等值查询，但是无法进行范围查询 
* 哈希索引没办法利用索引完成排序

* 哈希索引不支持多列联合索引的最左匹配规则
* 如果有大量重复键值得情况下，哈希索引的效率会很低，因为存在哈希碰撞问题

#### 最左前缀匹配

面试官：那你知道最左前缀匹配吗？

我：（我突然想起来原来面试官是想问这个，怪自己刚刚为什么就没想到这个呢。）哦哦哦。您刚刚问的是这个意思啊，在创建多列索引时，我们根据业务需求，where子句中使用最频繁的一列放在最左边，因为MySQL索引查询会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。所以当我们创建一个联合索引的时候，如(key1,key2,key3)，相当于创建了（key1）、(key1,key2)和(key1,key2,key3)三个索引，这就是最左匹配原则。





我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。



作者：陈大侠
链接：https://zhuanlan.zhihu.com/p/23624390
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



#### 数据库系统基础教程

* 关系中属性A上的索引是一种数据结构，他能提高在属性A上查找具有某个特定值的元祖的效率
* 

## 索引的选择

* 索引的选择需要做一个开销上的分析
* 索引的选择是数据库设计成败的一个重要因素

### 简单代价模型

​       为了检查元组，需要将包含它的整个磁盘页调入主存，检查一个磁盘页上的所有元组所花费的时间通常和检查一个元组所花费的时间几乎没什么差别。

### 一些有用的索引

通常关系上最有用的索引是其键上的索引，原因有两个”：

1. 在查询中为主键指定值是比较普遍的，因此，键上的索引通常会被频繁的使用
2. 因为键值是唯一的，故与给定键值匹配的元组最多只有一个，因此索引返回的要么是这个元组的位置，要么什么也不返回。也就是说，为了取得这个元组，最多只有一个磁盘页需要被读入到磁盘页



### 计算最佳索引

## 常见引擎的索引

### 参考文章

* [mysql 数据库引擎 ](https://www.cnblogs.com/0201zcr/p/5296843.html)

### MyIASM引擎的索引结构

　MyISAM引擎的索引结构为**B+Tree**，其中B+Tree的**数据域存储的内容为实际数据的地址**，也就是说它的索引和实际的数据是分开的，只不过是用索引指向了实际的数据，这种索引就是所谓的**非聚集索引**。如下图所示：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320204001959-1320607047.png)

　　这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320204143084-1395279894.png)

　　同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。

### Innodb引擎的索引结构

　　与MyISAM引擎的索引结构同样也是B+Tree，但是Innodb的索引文件本身就是数据文件，即**B+Tree的数据域存储的就是实际的数据**，这种索引就是**聚集索引**。这个索引的key就是数据表的主键，因此InnoDB表数据文件本身就是主索引。

　　并且和MyISAM不同，InnoDB的**辅助索引数据域存储的也是相应记录主键的值**而不是地址，所以当以辅助索引查找时，会先根据辅助索引找到主键，再根据主键索引找到实际的数据。所以Innodb不建议使用过长的主键，否则会使辅助索引变得过大。建议使用自增的字段作为主键，这样B+Tree的每一个结点都会被顺序的填满，而不会频繁的分裂调整，会有效的提升插入数据的效率。

两者区别：

　　第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320205622193-1852020506.jpg)

　　上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。

 

 

　　第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320205632724-1223368895.jpg)

　　这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。


　　了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调(可能是指“非递增”的意思)的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调(可能是指“非递增”的意思)的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。

### 2.主键的优点与缺点

1.自增主键，在mysql中应用最广泛。

**优点：**
    1>需要很小的数据存储空间，仅仅需要4 byte。（bigint类型，是8 byte）

　　　2>insert和update操作时使用INT的性能比UUID好，所以使用int将会提高应用程序的性能。

​    3>index和Join操作，int的性能最好。

​    4>容易记忆。

**缺点：**
    1>如果经常有合并表的操作，就可能会出现主键重复的情况。

​    2>使用int数据范围有限制。如果存在大量的数据，可能会超出int的取值范围。

​    3>很难处理分布式存储的数据表。

2。UUID

**优点：**
    1>能够保证独立性，程序可以在不同的数据库间迁移，效果不受影响。
    2>保证生成的ID不仅是表独立的，而且是库独立的，这点在你想切分数据库的时候尤为重要。
**缺点：**
    1>比较占地方，和INT类型相比，存储一个UUID要花费更多的空间。
    2>使用UUID后，URL显得冗长，不够友好。

​    3>没有内置的函数获取最新产生的UUID主键。

​    4>很难记忆。Join操作性能比int要低。

​    5>UUID做主键将会添加到表上的其他索引中，因此会降低性能。

# 范式

### 参考文献

作者：刘慰
链接：https://zhuanlan.zhihu.com/p/20028672
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### 第一范式

**符合1NF的关系中的每个属性都不可再分**(所有关系型数据库的最低要求)

反例：![](https://pic3.zhimg.com/89507a1682f28fd2dde066cf94d77b4a_b.jpg)

缺陷：但是仅仅符合1NF的设计，仍然会存在[数据冗余](https://www.zhihu.com/search?q=数据冗余&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"20028672"})过大，插入异常，删除异常，修改异常的问题，例如对于**表3**中的设计：

![](https://pic3.zhimg.com/dfdc86b0e2babe1f8da03d8e2b17ad06_b.jpg)



# 事务

**参考文章**

* [数据库内核杂谈](https://www.infoq.cn/article/teJA7X43BO2alp6rLCWk)

事务的定义是：**一个事务是一组对数据库中数据操作的集合**。无论集合中有多少操作，对于用户来说，只是对数据库状态的一个原子改变。

```
单从概念定义来理解，可能有些晦涩难懂，我们举个例子来讲解：数据库中有两个用户的银行账户 A:100 元; B:200 元。假设事务是 A 转账 50 元到 B，可以理解为这个事务由两个操作组成：1) A-= 50; 2) B+=50。对于用户来说，数据库对于这个事务只有两个状态：执行事务前的初始状态，即 A:100 元; B:200 元，以及执行事务后的转账成功状态：A:50 元;B:250 元，不会有中间状态，比如钱从 A 已经扣除，却还没转到 B 上:A:50 元; B:200 元。
```

## 四大特性

### 原子性(atomicity)

```
一个事务的所有操作要么全部执行，要么一个都不执行。如果在执行事务的过程中，因为任何原因导致事务失败，已经执行的操作都要被回滚(rollback)。这种“all-or-none"的属性就是所谓的事务的原子性(atomicity)。
```

### 一致性(consistency)

```
假定数据库的初始状态是稳定的，或者说对用户来说是一致的。由于事务执行的原子性，即执行失败就回滚到执行前的状态，执行成功就变成一个新的稳定状态。因此，事务的执行会保持数据库状态的一致性(consistency)。
```

### 隔离性(isolation)

```
数据库系统是多用户系统。多个用户可能在同一时间执行不同的事务，称为并发。如果想要做到事务的原子性，那么数据库就必须做到并发的事务互不影响。从事务的角度出发，在执行它本身的过程中，不会感知到其他事务的存在。从数据库的角度出发，即使同一时间有多个事务并发，从微观尺度上看，它们之间也有先来后到，必须等一个事务完成后，另一个事务才开始。这种并发事务之间的不感知就是所谓的事务隔离性(isolation)。
```

### 持久性(durability)

```
当一个事务被认定执行成功后，即代表这个事务的操作被数据库持久化。因此，即使数据库在此时奔溃了，比如进程被杀死了，甚至是服务器断电了，这个事务的操作依然有效，这就是事务的另一个属性，持久性(durability)。
```

## 隔离级别(Isolation Level)

如何实现隔离性？最简单的办法：给数据库加一个全局操作锁，同一时间只允许一个用户操作

**缺点**：限制了并发性。

**解决办法**：根据对隔离性的需求，设置多个隔离级别，越严格的隔离级别越接近全局锁，反之，越宽松则越有利于高并发

为了方便描述，首先定义一个简单的事务模型

| 数据单元 | 数据操作     | 事务操作           |
| -------- | ------------ | ------------------ |
| A        | read(A)      | begin(开启事务)    |
| A        | write(A,val) | commit(提交事务)   |
| A        |              | rollback(回滚事务) |

接下来介绍隔离级别

### 1. read uncommitted（读未提交）

读未提交就是在一个事务中，允许读取其他事务未提交的数据。下图示例很清晰地诠释了读未提交：	

![img](https://static001.infoq.cn/resource/image/fe/04/fe87384762bd9c1ade107aeec3f3f304.png)

在事务 T1 中，读取 A 得到结果是 5，是因为事务 T2 修改了 A 的值，虽然当时 T2 还未提交，甚至最后 T2 回滚了。读未提交导致的问题就是 dirty read(脏读)。

```
脏读的定义就是，一个事务读取了另一个事务还未提交的修改。虽然可能大多数情况下，我们都会认为脏读产生了不正确的结果。但是，抛开业务谈正确性都是耍流氓。或许，某些用户的某些业务，为了支持更大地并发，允许脏读的出现。因为，对于读未提交，完全不需要对操作进行加锁，自然并发性更高。
```

### 2. read committed(读提交)

为了避免脏读，引入第二层隔离级别：读提交。读提交就是指在一个事务中，只能够读取到其他事务已经提交的数据。

![img](https://static001.infoq.cn/resource/image/f9/0a/f971ba38573fc98794403f02162e930a.png)

```
在读提交的隔离级别下，再回看上面的例子，T1 中读取 A 的值就应该还是 10，因为当时 T2 还没有提交。沿着上面的例子，接着往下看，如果最后 T2 提交了事务，而 T1 在之后又读取了一次 A，这时候的值就变为 5 了。
```

**新的问题**

在 T1 事务中，先后读取了两次 A，两次的值不一样了。回顾最早提及的事务的隔离性，两次读取同一数据的值不一样，其实违反了隔离性。因为隔离性定义了一个事务不需要感知其他事务的存在，但显然，由于值不同，说明在这个过程中另一个事务提交了数据。这类问题就被定义为 **nonrepeatable read(不可重复度读)**：在一个事务过程中，可能出现多次读取同一数据但得到的值不同的现象。

### 3. repeatable read(可重复读)

为了避免不可重复读，引入第三层隔离级别：可重复读。

```
可重复读指的是在一个事务中，只能读取已经提交的数据，且可以重复查询这些数据，并且，在重复查询之间，不允许其他事务对这些数据进行写操作。虽然我们还没讲到实现，但不难想象，对读数据加读锁锁就能实现。
```

**新的问题**

```sql
T1:
BEGIN;
SELECT * FROM students WHERE class_id = 1;  // (1)
... 
SELECT * FROM students WHERE class_id = 1;  // (2)
...
COMMIT;

```

```sql
T2:
BEGIN;
INSERT INTO students (1 /* class_id */, ...);
COMMIT; 

```

T2 事务并没有修改现有数据，而是新增了一条新数据，恰巧 class_id = 1。如果这条插入介于(1)和(2)之间，(2)的结果会改变吗？答案是，会的。语句(2)会比(1)多显示一条记录，即 T2 插入的。这个问题被称为 phantom read(幻读)，

```
幻读指的是，在一个事务中，当查询了一组数据后，再次发起相同查询，却发现满足条件的数据被另一个提交的事务改变了。
```

### 4. serializable(可有序化)

如何才能避免幻读呢？数据库系统只能推出最保守的隔离机制，serializable(可有序化)，即所有的事务必须按照一定顺序执行，直接避免了不同事务并发带来的各种问题。

### 总结-四种级别

1. **读未提交：在一个事务中，允许读取其他事务未提交的数据。**
2. **读提交：在一个事务中，只能够读取到其他事务已经提交的数据。**
3. **可重复读：在一个事务中，只能读取已经提交的数据，且可以重复查询这些数据，并且，在重复查询之间，不允许其他事务对这些数据进行写操作。**
4. **可有序化：所有的事务必须按照一定顺序执行。**

依次解决的三个问题

1. **脏读：一个事务读取了另一个事务还未提交的修改**
2. **不可重复度：在一个事务过程中，可能出现多次读取同一数据但得到不同值的现象。**
3. **幻读：在一个事务中，当查询了一组数据后，再次发起相同查询，却发现满足条件的数据被另一个提交的事务改变了。**

下方列出了一张表格，更直观地展现它们之间的关系。

| 隔离级别 |   脏读   | 不可重复度 |   幻读   |
| :------: | :------: | :--------: | :------: |
| 读未提交 | 可能出现 |  可能出现  | 可能出现 |
|  读提交  |   不能   |  可能出现  | 可能出现 |
| 可重复读 |   不能   |    不能    | 可能出现 |
| 可有序化 |   不能   |    不能    |   不能   |

## 隔离实现机制

### 加锁实现机制(Lock-based protocols)

实现隔离性最简单的方法是对全局数据加锁，但这样性能大大降低。

1. 降低锁的粒度

可以想办法把锁的粒度变细，即**仅对要读写的数据加锁**而非全局锁。通过加锁来确保在同一时间，只有获得锁的事务可以对数据进行处理。

2. 定义不同类型的锁

并不是所有的事务对数据都是写操作，如果两个事务同时对某一数据进行读操作，它们之间并不需要互斥。因此，我们可以通过定义不同类型的锁，以及它们之间的兼容程度来获得更细粒度的控制。

**共享锁(share-mode lock; S-lock)**：

(share-mode lock; S-lock)，即当事务获得了某个数据的共享锁，它仅能对该数据进行读操作，但不能写，共享锁有时候也被称为读锁。

**独占锁(exclusive-mode lock; X-lock)**

当事务获得了某个数据的独占锁，它可以对数据进行读和写操作，独占锁也常被叫做写锁。

共享锁和独占锁的兼容模式如下：

|        | S-lock | X-lock |
| :----: | :----: | ------ |
| S-lock |  兼容  | 不兼容 |
| X-lock | 不兼容 | 不兼容 |

仅 S-lock 之间互相兼容，只有当多个事务同时持有共享锁时才能同时对数据进行读操作。

**新的问题**：是么时候加锁？是么时候释放锁？

**案例分析**

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>Unlock(B);<br/>X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(A). | T2:<br/>S-lock(A);<br/>Read(A);<br/>Unlock(A);<br/>S-lock(B);<br/>Read(B);<br/>Unlock(B);<br/>Display(A+B) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

两个事务对账号 A 和 B 进行操作(假设 A 初始值是 100；B 是 200)，事务 T1 用了 X-lock，因为需要对数据进行修改， 而 T2 仅需要使用 S-lock，因为只是读取数据。乍看之下，好像没有问题。无论是 T1 先执行，还是 T2 先执行，T2 中 display(A+B)都会是 300。但是，如果 T1 和 T2 的执行顺序如下：

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>Unlock(B);<br/> |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              | T2:<br/>S-lock(A);<br/>Read(A);<br/>Unlock(A);<br/>S-lock(B);<br/>Read(B);<br/>Unlock(B);<br/>Display(A+B) |
| X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(A). |                                                              |

这时候，T2 中的 display(A+B)的值就是 250，这是错误的数据。问题出在哪呢？T1 中释放对 B 的 X-lock 过早，使得 T2 获得了一个不正确的数值。既然原因是释放过早，那能不能通过延迟释放锁来解决这个问题。我们把 T1 和 T2 分别改写为 T3 和 T4(唯一的区别就是延缓了锁的释放到最后)，如下 表所示

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(B);<br/>Unlock(A). | T2:<br/>S-lock(A);<br/>Read(A);<br/>S-lock(B);<br/>Read(B);<br/>Display(A+B)<br/>Unlock(A);<br/>Unlock(B);<br/> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

T3 和 T4 分别获取了对 B 和对 A 的锁并且相互请求对 A 和对 B 的锁。相信大家都看出来了，这导致了死锁(dead lock)。这里就不具体介绍死锁的检查和破坏机制了(详情参见操作系统课)，你只需要知道，数据库系统是可以发现死锁的。解决方法也简单，选择其中一个参与的事务，回滚并放弃执行(如果一个不行，就两个)。相对于错误的数据，死锁显然是我们更愿意接受的，所谓两害取其轻

我们引入了第一个加锁实现：**两阶段加锁机制(Two-phase locking protocol)**。它要求事务在加锁的过程中遵循下面两步：



1）获取锁阶段(growing phase)：在这个过程中，事务只能不断地获取新的锁，但不能释放锁。

2）释放锁阶段(shrinking phase)：在这个过程中，事务只能逐渐释放锁，并且无权再获取新的锁。

**重要的事情说三遍：千万不要和两阶段提交(Two-phase commit (2PC))搞混；千万不要和两阶段提交搞混；千万不要和两阶段提交搞混。**两阶段提交是针对分布式事务的概念，我会在以后的文章中详细讲。

为了避免连锁回滚，我们可以引入两阶段提交的升级版：**严格的两阶段加锁(strict two-phase locking protocol)**。**除了需要遵循加锁和释放锁的两阶段外，它还规定，对于独占锁(X-lock)必须等到事务结束时才释放。**这个规定避免了其他事务对未提交的数据进行读写操作，因此也避免了连锁回滚。另一个更严格的升级本叫做**更严格的两阶段加锁(rigorous two-phase locking protocol)，**规定了所有获得的锁都得等到事务结束时才释放。

以上就是关于加锁的实现，如果我们总结一下，主要是介绍了这些内容：

1）引入共享锁(S-lock)和独占锁(X-lock)来获得对数据细粒度的控制；

2）引入两阶段加锁(Two-phase locking protocol)来保证数据的正确性；

3）两阶段加锁不能避免死锁，依然需要数据库系统来检查并破坏死锁，破坏死锁可以通过回滚相关的事务来进行；

4）两阶段加锁的两个升级版本：(更)严格的两阶段加锁(rigorous/strict two-phase locking)通过规定把释放锁等到事务结束来避免连锁回滚(cascading rollback)。



### 时间戳实现机制

使用时间戳记录事务开始的时间，根据时间戳为事务排序确定执行顺序。

为了避免两个时间戳一样可以使用计数的方法表示时间戳。

#### 实现

引入两个概念

1）W-timestamp(A): 记录对于数据 A，最近一次被某个事务修改的时间戳。

2）R-timestamp(A): 记录对于数据 A，最近一次被某个事务读取的时间戳。

一旦有一个更新的事务成功地对数据进行读取，相对应的读写时间戳就会被更新。

**对于事务 Ti 要读取数据 A read(A):**

1. 如果 TS(Ti) < W-timestamp(A)，说明 A 被一个 TS 比 Ti 更大的事务改写过，但 Ti 只能读取比自身 TS 小的数据。因此 Ti 的读取请求会被拒绝，Ti 会被回滚。
2. 如果 TS(Ti) > W-timestamp(A)，说明 A 最近一次被修改小于 TS(Ti)，因此读取成功，并且，R-timestamp(A)被改写为 TS(Ti)。

**对于事务 Ti 要修改数据 A write(A):**

1. 如果 TS(Ti) < R-timestamp(A)，说明 A 已经被一个更大 TS 的事务读取了，Ti 对 A 的修改就没有意义了，因此 Ti 的修改请求会被拒绝，Ti 会被回滚。
2. 如果 TS(Ti) < W-timestamp(A)，说明 A 已经被一个更大 TS 的事务修改了，Ti 对 A 的修改也没有意义了，因此 Ti 的修改请求会被拒绝，Ti 会被回滚。
3. 其他情况下，Ti 的修改会被接受，同时 W-timestamp(A)会被改写为 TS(Ti)。

一旦一个事务因为任何原因被回滚，再次重新执行时，会被系统分配一个新的 TS。



通过上述规则，系统就可以保证对于任意 Ti 和 Tj，如果 TS(Ti)<TS(Tj)，Ti 比 Tj 先运行完。我们通过一个示例来看时间戳是如何运行的。



假定下面两个事务 T1 和 T2，并且 TS(T1) < TS(T2)。

| T1:<br/>Read(B);<br/>Read(A);<br/>Display(A+B).<br/> | T2:<br/>Read(B)<br/>B= B- 50;<br/>Write(B);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Display(A+B). |
| ---------------------------------------------------- | ------------------------------------------------------------ |





## 多版本并发控制 (MVCC)

 Multi-Version Concurrency Control

```
为什么多版本并发控制更受欢迎呢？因为锁和时间戳机制都是通过阻塞或者回滚冲突的事务来确保事务的有序性。比如，一个读操作可能被迫回滚，因为它要读取的数据已经被另一个更新的事务修改了。但是，如果我们把每个数据的所有历史版本都记录下来，就可以避免上述这种情况发生。这也正是多版本控制的由来：对于每个数据 Q，每次写操作 write(Q)都会给 Q 建立一个新版本；而对于读操作 read(Q)，会根据事务的先后关系选择一个正确的版本去读取，来保证事务的有序性。多版本控制能够很好地解决这类读写冲突，尤其是长时间的读操作饿死写操作问题。
```

### 多版本时间戳

### 快照隔离(Snapshot Isolation)

```
快照隔离可以看作是对每一个事务，分配了一个独有的数据库快照。事务可以安心地读取这个快照中的数据而不需要去担心其他事务(因此只读事务是不会失败也不会被等待的)。同理，事务对数据的更新也首先暂存在这个独有的快照中，只有当事务提交的时候，这些更新才会试图被写回真正的数据库版本里。当一个事务准备提交时，它依然要确保没有其他事务更新了它所更新过的数据，否则，这个事务会被回滚
```

## 多版本时间戳(Multi-Version Timestamp Ordering)

把时间戳和多版本控制结合就形成了多版本时间戳机制。对于每个事务 Ti，系统都会设置相应的事务时间 TS(Ti)。对于每个数据单元 Q，系统会保存一系列的版本数据 Q1，Q2，Q3，… Qn。其中，每个版本 Qx 保存以下信息：



1. 当前版本的数据值
2. W-TS(Qx): 当 Qx 被某个事务 Ti 创建的时间戳，即 TS(Ti)
3. R-TS(Qx): 由于一个版本的数据可以被多个事务读取，这里存储的是最大的事务时间戳：最近一次被某个事务 Tj 读取成功的时间戳，即 TS(Tj)



当一个事务 Tx 创建了数据 Q 版本 Qk，Qk 会保存 Tx 所写入的数据值，同时，系统会把 W-TS(Qk)和 R-TS(Qk)都初始为 TS(Tx)；当有另一个事务 Ty 并且 TS(Ty) > TS(Tx)读取 Qk，系统会更新 R-TS(Qk)至 TS(Ty)。



现在介绍详细的操作机制：给定当前事务 Ti 对数据 Q 发起了一个读操作 read(Q)或者写操作 write(Q)。并假定，版本 Qk 是 Q 的所有版本中持有最大的但小于或等于 TS(Ti)的 W-TS 的时间戳。则：



1. 如果 Ti 是读操作，则读取成功，返回 Qk 中的值给 Ti。
2. 如果 Ti 是写操作，则需要判断，如果 TS(Ti) < R-TS(Qx)，即说明有一个更新的事务已经读取了数据，因此系统判定更新失败，回滚 Ti。如果 TS(Ti) = W-TS(Qx)，系统可以直接将 Ti 的值覆盖 Qk 的原值；如果 TS(Ti) > R-TS(Qx)，则创建一个新的版本 Q。



规则一很容易理解，一个事务可以读取到在它看来最新的数据。规则二则保证了一个事务被需要被撤销，如果已经有更新的事务读取了某个版本。



多版本时间戳机制的一大好处在于，一个读取数据的事务永远不会失败也不需要等待。在一个读多写少的场景下，相比于先前介绍的两种机制，会有很好的性能提升。



当然，它也是有缺点的。首先，就是在读取操作的事务中，也需要更新相应的 R-TS(Qk)，并且读取数据，这就导致可能产生两次磁盘操作，而非只读一次。另外，当写操作发生冲突时，它会要求回滚失败的事务，相比起等待，回滚操作可能更昂贵一些。下面介绍的另一种的实现可以解决这个问题。



## 多版本两阶段加锁(Multi-Version Two-Phase Locking)

多版本两阶段加锁机制，相比于上文介绍的多版本时间戳机制，是要集多版本控制和两阶段加锁之所长。它会区分对待只读操作的事务和有更新操作的事务。



有更新数据的事务会遵守两阶段加锁的规则，即事务需要持有所有的锁直至事务结束。这样，这些事务就能够保证有序性(在介绍 [两阶段加锁](https://www.infoq.cn/article/KyZjpzySYHUYDJa2e1fS)的时候已经讲解过)。这样做的好处在于不同的写事务可以等待并且按照顺序依次完成而不需要回滚后重试。对于只读操作，和上文介绍的多版本时间戳机制一样。不同的是，在多版本两阶段加锁中，事务的时间不再是时间戳，而是表现相对时序的事务计数 TS-Counter。这个 TS-Counter 在每次事务提交时被更新。



对于只读操作的事务 Ti，数据库系统会把 TS(Ti)赋于当前 TS-Counter 的值，这样 Ti 读取数据就和上面介绍的多版本时间戳一样，会读取到最大的但小于或等于 TS(Ti)的 Q 版本的值。对于有更新操作的事务，如果要读取一个数据，首先，它会获取这个数据的共享锁，并且读取最新版本的数据。当事务需要写数据时，首先要获取数据的独占锁并且创建一个新的版本，并把版本的时间戳设置为无穷大，当这个事务要被提交时，把它锁创建的所有数据的新版本的时间戳设置为 TS-Counter+1，并且同时更新系统的 TS-Ccounter，也变为 TS-Counter+1。



## 多版本并发控制的缺陷

天下没有免费的午餐，我们来讨论它有什么缺陷。



1. 额外的存储和计算资源支出：首先，需要额外存储历史版本数据，并且在执行时，每个事务要快速定位到正确的版本，并且对于更新的事务，通常情况会复制一份或者创建一个新的版本来暂存数据，这些都是需要消耗存储和计算资源的。当然，数据库系统可以定期对老的数据版本进行清理来释放存储空间。
2. 多线程竞争时间戳分配：由于需要保证不同事务的有序性，因此需要有一个共享的时间戳实现来分配(无论是用时间，还是相对的 counter)，免不了不同的事务线程需要去竞争时间戳。
3. 有些情况下，会导致频繁的事务回滚：特别当事务之间存在大量竞争的时候，会造成频繁的事务回滚。



总结一下，我们介绍了两种具体的多版本并发控制的实现，多版本时间戳机制和多版本两阶段加锁，两者都保证了对于只读操作的事务，不会失败也不会被等待。区别在于写操作的事务，多版本时间戳机制会回滚“迟到”的写事务，而多版本两阶段加锁通过共享和独占锁来对多个写操作事务排序。同时，我们也讨论了一些多版本并发控制的缺陷。但是，瑕不掩瑜，它依然是最常见的并发控制实现。



回忆一下在[第十期](https://www.infoq.cn/article/teJA7X43BO2alp6rLCWk)介绍的隔离级别：读未提交；读提交；可重复读和可有序化。那用多版本并发控制实现了哪个隔离级别呢？读者可能会觉得，应该是可有序化。但其实不然，它实现了一个新的隔离级别叫做 Snapshot Isolation(快照隔离)。



## 快照隔离(Snapshot Isolation)

快照隔离可以看作是对每一个事务，分配了一个独有的数据库快照。事务可以安心地读取这个快照中的数据而不需要去担心其他事务(因此只读事务是不会失败也不会被等待的)。同理，事务对数据的更新也首先暂存在这个独有的快照中，只有当事务提交的时候，这些更新才会试图被写回真正的数据库版本里。当一个事务准备提交时，它依然要确保没有其他事务更新了它所更新过的数据，否则，这个事务会被回滚。



那为什么说快照隔离是一个单独的隔离级别而不是可有序化呢？问题就在于，它提供了“太多”的隔离性(英语中用 too much！貌似更形象一些)。我想借用 CMU 数据库教授 Andy Pavlo 课里举过的一个非常贴切的例子，我们现在假设数据就是围棋的棋子，一部分是黑子，一部分是白子。现在同时有两个更新事务：T1: 把所有的白子变成黑子(写成 SQL 语句可以看作是这样的： **UPDATE color = ‘black’ FROM marbles WHERE color = ‘white’** )。T2:把所有的黑子变成白子。执行这两个操作会怎么样呢？由于快照隔离(多版本并发控制)机制，这两个事务更新的数据不一样，因此都会视为成功。这就导致了最终，白子和黑子的颜色互换。见下图示例。



![img](https://static001.infoq.cn/resource/image/79/ce/79ae28d92c24970f2ee1eeb8ff6590ce.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



但是，根据可有序化的定义，要确保不同的事务最终是可以沿着时间线排成一溜执行，因此无论是 T1 先执行还是 T2 先执行，最终的颜色应该全是黑色或是白色，如下图所示。



![img](https://static001.infoq.cn/resource/image/42/58/42e7b461db5c7d01327374643f309b58.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



上述的示例被称为 Write Skew Anomaly。因此，快照隔离是一个区别于可有序化的隔离级别，如果把它安插在我们介绍过的隔离级别，应该如下图所示。



![img](https://static001.infoq.cn/resource/image/06/4e/0637yy401860a95bf8c2d70e2092544e.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



大部分的数据库都支持快照隔离。Orcale 和 PostgreSQL 数据库其实是使用快照隔离机制来实现可有序化机制。因此，在极端小概率情况下，数据库的状态是有可能“非有序化”的。



## 总结

至此，事务、隔离和并发就全部介绍完毕。我们分别介绍了事务的 ACID 属性以及不同的隔离级别，再依次介绍了不同的并发控制实现，两阶段加锁，时间戳机制，和多版本并发控制。



对于单机的数据库系统就介绍得差不多了。下一篇文章，我们聊一个非常有意思的话题：假设给你一个单机的数据库系统实现，要求在这个基础上把它扩建成分布式数据库系统，你会怎么做？这个问题还有个小故事。很久很久以前，在原来公司参与系统设计面试的时候，候选人和我说，我原本准备的面试题另一个面试官已经问过了(当时我的内心是崩溃的…)。这是我临时想出来的面试题，留给大家做思考
