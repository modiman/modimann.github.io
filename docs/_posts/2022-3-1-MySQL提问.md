---
typora-root-url: imgs
---

* [InnoDB并发如此高，原因竟然在这？](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651967047&idx=1&sn=b605fe50e6dd74ecad659c464a0e29ee&chksm=bd2d7b9b8a5af28d35c13e469e8e2c6a7082f00e608fd52d999fc0f7a6aec016faf2a8d748fa&scene=178&cur_album_id=1776446719614910465#rd？)



# 数据库管理系统

![image-20220710170540048](/image-20220710170540048.png)

## 查询语句执行

参考文献

* [SQL查询语句是如何被执行的](https://juejin.cn/post/7002578653385080863#heading-6)
* 作者：代码迷途
  链接：https://juejin.cn/post/7002578653385080863
  来源：稀土掘金
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

一条SQL语句执行的流程



**建立连接→查询缓存→分析器→优化器→执行器**

### 1. 建立连接

### 2. 查询缓存

```
查询缓存中存放着，之前执行过的语句及其结果，会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。
```

如果命中查询缓存，则直接将结果返回给客户端，效率非常高。

如果没有命中查询缓存（查询的语句不在查询缓存中），则执行后续操作。**执行完成后的结果会被存入查询缓存中（如果开启了query_cache）**

#### 不推荐使用查询缓存

查询缓存往往弊大于利，查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

而且，任何字符上的不同，例如空格、注释等都会导致缓存的不命中。

**对于更新压力大的数据库来说，查询缓存的命中率会非常低**。

除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

`MySQL` 中可以**将参数 `query_cache_type` 设置成 `DEMAND`**，这样对于默认的 SQL 语句都不使用查询缓存。

对于确定要使用查询缓存的语句，可以用 `SQL_CACHE` 显式指定，如：

```sql
mysql> select SQL_CACHE * from T where ID=10；
```

**默认查询缓存功能是关闭的**。通过在MySQL服务器配置文件中添加`query_cache_type=2`，重启mysql服务启用。

**MySQL 8.0 版本直接去掉了查询缓存的整块功能， 即 8.0 开始就没有了查询缓存功能**。

### 3. 分析器

未命中查询缓存，或者未开启查询缓存，就会进入分析器对SQL语句做解析。

1. 分析器先做“词法分析”。一条 SQL 语句由多个字符串和空格组成，MySQL 需要识别出里面的字符串分别是什么，代表什么。

*比如，从输入的"select"这个关键字，识别出是一个查询语句。同时，还要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”等。*

1. 然后做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。

*如果语句存在语法错误，比如这个语句`elect \* from t where ID=1;`，`select`少了开头的`s`，语法分析就会收到“You have an error in your SQL syntax”的错误。*

> 语法错误会提示第一个出现错误的位置，错误信息的“use near”位置。

1. 语义分析（语义解析），将上面分析的结果（即生成的解析树），进一步分析字符串标识符的语义，检查表、列是否存在，别名是否有歧义等，通过后生成新的解析树。

> 分析器部分可以分为 **解析器和预处理器**。
>
> - 解析器处理语法和解析查询, 生成一课对应的“解析树”。解析器将查询分解成一个个Identifier（词法分析），然后确保查询中的标识都是有效的，检查其中的语法错误（语法分析），比如标识符是否有效、语句是否闭合等。
> - 预处理器进一步检查“解析树”的合法性，解决解析器无法解析的语义。比如: 检查数据表和数据列是否存在、检查名字和别名保证没有歧义等。如果通过则生成新的解析树，再提交给优化器。
>
> 即，在优化器之前还需要进行预处理器的"语义分析"。

一个 Parser 的核心分为**词法分析、语法分析和语义分析**

如果表 T 中没有字段 k，执行语句 `select * from T where k=1;`, 在 分析器 的 预处理器 分析中，就会报“不存在这个列”的错误： `“Unknown column ‘k’ in ‘where clause’”`。

### 4. 优化器

优化器是专门用于对查询进行优化的（包括所有类型的SQL语句：DDL和DML）。

优化器会根据解析树生成执行计划，同时，在表中有多个索引的时候，决定使用哪个索引；一个语句有多表关联(join)时，决定各个表的连接顺序。

优化器执行完，一个语句的执行方案就确定了下来。

### 5. 执行器（查询执行引擎）

MySQL 通过分析器知道SQL语句要做什么，通过优化器知道该怎么去做。剩下的就是进入执行器（或者执行阶段），依照“执行计划”开始执行。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，会返回没有权限的错误。

*权限错误的信息通常为：SELECT command denied to user 'xx'@'localhost' for table 'xx'*

权限OK，就会打开表继续执行。

打开表的时候，执行器就会根据表的引擎定义，使用这个引擎提供的接口。

比如，这个查询语句的例子，T 表的 ID 字段没有索引，则执行流程是这样的：

1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

这样，一条SELECT语句完整的执行逻辑就结束了。

对于有索引的表，（如果能够命中索引）执行的逻辑基本相同，【开启了索引下推，默认都是开启，并且没必要关闭】。执行的逻辑为：第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，由引擎提供这些接口。

无索引时，执行器循环调用引擎接口，先调用“第一行”并过滤数据，之后循环调用“下一行”并过滤数据，一直到最后一行。过滤是在Server层完成的。

有索引时，执行器调用“满足条件的第一行”的引擎接口，之后调用“满足条件的下一行”的接口。有索引时在引擎层完成数据的过滤。

索引下推（`index condition pushdown` —— `ICP`）就是把数据筛选交由存储引擎层处理，减少Server层的负担，提高查询效率。

> 索引下推(`Index Condition Pushdown`) ICP 是 Mysql5.6 之后新增的功能，主要的核心点就在于把数据筛选的过程放在了存储引擎层去处理，而不是像之前一样放到Server层去做过滤。
>
> 在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 。
>
> 索引条件下推优化是非常好的查询优化策略，把本来由Server层做的索引条件检查下推给存储引擎层来做，可以减少存储引擎查询基础表的次数、降低回表，也可以减少MySQL服务器从存储引擎接收数据的次数、减少server层的过滤处理。

> MySQL 慢查询日志中有 rows_examined 的字段，表示这个语句执行过程中“调用”存储引擎取数据行的次数。
>
> rows_examined 会被笼统的看做语句执行过程扫描了多少行。但有时，执行器调用一次，引擎内部可能扫描了多行，因此**引擎扫描行数跟 rows_examined 并不完全相同**。但可以看做：**执行器获取到的数据行数**。

## 更新语句执行

* Mysql实战45讲 

更新与查询的大体流程相似，但引入了两个重要的日志模块**redo log（重做日志）和 binlog（归档日志）。**

### redo log（InnoDB引擎特有）

更新数据的两种方式：

> 1. 即来即更新
> 2. 汇总一段时间的数据统一更新

由于读写磁盘的IO成本太高，一般使用后者进行数据库更新操作。这也叫做WAL技术，全称Writing-Ahead Loging.关键点为先写日志，再写磁盘。

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里 面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作 记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

redo log的大小是固定的，从头开始写，写到末尾又从头开始循环。

![image-20220712170308529](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220712170308529.png)

* write pos是当前记录的位置
* checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文 件。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为**crash-safe**。 要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或 写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。

### binlog

Server层自己的日志，称为binlog（归档日志）。

为什么会有两份日志呢？ 

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有 crash-safe的能力，binlog日志只能用于归档。

而InnoDB是另一个公司以插件形式引入MySQL 的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是 redo log来实现crash-safe能力。

 **这两种日志有以下三点不同**。 

> 1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 
> 2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的 是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 
> 3.  redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件 写到一定大小后会切换到下一个，并不会覆盖以前的日志。 

有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语 句时的内部流程。 

> 1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一 行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然 后再返回。 
> 2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行 数据，再调用引擎接口写入这行新数据。 
> 3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处 于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 4. 
> 4. 执行器生成这个操作的binlog，并把binlog写入磁盘。 5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更 新完成。 这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表 示是在执行器中执行的

![image-20220712170859812](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220712170859812.png)



### 两阶段提交

**redo log的写入拆成了两个步骤：prepare和 commit，这就是"两阶段提交"**

​	为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得 从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承 诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期 做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数 据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备 份恢复到临时库； 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时 刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢 复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法 来进行解释。 由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语 句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？ 1. 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异 常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回 来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份 日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这 个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。 2. 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以 后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日 志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的 状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再 多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用binlog来 实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保 持逻辑上的一致。 小结 今天，我介绍了MySQL里面最重要的两个日志，即物理日志redo log和逻辑日志binlog。 redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候， 表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证 MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建 议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 我还跟你介绍了与MySQL日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑 一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。 文章的最后，我给你留一个思考题吧。前面我说到定期全量备份的周期“取决于系统重要性，有 的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或 者说，它影响了这个数据库系统的哪个指标？ 你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾给出我的答案。 感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读

## SQL优化

- 最大化利用索引；
- 尽可能避免全表扫描；
- 减少无效数据的查询；



## 存储引擎的作用

* 存储引擎用于具体的组织、管理物理文件，包括二进制日志（binlog属于Server管理的文件，但也提供了存储引擎使用的接口）、数据文件、错误日志、慢查询日志、全日志、redo/undo 日志等。
* 除此之外，还有引擎程序运行的实例，包括功能结构、内存中数据的管理等（这就是具体引擎自己的功能实现了）。
* 不同的存储引擎有着不同的文件组织、管理的方式。
* 同时存储引擎提供Server层可以访问、存取数据的接口，用于 server 层的执行引擎获取或存入数据。

| 数据库引擎        | 特点                                                         |
| ----------------- | ------------------------------------------------------------ |
| ISAM              | ISAM是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到数据库被查询的次数要远大于更新的次数。因此，ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要**不足之处**在于，它**不支持事务处理**，也**不能够容错**：如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。 |
| MYISAM            | MYISAM是MYSQL的ISAM扩展格式和**缺省的数据库引擎**。除了**提供**ISAM里所没有的**索引和字段管理的**功能，MYISAM还使用一种**表格锁定的机制**，来**优化多个并发的读写操作**。其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。MYISAM还有一些有用的扩展，例如用来修复数据库文件的MYISAMCHK工具和用来恢复浪费空间的MYISAMPACK工具。　<br/>MYISAM强调了快速读取操作，这可能就是为什么MYSQL受到了WEB开发如此青睐的主要原因：在WEB开发中你所进行的大量数据操作都是读取操作。所以，大多数虚拟主机提供商和INTERNET平台提供商只允许使用MYISAM格式。 |
| HEAP              | HEAP**允许只驻留在内存里的临时表格**。驻留在内存里让HEAP要比ISAM和MYISAM都快，但是它所**管理的数据是不稳定的**，而且如果在关机之前没有进行保存，那么所有的数据都会丢失。在数据行被删除的时候，HEAP也不会浪费大量的空间。HEAP表格在你需要使用SELECT表达式来选择和操控数据的时候非常有用。要记住，在用完表格之后就删除表格。 |
| INNODB和BERKLEYDB | INNODB和BERKLEYDB（BDB）数据库引擎都是造就MYSQL灵活性的技术的直接产品，这项技术就是MYSQL++ API。在使用MYSQL的时候，你所面对的每一个挑战几乎都源于ISAM和MYISAM数据库引擎不支持事务处理也不支持外来键。尽管要比ISAM和MYISAM引擎慢很多，但是INNODB和BDB包括了**对事务处理和外来键的支持**，这两点都是前两个引擎所没有的。如前所述，如果你的设计需要这些特性中的一者或者两者，那你就要被迫使用后两个引擎中的一个了。 |



### MyISAM与InnoDB的比较

|                      | MyISAM                                                       | InnoDB                                                       |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 存储结构             | frm、 MYD:表里面的数据文件 MYI:表里面的索引文件              | frm:创建表的语句 idb:表里面的数据+索引文件                   |
| 存储空间             | 静态表、动态表、压缩表                                       | 需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 |
| 可移植性、备份及恢复 | 数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。   | 在数据量很大的时候就相对痛苦； InnoDB可借由事务记录档（Transaction Log）来恢复程序崩溃（crash），或非预期结束所造成的数据错误； InnoDB的修复时间，大略都是固定的。 |
| 索引                 | `非聚集索引`，索引保存的是数据文件的指针。                   | `聚集索引`，聚集索引的文件存放在主键索引的叶子节点上         |
| 事务支持             | `不提供事务支持`。                                           | `提供事务支持`                                               |
| 主键自增长           | 可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 | InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 |
| 锁的粒度             | `只支持表级锁`，                                             | `支持行级锁`。                                               |
| 全文索引             | 支持 FULLTEXT类型的全文索引                                  | 不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 |
| 表主键               | 允许没有任何索引和主键的表存在，索引都是保存行的地址。       | 如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 |
| 存储表的具体行数     | 保存有表的总行数，如果select count() from table;会直接取出出该值。 | 没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 |
| 外键                 | 不支持                                                       | 支持，对一个包含外键的 InnoDB 表转为 MYISAM 会失败。         |

### 事务支持

MyISAM 不提供事务支持，InnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力，这一点保证数据的正确性和完整性非常重要。

1. MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。
2. MySQL InnoDB 引擎通过 锁机制、MVCC 等手段来保证事务的隔离性（ 默认支持的隔离级别是 REPEATABLE-READ ）。
3. 保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。





链接：https://juejin.cn/post/7084093849198395399
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

# 索引

## **索引分类**

| 功能逻辑 | 物理实现方式 | 字段个数 |
| -------- | ------------ | -------- |
| 普通索引 | 聚簇索引     | 单列索引 |
| 唯一索引 | 非聚簇索引   | 联合索引 |
| 主键索引 |              |          |
| 全文索引 |              |          |

 一张表可以建立**任意多个索引，**每个索引可以是**任意多个字段**的组合。索引**可能会提高查询速度**（如果查询时使用了索引），但**一定会减慢写入速度**，因为每次写入时都需要更新索引，所以索引只应该加在经常需要搜索的列上，不要加在写多读少的列上。

**单列索引与复合索引**

只包含一个字段的索引叫做**单列索引**，包含两个或以上字段的索引叫做**复合索引**（或组合索引）。

建立复合索引时，字段的顺序极其重要。

下面这个SQL语句在 列X，列Y，列Z 上建立了一个复合索引。

```mysql
CREATE INDEX 索引名 ON 表名(列名X, 列名Y, 列名Z);
```

这样SQL查询处理器在处理制定年份的查询时，仅仅对年份为指定值的Movies的元组进行测试，从而使获得查询结果的时间大大缩短

* **覆盖索引**：覆盖索引是select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的[索引](https://baike.baidu.com/item/索引/5716853?fromModule=lemma_inlink)覆盖。



5. 

### 联合索引

#### 最左前缀匹配

面试官：那你知道最左前缀匹配吗？

* 根据业务需求，where子句中使用最频繁的一列放在最左边，
* 因为MySQL索引查询会遵循最左前缀匹配的原则，即最左优先，
* 在检索数据时从联合索引的最左边开始匹配。
* 所以当我们创建一个联合索引的时候，
* 如(key1,key2,key3)，相当于创建了
* （key1）、(key1,key2)和(key1,key2,key3)三个索引，
* 这就是最左匹配原则。

## 回表

![image-20220911153409074](https://img2022.cnblogs.com/blog/2605549/202209/2605549-20220911171256268-1706020094.png)

**聚簇|非聚簇索引**

* 找到了索引就找到了需要的数据，那么这个索引就是聚簇索引，所以主键就是聚簇索引，修改聚簇索引其实就是修改主键。
* 索引的存储和数据的存储是分离的，也就是说找到了索引但没找到数据，需要根据索引上的值(主键)再次回表查询,非聚簇索引也叫做[辅助索引](https://www.zhihu.com/search?q=辅助索引&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"142139541"})。

上图的age就是一个**非聚簇**索引，

## explain命令

1. 建立索引

```mysql
create index join_index on maoyan(box,avgbox,avgseat)
```

**=判断下，只要box出现索引就会生效**

* EXPLAIN SELECT * FROM maoyan WHERE avgbox = 1 AND box = 1

**索引失效**

1. 查询条件里box不存在
2. 查询条件里出现> 、<号
3. 

```mysql
EXPLAIN SELECT * FROM maoyan WHERE avgbox = 1 AND avgseat = 1

```





```mysql
explain select goods_id,user_id from t_seckill_order where user_id = 1;
```

mysql> explain select goods_id,user_id from t_seckill_order where user_id = 1;
+----+-------------+-----------------+------+-----------------+-----------------+---------+-------+------+-------------+
| id | select_type | table           | type | possible_keys   | key             | key_len | ref   | rows | Extra       |
+----+-------------+-----------------+------+-----------------+-----------------+---------+-------+------+-------------+
|  1 | SIMPLE      | t_seckill_order | ref  | seckill_uid_gid | seckill_uid_gid | 8       | const |    1 | Using index |
+----+-------------+-----------------+------+-----------------+-----------------+---------+-------+------+-------------+

### id

select查询的序列号包含一组数字，表示查询中执行select子句或者操作表的顺序

### select_type

* SIMPLE：简单的select查询，查询中不包含子查询或者UNION
* PRIMARY：查询中若包含任何复杂的子部分，最外层查询则被标记为
* SUBQUER： 在select或者where列表中包含了子查询
* DERIVED：在from列表中包含的子查询被标记为DERIVED(衍生)MySql会递归执行这些子查询，把结果放在临时表
* UNION：若第二个select出现在union之后，则被标记为UNION，若union包含在from子句的子查询中，外层select被标记为：DERIVED
* UNION RESULT：从union表获取结果的select

### type

访问类型

* all：全表扫描
* index：全索引扫描
* range：范围扫描
* ref：不唯一的索引扫描，有多个符合条件的行
* eq_ref：唯一的索引扫描，只有一条记录与之匹配，常见于主键索引
* const、system：这两种的情况都比较好，前一种值的是条件为常量
* null：执行阶段不需要访问的表





### 建索引的几大原则

1. 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如`a = 1 and b = 2 and c > 3 and d = 4` 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2. =和in可以乱序，比如`a = 1 and b = 2 and c = 3 `建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。

3. 尽量选择区分度高的列作为索引，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。

4. 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。

5. 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

（1）对于经常查询的字段，建议创建索引。

（2）索引不是越多越好，一个表如果有大量索引，不仅占用磁盘空间，而且会影响INSERT，DELETE，UPDATE等语句的性能。

（3）避免对经常更新的表进行过多的索引，因为当表中数据更改的同时，索引也会进行调整和更新，十分消耗系统资源。

（4）数据量小的表建议不要创建索引，数据量小时索引不仅起不到明显的优化效果，对于索引结构的维护反而消耗系统资源。

（5）不要在区分度低的字段建立索引。比如性别字段，只有 “男” 和 “女” ，建索引完全起不到优化效果。

（6）当唯一性是某字段本身的特征时，指定[唯一索引](https://www.zhihu.com/search?q=唯一索引&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"416550784"})能提高查询速度。

（7）在频繁进行跑排列分组（即进行 group by 或 order by操作）的列上建立索引，如果待排序有多个，可以在这些列上建立组合索引。



作者：一只菜鸟程序员
链接：https://zhuanlan.zhihu.com/p/416550784
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 常见引擎的索引

### 参考文章

* [mysql 数据库引擎 ](https://www.cnblogs.com/0201zcr/p/5296843.html)

### MyIASM引擎的索引结构

　MyISAM引擎的索引结构为**B+Tree**，其中B+Tree的**数据域存储的内容为实际数据的地址**，也就是说它的索引和实际的数据是分开的，只不过是用索引指向了实际的数据，这种索引就是所谓的**非聚集索引**。如下图所示：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320204001959-1320607047.png)

　　这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320204143084-1395279894.png)

　　同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。

### Innodb引擎的索引结构

　　与MyISAM引擎的索引结构同样也是B+Tree，但是Innodb的索引文件本身就是数据文件，即**B+Tree的数据域存储的就是实际的数据**，这种索引就是**聚集索引**。这个索引的key就是数据表的主键，因此InnoDB表数据文件本身就是主索引。

　　并且和MyISAM不同，InnoDB的**辅助索引数据域存储的也是相应记录主键的值**而不是地址，所以当以辅助索引查找时，会先根据辅助索引找到主键，再根据主键索引找到实际的数据。所以Innodb不建议使用过长的主键，否则会使辅助索引变得过大。建议使用自增的字段作为主键，这样B+Tree的每一个结点都会被顺序的填满，而不会频繁的分裂调整，会有效的提升插入数据的效率。

两者区别：

　　第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320205622193-1852020506.jpg)

　　上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。

 

 

　　第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引：

![img](https://images2015.cnblogs.com/blog/731178/201603/731178-20160320205632724-1223368895.jpg)

　　这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。


　　了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调(可能是指“非递增”的意思)的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调(可能是指“非递增”的意思)的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。

### 2.主键的优点与缺点

1.自增主键，在mysql中应用最广泛。

**优点：**
    1>需要很小的数据存储空间，仅仅需要4 byte。（bigint类型，是8 byte）

　　　2>insert和update操作时使用INT的性能比UUID好，所以使用int将会提高应用程序的性能。

​    3>index和Join操作，int的性能最好。

​    4>容易记忆。

**缺点：**
    1>如果经常有合并表的操作，就可能会出现主键重复的情况。

​    2>使用int数据范围有限制。如果存在大量的数据，可能会超出int的取值范围。

​    3>很难处理分布式存储的数据表。

2。UUID

**优点：**
    1>能够保证独立性，程序可以在不同的数据库间迁移，效果不受影响。
    2>保证生成的ID不仅是表独立的，而且是库独立的，这点在你想切分数据库的时候尤为重要。
**缺点：**
    1>比较占地方，和INT类型相比，存储一个UUID要花费更多的空间。
    2>使用UUID后，URL显得冗长，不够友好。

​    3>没有内置的函数获取最新产生的UUID主键。

​    4>很难记忆。Join操作性能比int要低。

​    5>UUID做主键将会添加到表上的其他索引中，因此会降低性能。

## 索引失效

### 使用select * 

如果使用select *，当违背最左前缀匹配原则时索引一定会失效。

```sql
explain select * from t_user where username = 'Tom2';
```

 但如果指定要查询的字段，可能会走覆盖索引

### 索引列参与运算

```sql
explain select * from t_user where id + 1 = 2 ;
```

这会导致数据库扫描全部的id,进行运算，之后进行比较。

### 索引列参与了函数处理

第四种索引失效情况：**索引列参与了函数处理，会导致全表扫描，索引失效**。

### like

第五种索引失效情况：**模糊查询时（like语句），模糊匹配的占位符位于条件的首部**。

### 使用OR操作

* or两侧的两列一个加索引一个没加索引，由于没加索引的一定会全表扫描，所以加了索引的一列也就没必要走索引了
* or两边分别为>和<且范围包含全集(age>1 or age < 2)，但限定在`select *`中

### 两列作比较

**两列数据做比较，即便两列都创建了索引，索引也会失效**。

### 不等于比较



# 范式

### 参考文献

作者：刘慰
链接：https://zhuanlan.zhihu.com/p/20028672
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### 第一范式

**符合1NF的关系中的每个属性都不可再分**(所有关系型数据库的最低要求)

反例：![](https://pic3.zhimg.com/89507a1682f28fd2dde066cf94d77b4a_b.jpg)

缺陷：但是仅仅符合1NF的设计，仍然会存在[数据冗余](https://www.zhihu.com/search?q=数据冗余&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"20028672"})过大，插入异常，删除异常，修改异常的问题，例如对于**表3**中的设计：

![](https://pic3.zhimg.com/dfdc86b0e2babe1f8da03d8e2b17ad06_b.jpg)



# 事务

**参考文章**

* [数据库内核杂谈](https://www.infoq.cn/article/teJA7X43BO2alp6rLCWk)

事务的定义是：**一个事务是一组对数据库中数据操作的集合**。无论集合中有多少操作，对于用户来说，只是对数据库状态的一个原子改变。

```
单从概念定义来理解，可能有些晦涩难懂，我们举个例子来讲解：数据库中有两个用户的银行账户 A:100 元; B:200 元。假设事务是 A 转账 50 元到 B，可以理解为这个事务由两个操作组成：1) A-= 50; 2) B+=50。对于用户来说，数据库对于这个事务只有两个状态：执行事务前的初始状态，即 A:100 元; B:200 元，以及执行事务后的转账成功状态：A:50 元;B:250 元，不会有中间状态，比如钱从 A 已经扣除，却还没转到 B 上:A:50 元; B:200 元。
```

## 四大特性

### 原子性(atomicity)

* 基于undo log实现
* 一个事务的所有操作要么全部执行，要么一个都不执行。
* 如果在执行事务的过程中，因为任何原因导致事务失败，已经执行的操作都要被回滚(rollback)。这种“all-or-none"的属性就是所谓的事务的原子性(atomicity)。

#### undo log 回滚日志

**Undo Log**记录的是逻辑日志，也就是SQL语句。

比如：当我们执行一条insert语句时，**Undo Log**就记录一条相反的delete语句。

**作用：**

1. 回滚事务时，恢复到修改前的数据。
2. 实现 **MVCC** 。

### 一致性(consistency)

* 假定数据库的初始状态是稳定的，或者说对用户来说是一致的。由于事务执行的原子性，即执行失败就回滚到执行前的状态，执行成功就变成一个新的稳定状态。因此，事务的执行会保持数据库状态的一致性(consistency)。



### 隔离性(isolation)

* 数据库系统是多用户系统。多个用户可能在同一时间执行不同的事务，称为并发。
* 如果想要做到事务的原子性，那么数据库就必须做到并发的事务互不影响。
* 从事务的角度出发，在执行它本身的过程中，不会感知到其他事务的存在
* 从数据库的角度出发，即使同一时间有多个事务并发，从微观尺度上看，它们之间也有先来后到，必须等一个事务完成后，另一个事务才开始。这种并发事务之间的不感知就是所谓的事务隔离性(isolation)。



### 持久性(durability)

* 当一个事务被认定执行成功后，即代表这个事务的操作被数据库持久化。
* 因此，即使数据库在此时奔溃了，比如进程被杀死了，甚至是服务器断电了，这个事务的操作依然有效，这就是事务的另一个属性，持久性(durability)。

## 隔离级别(Isolation Level)

如何实现隔离性？最简单的办法：给数据库加一个全局操作锁，同一时间只允许一个用户操作

**缺点**：限制了并发性。

**解决办法**：根据对隔离性的需求，设置多个隔离级别，越严格的隔离级别越接近全局锁，反之，越宽松则越有利于高并发

为了方便描述，首先定义一个简单的事务模型

| 数据单元 | 数据操作     | 事务操作           |
| -------- | ------------ | ------------------ |
| A        | read(A)      | begin(开启事务)    |
| A        | write(A,val) | commit(提交事务)   |
| A        |              | rollback(回滚事务) |

接下来介绍隔离级别

### 1. read uncommitted（读未提交）

**读未提交就是在一个事务中，允许读取其他事务未提交的数据。下图示例很清晰地诠释了读未提交：**	

**场景描述**

1. 数据A，原本值为10
2. 事物甲修改为5
3. 事务乙读取到5
4. 事务甲出现问题，提交失败，A被改回10
5. 事务乙出现脏读



在事务 T1 中，读取 A 得到结果是 5，是因为事务 T2 修改了 A 的值，虽然当时 T2 还未提交，甚至最后 T2 回滚了。读未提交导致的问题就是 dirty read(脏读)。

* 脏读的定义就是，一个事务读取了另一个事务还未提交的修改。
* 虽然可能大多数情况下，我们都会认为脏读产生了不正确的结果。
* 但是，抛开业务谈正确性都是耍流氓。或许，某些用户的某些业务，为了支持更大地并发，允许脏读的出现。因为，对于读未提交，完全不需要对操作进行加锁，自然并发性更高。

### 2. read committed(读提交)

为了避免脏读，引入第二层隔离级别：读提交。**读提交就是指在一个事务中，只能够读取到其他事务已经提交的数据。**

还是刚才的背景

1. 事务甲第一次读取，得到值10
2. 中间事务乙对数据A进行修改并提交，这时A变为5
3. 事务甲第二次读取A，值变成了5，两次读取数据不一致

**新的问题**

在 T1 事务中，先后读取了两次 A，两次的值不一样了。回顾最早提及的事务的隔离性，两次读取同一数据的值不一样，其实违反了隔离性。因为隔离性定义了一个事务不需要感知其他事务的存在，但显然，由于值不同，说明在这个过程中另一个事务提交了数据。这类问题就被定义为 **nonrepeatable read(不可重复度读)**：在一个事务过程中，可能出现多次读取同一数据但得到的值不同的现象。

### 3. repeatable read(可重复读)

为了避免不可重复读，引入第三层隔离级别：可重复读。

* 可重复读指的是在一个事务中，只能读取已经提交的数据，且可以重复查询这些数据，
* 并且，在重复查询之间，不允许其他事务对这些数据进行写操作
* 虽然我们还没讲到实现，但不难想象，对读数据加读写锁就能实现。

**新的问题**

```sql
T1:
BEGIN;
SELECT * FROM students WHERE class_id = 1;  // (1)
... 
SELECT * FROM students WHERE class_id = 1;  // (2)
...
COMMIT;

```

```sql
T2:
BEGIN;
INSERT INTO students (1 /* class_id */, ...);
COMMIT; 

```

T2 事务并没有修改现有数据，而是新增了一条新数据，恰巧 class_id = 1。如果这条插入介于(1)和(2)之间，(2)的结果会改变吗？答案是，会的。语句(2)会比(1)多显示一条记录，即 T2 插入的。这个问题被称为 phantom read(幻读)，

**幻读**指的是，在一个事务中，当查询了一组数据后，再次发起相同查询，却发现满足条件的数据被另一个提交的事务改变了。

### 4. serializable(可有序化)

如何才能避免幻读呢？数据库系统只能推出最保守的隔离机制，serializable(可有序化)，即所有的事务必须按照一定顺序执行，直接避免了不同事务并发带来的各种问题。

### 总结-四种级别

1. **读未提交：在一个事务中，允许读取其他事务未提交的数据。**
2. **读提交：在一个事务中，只能够读取到其他事务已经提交的数据。**
3. **可重复读：在一个事务中，只能读取已经提交的数据，且可以重复查询这些数据，并且，在重复查询之间，不允许其他事务对这些数据进行写操作。**
4. **可有序化：所有的事务必须按照一定顺序执行。**

依次解决的三个问题

1. **脏读：一个事务读取了另一个事务还未提交的修改**
2. **不可重复度：在一个事务过程中，可能出现多次读取同一数据但得到不同值的现象。**
3. **幻读：在一个事务中，当查询了一组数据后，再次发起相同查询，却发现满足条件的数据被另一个提交的事务改变了。**

下方列出了一张表格，更直观地展现它们之间的关系。

| 隔离级别 |   脏读   | 不可重复读 |   幻读   |
| :------: | :------: | :--------: | :------: |
| 读未提交 | 可能出现 |  可能出现  | 可能出现 |
|  读提交  |   不能   |  可能出现  | 可能出现 |
| 可重复读 |   不能   |    不能    | 可能出现 |
| 可有序化 |   不能   |    不能    |   不能   |

## 隔离实现机制

### 加锁实现机制(Lock-based protocols)

实现隔离性最简单的方法是对全局数据加锁，但这样性能大大降低。

1. 降低锁的粒度

可以想办法把锁的粒度变细，即**仅对要读写的数据加锁**而非全局锁。通过加锁来确保在同一时间，只有获得锁的事务可以对数据进行处理。

2. 定义不同类型的锁

并不是所有的事务对数据都是写操作，如果两个事务同时对某一数据进行读操作，它们之间并不需要互斥。因此，我们可以通过定义不同类型的锁，以及它们之间的兼容程度来获得更细粒度的控制。

**共享锁(share-mode lock; S-lock)**：

(share-mode lock; S-lock)，即当事务获得了某个数据的共享锁，它仅能对该数据进行读操作，但不能写，共享锁有时候也被称为读锁。

**独占锁(exclusive-mode lock; X-lock)**

当事务获得了某个数据的独占锁，它可以对数据进行读和写操作，独占锁也常被叫做写锁。

共享锁和独占锁的兼容模式如下：

|        | S-lock | X-lock |
| :----: | :----: | ------ |
| S-lock |  兼容  | 不兼容 |
| X-lock | 不兼容 | 不兼容 |

仅 S-lock 之间互相兼容，只有当多个事务同时持有共享锁时才能同时对数据进行读操作。

**新的问题**：什么时候加锁？什么时候释放锁？

**案例分析**

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>Unlock(B);<br/>X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(A). | T2:<br/>S-lock(A);<br/>Read(A);<br/>Unlock(A);<br/>S-lock(B);<br/>Read(B);<br/>Unlock(B);<br/>Display(A+B) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

两个事务对账号 A 和 B 进行操作(假设 A 初始值是 100；B 是 200)，事务 T1 用了 X-lock，因为需要对数据进行修改， 而 T2 仅需要使用 S-lock，因为只是读取数据。乍看之下，好像没有问题。无论是 T1 先执行，还是 T2 先执行，T2 中 display(A+B)都会是 300。但是，如果 T1 和 T2 的执行顺序如下：

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>Unlock(B);<br/> |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              | T2:<br/>S-lock(A);<br/>Read(A);<br/>Unlock(A);<br/>S-lock(B);<br/>Read(B);<br/>Unlock(B);<br/>Display(A+B) |
| X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(A). |                                                              |

这时候，T2 中的 display(A+B)的值就是 250，这是错误的数据。问题出在哪呢？T1 中释放对 B 的 X-lock 过早，使得 T2 获得了一个不正确的数值。既然原因是释放过早，那能不能通过延迟释放锁来解决这个问题。我们把 T1 和 T2 分别改写为 T3 和 T4(唯一的区别就是延缓了锁的释放到最后)，如下 表所示

| T1:<br/>X-lock(B);<br/>Read(B):<br/>B= B-50;<br/>Write(B);<br/>X-lock(A);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Unlock(B);<br/>Unlock(A). | T2:<br/>S-lock(A);<br/>Read(A);<br/>S-lock(B);<br/>Read(B);<br/>Display(A+B)<br/>Unlock(A);<br/>Unlock(B);<br/> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

T3 和 T4 分别获取了对 B 和对 A 的锁并且相互请求对 A 和对 B 的锁。相信大家都看出来了，这导致了死锁(dead lock)。这里就不具体介绍死锁的检查和破坏机制了(详情参见操作系统课)，你只需要知道，数据库系统是可以发现死锁的。解决方法也简单，选择其中一个参与的事务，回滚并放弃执行(如果一个不行，就两个)。相对于错误的数据，死锁显然是我们更愿意接受的，所谓两害取其轻

我们引入了第一个加锁实现：**两阶段加锁机制(Two-phase locking protocol)**。它要求事务在加锁的过程中遵循下面两步：



1）获取锁阶段(growing phase)：在这个过程中，事务只能不断地获取新的锁，但不能释放锁。

2）释放锁阶段(shrinking phase)：在这个过程中，事务只能逐渐释放锁，并且无权再获取新的锁。

**重要的事情说三遍：千万不要和两阶段提交(Two-phase commit (2PC))搞混；千万不要和两阶段提交搞混；千万不要和两阶段提交搞混。**两阶段提交是针对分布式事务的概念，我会在以后的文章中详细讲。

为了避免连锁回滚，我们可以引入两阶段提交的升级版：**严格的两阶段加锁(strict two-phase locking protocol)**。**除了需要遵循加锁和释放锁的两阶段外，它还规定，对于独占锁(X-lock)必须等到事务结束时才释放。**这个规定避免了其他事务对未提交的数据进行读写操作，因此也避免了连锁回滚。另一个更严格的升级本叫做**更严格的两阶段加锁(rigorous two-phase locking protocol)，**规定了所有获得的锁都得等到事务结束时才释放。

以上就是关于加锁的实现，如果我们总结一下，主要是介绍了这些内容：

1）引入共享锁(S-lock)和独占锁(X-lock)来获得对数据细粒度的控制；

2）引入两阶段加锁(Two-phase locking protocol)来保证数据的正确性；

3）两阶段加锁不能避免死锁，依然需要数据库系统来检查并破坏死锁，破坏死锁可以通过回滚相关的事务来进行；

4）两阶段加锁的两个升级版本：(更)严格的两阶段加锁(rigorous/strict two-phase locking)通过规定把释放锁等到事务结束来避免连锁回滚(cascading rollback)。



### 时间戳实现机制

使用时间戳记录事务开始的时间，根据时间戳为事务排序确定执行顺序。

为了避免两个时间戳一样可以使用计数的方法表示时间戳。

#### 实现

引入两个概念

1）W-timestamp(A): 记录对于数据 A，最近一次被某个事务修改的时间戳。

2）R-timestamp(A): 记录对于数据 A，最近一次被某个事务读取的时间戳。

一旦有一个更新的事务成功地对数据进行读取，相对应的读写时间戳就会被更新。

**对于事务 Ti 要读取数据 A read(A):**

1. 如果 TS(Ti) < W-timestamp(A)，说明 A 被一个 TS 比 Ti 更大的事务改写过，但 Ti 只能读取比自身 TS 小的数据。因此 Ti 的读取请求会被拒绝，Ti 会被回滚。
2. 如果 TS(Ti) > W-timestamp(A)，说明 A 最近一次被修改小于 TS(Ti)，因此读取成功，并且，R-timestamp(A)被改写为 TS(Ti)。

**对于事务 Ti 要修改数据 A write(A):**

1. 如果 TS(Ti) < R-timestamp(A)，说明 A 已经被一个更大 TS 的事务读取了，Ti 对 A 的修改就没有意义了，因此 Ti 的修改请求会被拒绝，Ti 会被回滚。
2. 如果 TS(Ti) < W-timestamp(A)，说明 A 已经被一个更大 TS 的事务修改了，Ti 对 A 的修改也没有意义了，因此 Ti 的修改请求会被拒绝，Ti 会被回滚。
3. 其他情况下，Ti 的修改会被接受，同时 W-timestamp(A)会被改写为 TS(Ti)。

一旦一个事务因为任何原因被回滚，再次重新执行时，会被系统分配一个新的 TS。



通过上述规则，系统就可以保证对于任意 Ti 和 Tj，如果 TS(Ti)<TS(Tj)，Ti 比 Tj 先运行完。我们通过一个示例来看时间戳是如何运行的。



假定下面两个事务 T1 和 T2，并且 TS(T1) < TS(T2)。

| T1:<br/>Read(B);<br/>Read(A);<br/>Display(A+B).<br/> | T2:<br/>Read(B)<br/>B= B- 50;<br/>Write(B);<br/>Read(A);<br/>A= A + 50;<br/>Write(A);<br/>Display(A+B). |
| ---------------------------------------------------- | ------------------------------------------------------------ |





## 多版本并发控制 (MVCC)

* [知乎](https://zhuanlan.zhihu.com/p/52977862)

 Multi-Version Concurrency Control

### 核心思想

* 写：每个事务的写操作都会创建一个新版本的数据快照
* 读：选择合适版本的数据进行读取

### 实现原理

#### 数据结构

* 数据表的隐藏列：得到当前版本数据的事务id、
* undo_log:存放每个历史版本的数据
* read_view:辅助读取数据
  * trx_ids（尚未提交commit的事务版本号集合）
  * up_limit_id（下一次要生成的事务ID值）
  * low_limit_id（尚未提交版本号的事务ID最小值）
  * creator_trx_id（当前的事务版本号）


在每行数据有两列隐藏的字段，分别是DB_TRX_ID（记录着当前ID）以及DB_ROLL_PTR（指向上一个版本数据在undo log 里的位置指针）

**up_limit_id**：当前已经提交的事务号 + 1，事务号 < up_limit_id ，对于当前Read View都是可见的。理解起来就是创建Read View视图的时候，之前已经提交的事务对于该事务肯定是可见的。

**low_limit_id**：当前最大的事务号 + 1，事务号 >= low_limit_id，对于当前Read View都是不可见的。理解起来就是在创建Read View视图之后创建的事务对于该事务肯定是不可见的。

**trx_ids**：为活跃事务id列表，即Read View初始化时当前未提交的事务列表。所以当进行RR读的时候，trx_ids中的事务对于本事务是不可见的（除了自身事务，自身事务对于表的修改对于自己当然是可见的）。理解起来就是创建RV时，将当前活跃事务ID记录下来，后续即使他们提交对于本事务也是不可见的。

![img](https://pic1.zhimg.com/v2-fef7954f5e3c7713f48b35597e7f9fb8_b.jpg)

### 读写流程

#### 写



```
为什么多版本并发控制更受欢迎呢？因为锁和时间戳机制都是通过阻塞或者回滚冲突的事务来确保事务的有序性。比如，一个读操作可能被迫回滚，因为它要读取的数据已经被另一个更新的事务修改了。但是，如果我们把每个数据的所有历史版本都记录下来，就可以避免上述这种情况发生。这也正是多版本控制的由来：对于每个数据 Q，每次写操作 write(Q)都会给 Q 建立一个新版本；而对于读操作 read(Q)，会根据事务的先后关系选择一个正确的版本去读取，来保证事务的有序性。多版本控制能够很好地解决这类读写冲突，尤其是长时间的读操作饿死写操作问题。
```

### 多版本时间戳

### 快照隔离(Snapshot Isolation)

```
快照隔离可以看作是对每一个事务，分配了一个独有的数据库快照。事务可以安心地读取这个快照中的数据而不需要去担心其他事务(因此只读事务是不会失败也不会被等待的)。同理，事务对数据的更新也首先暂存在这个独有的快照中，只有当事务提交的时候，这些更新才会试图被写回真正的数据库版本里。当一个事务准备提交时，它依然要确保没有其他事务更新了它所更新过的数据，否则，这个事务会被回滚
```

## 多版本时间戳(Multi-Version Timestamp Ordering)

把时间戳和多版本控制结合就形成了多版本时间戳机制。对于每个事务 Ti，系统都会设置相应的事务时间 TS(Ti)。对于每个数据单元 Q，系统会保存一系列的版本数据 Q1，Q2，Q3，… Qn。其中，每个版本 Qx 保存以下信息：



1. 当前版本的数据值
2. W-TS(Qx): 当 Qx 被某个事务 Ti 创建的时间戳，即 TS(Ti)
3. R-TS(Qx): 由于一个版本的数据可以被多个事务读取，这里存储的是最大的事务时间戳：最近一次被某个事务 Tj 读取成功的时间戳，即 TS(Tj)



当一个事务 Tx 创建了数据 Q 版本 Qk，Qk 会保存 Tx 所写入的数据值，同时，系统会把 W-TS(Qk)和 R-TS(Qk)都初始为 TS(Tx)；当有另一个事务 Ty 并且 TS(Ty) > TS(Tx)读取 Qk，系统会更新 R-TS(Qk)至 TS(Ty)。



现在介绍详细的操作机制：给定当前事务 Ti 对数据 Q 发起了一个读操作 read(Q)或者写操作 write(Q)。并假定，版本 Qk 是 Q 的所有版本中持有最大的但小于或等于 TS(Ti)的 W-TS 的时间戳。则：



1. 如果 Ti 是读操作，则读取成功，返回 Qk 中的值给 Ti。
2. 如果 Ti 是写操作，则需要判断，如果 TS(Ti) < R-TS(Qx)，即说明有一个更新的事务已经读取了数据，因此系统判定更新失败，回滚 Ti。如果 TS(Ti) = W-TS(Qx)，系统可以直接将 Ti 的值覆盖 Qk 的原值；如果 TS(Ti) > R-TS(Qx)，则创建一个新的版本 Q。



规则一很容易理解，一个事务可以读取到在它看来最新的数据。规则二则保证了一个事务被需要被撤销，如果已经有更新的事务读取了某个版本。



多版本时间戳机制的一大好处在于，一个读取数据的事务永远不会失败也不需要等待。在一个读多写少的场景下，相比于先前介绍的两种机制，会有很好的性能提升。



当然，它也是有缺点的。首先，就是在读取操作的事务中，也需要更新相应的 R-TS(Qk)，并且读取数据，这就导致可能产生两次磁盘操作，而非只读一次。另外，当写操作发生冲突时，它会要求回滚失败的事务，相比起等待，回滚操作可能更昂贵一些。下面介绍的另一种的实现可以解决这个问题。



## 多版本两阶段加锁(Multi-Version Two-Phase Locking)

多版本两阶段加锁机制，相比于上文介绍的多版本时间戳机制，是要集多版本控制和两阶段加锁之所长。它会区分对待只读操作的事务和有更新操作的事务。



有更新数据的事务会遵守两阶段加锁的规则，即事务需要持有所有的锁直至事务结束。这样，这些事务就能够保证有序性(在介绍 [两阶段加锁](https://www.infoq.cn/article/KyZjpzySYHUYDJa2e1fS)的时候已经讲解过)。这样做的好处在于不同的写事务可以等待并且按照顺序依次完成而不需要回滚后重试。对于只读操作，和上文介绍的多版本时间戳机制一样。不同的是，在多版本两阶段加锁中，事务的时间不再是时间戳，而是表现相对时序的事务计数 TS-Counter。这个 TS-Counter 在每次事务提交时被更新。



对于只读操作的事务 Ti，数据库系统会把 TS(Ti)赋于当前 TS-Counter 的值，这样 Ti 读取数据就和上面介绍的多版本时间戳一样，会读取到最大的但小于或等于 TS(Ti)的 Q 版本的值。对于有更新操作的事务，如果要读取一个数据，首先，它会获取这个数据的共享锁，并且读取最新版本的数据。当事务需要写数据时，首先要获取数据的独占锁并且创建一个新的版本，并把版本的时间戳设置为无穷大，当这个事务要被提交时，把它锁创建的所有数据的新版本的时间戳设置为 TS-Counter+1，并且同时更新系统的 TS-Ccounter，也变为 TS-Counter+1。



## 多版本并发控制的缺陷

天下没有免费的午餐，我们来讨论它有什么缺陷。



1. 额外的存储和计算资源支出：首先，需要额外存储历史版本数据，并且在执行时，每个事务要快速定位到正确的版本，并且对于更新的事务，通常情况会复制一份或者创建一个新的版本来暂存数据，这些都是需要消耗存储和计算资源的。当然，数据库系统可以定期对老的数据版本进行清理来释放存储空间。
2. 多线程竞争时间戳分配：由于需要保证不同事务的有序性，因此需要有一个共享的时间戳实现来分配(无论是用时间，还是相对的 counter)，免不了不同的事务线程需要去竞争时间戳。
3. 有些情况下，会导致频繁的事务回滚：特别当事务之间存在大量竞争的时候，会造成频繁的事务回滚。



总结一下，我们介绍了两种具体的多版本并发控制的实现，多版本时间戳机制和多版本两阶段加锁，两者都保证了对于只读操作的事务，不会失败也不会被等待。区别在于写操作的事务，多版本时间戳机制会回滚“迟到”的写事务，而多版本两阶段加锁通过共享和独占锁来对多个写操作事务排序。同时，我们也讨论了一些多版本并发控制的缺陷。但是，瑕不掩瑜，它依然是最常见的并发控制实现。



回忆一下在[第十期](https://www.infoq.cn/article/teJA7X43BO2alp6rLCWk)介绍的隔离级别：读未提交；读提交；可重复读和可有序化。那用多版本并发控制实现了哪个隔离级别呢？读者可能会觉得，应该是可有序化。但其实不然，它实现了一个新的隔离级别叫做 Snapshot Isolation(快照隔离)。



## 快照隔离(Snapshot Isolation)

快照隔离可以看作是对每一个事务，分配了一个独有的数据库快照。事务可以安心地读取这个快照中的数据而不需要去担心其他事务(因此只读事务是不会失败也不会被等待的)。同理，事务对数据的更新也首先暂存在这个独有的快照中，只有当事务提交的时候，这些更新才会试图被写回真正的数据库版本里。当一个事务准备提交时，它依然要确保没有其他事务更新了它所更新过的数据，否则，这个事务会被回滚。



那为什么说快照隔离是一个单独的隔离级别而不是可有序化呢？问题就在于，它提供了“太多”的隔离性(英语中用 too much！貌似更形象一些)。我想借用 CMU 数据库教授 Andy Pavlo 课里举过的一个非常贴切的例子，我们现在假设数据就是围棋的棋子，一部分是黑子，一部分是白子。现在同时有两个更新事务：T1: 把所有的白子变成黑子(写成 SQL 语句可以看作是这样的： **UPDATE color = ‘black’ FROM marbles WHERE color = ‘white’** )。T2:把所有的黑子变成白子。执行这两个操作会怎么样呢？由于快照隔离(多版本并发控制)机制，这两个事务更新的数据不一样，因此都会视为成功。这就导致了最终，白子和黑子的颜色互换。见下图示例。



![img](https://static001.infoq.cn/resource/image/79/ce/79ae28d92c24970f2ee1eeb8ff6590ce.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



但是，根据可有序化的定义，要确保不同的事务最终是可以沿着时间线排成一溜执行，因此无论是 T1 先执行还是 T2 先执行，最终的颜色应该全是黑色或是白色，如下图所示。



![img](https://static001.infoq.cn/resource/image/42/58/42e7b461db5c7d01327374643f309b58.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



上述的示例被称为 Write Skew Anomaly。因此，快照隔离是一个区别于可有序化的隔离级别，如果把它安插在我们介绍过的隔离级别，应该如下图所示。



![img](https://static001.infoq.cn/resource/image/06/4e/0637yy401860a95bf8c2d70e2092544e.png)



(Credit to https://15721.courses.cs.cmu.edu/spring2020/slides/03-mvcc1.pdf)



大部分的数据库都支持快照隔离。Orcale 和 PostgreSQL 数据库其实是使用快照隔离机制来实现可有序化机制。因此，在极端小概率情况下，数据库的状态是有可能“非有序化”的。

## 各种锁

* [详解 MySql InnoDB 中的三种行锁（记录锁、间隙锁与临键锁）](https://juejin.cn/post/6844903666420285454)

### 记录锁（Record Locks）

顾名思义，记录锁就是为**某行**记录加锁，它`封锁该行的索引记录`：

```sql
SELECT * FROM table WHERE id = 1 FOR UPDATE;
```

需要注意的是：`id` 列必须为`唯一索引列`或`主键列`，否则上述语句加的锁就会变成`临键锁`。

### 间隙锁（Gap Locks）

**间隙锁**基于`非唯一索引`，它`锁定一段范围内的索引记录`。**间隙锁**基于下面将会提到的`Next-Key Locking` 算法，请务必牢记：**使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据**。

```java
SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE;
```

即所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。

除了手动加锁外，在执行完某些 SQL 后，InnoDB 也会自动加**间隙锁**，这个我们在下面会提到。

### 临键锁（Next-Key Locks）

Next-Key 可以理解为一种特殊的**间隙锁**，也可以理解为一种特殊的**算法**。通过**临建锁**可以解决`幻读`的问题。 每个数据行上的`非唯一索引列`上都会存在一把**临键锁**，当某个事务持有该数据行的**临键锁**时，会锁住一段**左开右闭区间**的数据。需要强调的一点是，`InnoDB` 中`行级锁`是基于索引实现的，**临键锁**只与`非唯一索引列`有关，在`唯一索引列`（包括`主键列`）上不存在**临键锁**。

**如果你的 where 条件不加主键，那么 innodb 的行级锁就可能变成表级锁。如果升级为表级锁，那么并发性就将大打折扣了。**

## 总结

至此，事务、隔离和并发就全部介绍完毕。我们分别介绍了事务的 ACID 属性以及不同的隔离级别，再依次介绍了不同的并发控制实现，两阶段加锁，时间戳机制，和多版本并发控制。



对于单机的数据库系统就介绍得差不多了。下一篇文章，我们聊一个非常有意思的话题：假设给你一个单机的数据库系统实现，要求在这个基础上把它扩建成分布式数据库系统，你会怎么做？这个问题还有个小故事。很久很久以前，在原来公司参与系统设计面试的时候，候选人和我说，我原本准备的面试题另一个面试官已经问过了(当时我的内心是崩溃的…)。这是我临时想出来的面试题，留给大家做思考

# 慢查询

**参考**

* [基于代价的慢查询优化建议 (qq.com)](https://mp.weixin.qq.com/s/MaQTI4afIh2Zehc-F-iisQ)
* [MySQL索引原理及慢查询优化 - 美团技术团队 (meituan.com)](https://tech.meituan.com/2014/06/30/mysql-index.html)

**查询时间超过指定阈值的查询语句**

查询速度比较慢的sql语句

**定位慢查询**

## 慢查询优化基本步骤

0. 先运行看看是否真的很慢，注意设置SQL_NO_CACHE

1. where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高

2. explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）

3. order by limit 形式的sql语句让排序的表优先查

4. 了解业务方使用场景

5. 加索引时参照建索引的几大原则

6. 观察结果，不符合预期继续从0分析

1. 开启慢查询日志`set global slow_query_log=1;`

2. 设置慢查询阈值`set long_query_time=1;`

3. 增大慢查询日志范围，纳入管理语句`set global log_slow_admin_statements=ON;`

4. 分析慢查询日志

   * 结果最多的语句

     ```sql
     mysqldumpslow -s r -t 10 /usr/local/mysql/data/localhost_slow.log
     ```

   * 耗时最多的语句

     ```sql
     mysqldumpslow -s t -t 10 /usr/local/mysql/data/localhost_slow.log
     ```

     

## 解决方法

选用一个查询效率高的索引，两种算法

* **基于经验规则**   适合简单语句，直观就能给出索引建立方法
* **基于代价**  往往用在复杂查询、多表查询等复杂场景，量化评估所有可能性，最终选出最优方案

对于优化器来说，执行一条语句往往有多种选择

* 是否用索引
* 用哪个索引
* 是否选择范围扫描
* 多表join的连接顺序
* 子查询的执行方式

如何选择最优的？建立量化评价指标cost,分别计算候选方案耗时，选出最佳方案

## 代价模型

### 操作分层

#### server层

主要是CPU代价，

#### Engine层

主要是IO代价

* 从磁盘读取一个数据页的代价
* 内存临时表的创建代价
* 内存临时表的行代价
* 键比较的代价

### 基于代价的索引选择





**索引没起作用**

1.  使用LIKE关键字的查询语句

   在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。只有“%”不在第一个位置索引才会起作用。

2. 使用多列索引的查询语句

    MySQL可以为多个字段创建索引。一个索引最多可以包括16个字段。对于多列索引，只有查询条件使用了这些字段中的第一个字段时，索引才会被使用。

**优化数据库结构**

1. 字段太多分表
2. 建立中间表，将需要连接查询的数据放进中间表

**分解关联查询**

将一个大的查询分解为多个小查询是很有必要的。

**优化LIMIT分页**

# 一致性

四种同步策略

| 策略         | 优点           | 缺点                                       |
| ------------ | -------------- | ------------------------------------------ |
| 修改缓存     | 减少查询未命中 | 消耗比较大，不适合频繁写入场景，因为没人读 |
| 删除缓存     | 操作简单       | 下次查询需要访问数据库                     |
| 先删除缓存   |                | 修改数据库失败导致缓存和数据库都出错       |
| 先修改数据库 |                | 删缓存失败导致数据不一致                   |

**最优方案**：先更新数据库，再删除缓存

删除缓存失败解决办法：**重试机制**



