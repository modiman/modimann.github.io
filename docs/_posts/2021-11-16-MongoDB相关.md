### 背景

当系统增长到以下两种情形时，可以考虑使用分片：

1. 单台服务的硬盘容量或者内存容量无法满足需求
2. **单台服务器的吞吐达不到要求**



### 分片集群

* 分片（sharding）是MongoDB用来将大型集合分割到不同服务器（或者说一个集群）上所采用的方法。

* 使用分片减少了每个分片需要处理的请求数，通过水平扩展，集群可以提高自己的存储容量和吞吐量。例如，如果数据库1tb的数据集，并有4个分片，然后每个分片可能仅持有256 GB的数据。如果有40个分片，那么每个切分可能只有25GB的数据。

### 优势

* MongoDB自带了一个叫做mongos的专有路由进程。mongos就是掌握统一路口的路由器，其会将客户端发来的请求准确无误的路由到集群中的一个或者一组服务器上，同时会把接收到的响应拼装起来发回到客户端
* 保证集群总是可读写。将MongoDB的分片和复制功能结合使用，在确保数据分片到多台服务器的同时，也确保了每分数据都有相应的备份，这样就可以确保有服务器坏掉时，其他的从库可以立即接替坏掉的部分继续工作。
* 集群易于扩展。当系统需要更多的空间和资源的时候，MongoDB使我们可以按需方便的扩充系统容量。

### 架构

* Mongos

提供对外应用访问，所有操作均通过mongos执行。

* Config Server

l存储集群所有节点、分片数据路由信息，默认需要配置3个Config Server节点。

* Shard

数据分片，存储应用数据记录，一般有多个MongoDB服务节点，达到数据分片目的。

### **集群中的数据分布**

![image-20220811142152501](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142152501.png)

![image-20220811142158035](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142158035.png)

* 在一个shard server内部，MongoDB会把数据分为chunks，每个chunk代表这个shard server内部一部分数据。chunk的产生，会有以下两个用途：

* Splitting：当一个chunk的大小超过配置中的chunk size（默认64M）时，MongoDB的后台进程会把这个chunk切分成更小的chunk，从而避免chunk过大的情况。

* Balancing：balancer是一个后台进程，负责chunk的迁移，从而均衡各个shard server的负载，系统初始1个chunk，mongoDB会自动拆分和迁移chunks。

### 搭建集群

下面我们搭建一个分片式的集群，包括：1个Mongos服务，3个Config Server，2个Shard，每个Shard中包含3个MongoDB实例，其为副本集，保障数据的安全。具体方法参考《搭建MongoDB分片式集群.md》文档。

![image-20220811142703138](E:\gitfile\modiman.github.io\docs\_posts\imgs\image-20220811142703138.png)

```shell
#创建3个config节点
docker create --name configsvr01  -p 17000:27019 -v mongoconfigsvr-data-01:/data/configdb mongo:4.0.3 --configsvr --replSet "rs_configsvr"  --bind_ip_all

docker create --name configsvr02  -p 17001:27019 -v mongoconfigsvr-data-02:/data/configdb mongo:4.0.3 --configsvr --replSet "rs_configsvr"  --bind_ip_all

docker create --name configsvr03  -p 17002:27019 -v mongoconfigsvr-data-03:/data/configdb mongo:4.0.3 --configsvr --replSet "rs_configsvr"  --bind_ip_all

#启动服务
docker start configsvr01 configsvr02 configsvr03

#进去容器进行操作
docker exec -it configsvr01 /bin/bash
mongo 192.168.31.81:17000

#集群初始化
rs.initiate(
  {
    _id: "rs_configsvr",
    configsvr: true,
    members: [
      { _id : 0, host : "192.168.31.81:17000" },
      { _id : 1, host : "192.168.31.81:17001" },
      { _id : 2, host : "192.168.31.81:17002" }
    ]
  }
)

#创建2个shard分片，每个分片都有3个数据节点

#集群一
docker create --name shardsvr01  -p 37000:27018 -v mongoshardsvr-data-01:/data/db mongo:4.0.3 --replSet "rs_shardsvr1" --bind_ip_all --shardsvr
docker create --name shardsvr02  -p 37001:27018 -v mongoshardsvr-data-02:/data/db mongo:4.0.3 --replSet "rs_shardsvr1" --bind_ip_all --shardsvr
docker create --name shardsvr03  -p 37002:27018 -v mongoshardsvr-data-03:/data/db mongo:4.0.3 --replSet "rs_shardsvr1" --bind_ip_all --shardsvr

#集群二
docker create --name shardsvr04  -p 37003:27018 -v mongoshardsvr-data-04:/data/db mongo:4.0.3 --replSet "rs_shardsvr2" --bind_ip_all --shardsvr
docker create --name shardsvr05  -p 37004:27018 -v mongoshardsvr-data-05:/data/db mongo:4.0.3 --replSet "rs_shardsvr2" --bind_ip_all --shardsvr
docker create --name shardsvr06  -p 37005:27018 -v mongoshardsvr-data-06:/data/db mongo:4.0.3 --replSet "rs_shardsvr2" --bind_ip_all --shardsvr

#启动容器
docker start shardsvr01 shardsvr02 shardsvr03
docker start shardsvr04 shardsvr05 shardsvr06

#进去容器执行
docker exec -it shardsvr01 /bin/bash
mongo 192.168.31.81:37000

#初始化集群
rs.initiate(
  {
    _id: "rs_shardsvr1",
    members: [
      { _id : 0, host : "192.168.31.81:37000" },
      { _id : 1, host : "192.168.31.81:37001" },
      { _id : 2, host : "192.168.31.81:37002" }
    ]
  }
)

#初始化集群二
mongo 192.168.31.81:37003

rs.initiate(
  {
    _id: "rs_shardsvr2",
    members: [
      { _id : 0, host : "192.168.31.81:37003" },
      { _id : 1, host : "192.168.31.81:37004" },
      { _id : 2, host : "192.168.31.81:37005" }
    ]
  }
)

#创建mongos节点容器，需要指定config服务
docker create --name mongos -p 6666:27017 --entrypoint "mongos" mongo:4.0.3 --configdb rs_configsvr/192.168.31.81:17000,192.168.31.81:17001,192.168.31.81:17002 --bind_ip_all

docker start mongos

#进入容器执行
docker exec -it mongos bash
mongo 192.168.31.81:6666

#添加shard节点
sh.addShard("rs_shardsvr1/192.168.31.81:37000,192.168.31.81:37001,192.168.31.81:37002")
sh.addShard("rs_shardsvr2/192.168.31.81:37003,192.168.31.81:37004,192.168.31.81:37005")

#启用分片
sh.enableSharding("geoserver")

#设置分片规则，按照_id的hash进行区分
sh.shardCollection("geoserver.tb_route_point", {"userId": "hashed" })

#插入测试数据
use geoserver

for (i = 1; i <= 1000; i=i+1){
    db.tb_route_point.insert({"userId" : i, "speed" : 10.21, "longitude" : 121.612063, "latitude" : 31.034952, "routeId" : "abc1231", "created" : NumberLong("1626853471529")})
}

#分别在2个shard集群中查询数据进行测试
db.tb_route_point.count()


#集群操作（在mongos中执行）
use config
db.databases.find()  #列出所有数据库分片情况
db.collections.find() #查看分片的片键
sh.status()  #查询分片集群的状态信息
```
