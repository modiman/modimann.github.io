# 数据集

## ogbl-biokg

### 概述

ogbl-biokg数据集是一个知识图(kg)，它是我们使用大量生物医学数据仓库的数据创建的。它包含5种类型的实体:

| 结点类型 | 疾病  | 蛋白质 | 药物  | 副作用 | 蛋白质功能 |
| -------- | ----- | ------ | ----- | ------ | ---------- |
| 结点数量 | 10687 | 17499  | 10533 | 9969   | 45085      |

### 关系


连接两类实体的有向关系共有51种，包括39种药物-药物相互作用、8种蛋白质-蛋白质相互作用，以及药物-蛋白质、药物-副作用、药物-蛋白质、功能-功能关系。所有关系都被建模为有向边，其中连接相同实体类型(例如，蛋白质-蛋白质、药物-药物、功能-功能)的关系总是对称的，即边是双向的。

### 研究意义

该数据集与生物医学和基础ML研究都相关。在生物医学方面，该数据集使我们能够更好地了解人类生物学，并生成可以指导下游生物医学研究的预测。在最基本的最大似然方面，数据集在处理可能有矛盾观测值的噪声、不完整的KG时带来了挑战。这是因为ogbl-biokg数据集涉及从分子规模(例如，细胞内的蛋白质-蛋白质相互作用)到整个群体(例如，特定国家患者经历的不良副作用的报告)的异质性相互作用。此外，KG中的三元组来自具有各种置信水平的来源，包括实验读数、人工辅助注释和自动提取的元数据。

### 预测任务:

任务是预测给定训练三元组的新三元组。评估协议与ogbl-wiki G2完全相同，只是这里我们只考虑针对相同类型的实体进行排名。例如，当腐蚀蛋白质类型的头部实体时，我们只考虑负蛋白质实体。

### 数据集分割

对于这个数据集，我们采用随机分割。虽然根据时间分割三元组是一个有吸引力的选择，但我们注意到，获得关于三元组背后的单个实验和观察是何时进行的准确信息是非常具有挑战性的。我们努力在OGB的未来版本中提供额外的数据集分割



### 数据载入方法

```python
from ogb.linkproppred import PygLinkPropPredDataset

dataset = PygLinkPropPredDataset(name = d_name) 

split_edge = dataset.get_edge_split()
train_edge, valid_edge, test_edge = split_edge["train"], split_edge["valid"], split_edge["test"]
graph = dataset[0] # pyg graph object containing only training edges
```

dataset存储形式

```json
dataset[0]=
{
    'edge_index_dict':
    {
        ('disease', 'disease-protein', 'protein'): array([[ 1718, ...,   1198],  [ 3207...,1962]]), 
		('drug', 'drug-disease', 'disease'): array([[1411 ...,  679], [1402, ...,  2416]]),
			...(共51行，对应51个关系)  

    }，
	'edge_feat_dict': None,
	'node_feat_dict': None,
	'num_nodes_dict': {'disease': 10687, 'drug': 10533, 'function': 45085, 'protein': 17499, 'sideeffect': 9969},
		
	'edge_reltype': {('disease', 'disease-protein', 'protein'): array([[0],  [0],  ...
       [0]]), ('drug', 'drug-disease', 'disease'): array([[1],       [1],...
                                                          
 }

}
dataset只包含一张图biokg，故长度为1;
dataset[0]包含5个key                                                   
```

### 变量名解释

The library-agnostic graph object is a dictionary containing the following keys: `edge_index`, `edge_feat`, `node_feat`, and `num_nodes`, which are detailed below.

- `edge_index`:形状为(2, num_edges)的numpy数组，一列表示一个边，第一行、第二行表示头、尾节点的索引，无向边表示为双向边
- `edge_feat`: 形状为(num_edges, edgefeat_dim)的numpy数组， `edgefeat_dim`是边特征的维度，一行表示一个边的特征，如果没有输入，
- `node_feat`: numpy ndarray of shape `(num_nodes, nodefeat_dim)`, where `nodefeat_dim` is the dimensionality of node features and i-th row represents the feature of i-th node. This can be `None` if no input node features are available.
- `num_nodes`: 图节点数量

**Heterogeneous graph:** We represent a heterogeneous graph using dictionaries: `edge_index_dict`, `edge_feat_dict`, `node_feat_dict`, and `num_nodes_dict`.

- `edge_index_dict`:将三元组映射为与 `edge_index`.相关的字典
- `edge_feat_dict`: A dictionary mapping each triplet `(head type, relation type, tail type)` into corresponding `edge_feat`.
- `node_feat_dict`: A dictionary mapping each `node type` into corresponding `node_feat`.
- `num_nodes_dict`: A dictionary mapping each `node type` into corresponding `num_nodes`.

**Note:** 一些图形数据集可能在节点或边中包含额外的元信息，例如它们的时间戳。尽管它们没有作为默认输入特征给出，但研究人员应该可以随意利用这些附加信息

### 性能评估

​     评估器是为每个数据集定制的。我们要求用户将预先指定的格式传递给评估者。首先，请学习评估器的输入输出格式规范如下。



```python
from ogb.linkproppred import Evaluator
evaluator = Evaluator(name = d_name)
print(evaluator.expected_input_format) 
print(evaluator.expected_output_format) 
```



### 环境 服务器149

* python 3.7.11
* torch 1.7.1
* cuda 11.4
* ogb 1.3.2

# 1.Auto-SF 

##  1.1论文部分

### 概述 

* 数据集 OGB-biokg
* 消耗显存  9G
* 完全复现



## 1.2代码部分(run.py)

### 命令行参数

* '--cuda', action='store_true', help='use GPU'
* '--do_train', action='store_true'
* '--do_valid', action='store_true'
* '--do_test', action='store_true'
* '--evaluate_train', action='store_true', help='Evaluate on training data'
* '--dataset', type=str, default='ogbl-biokg', help='dataset name, default to biokg'
* '--model', default='TransE', type=str)
* '-de', '--double_entity_embedding', action='store_true')
* '-dr', '--double_relation_embedding', action='store_true')
* '-n', '--negative_sample_size', default=128, type=int)
* '-d', '--hidden_dim', default=500, type=int)
* '-g', '--gamma', default=12.0, type=float
* '-adv', '--negative_adversarial_sampling', action='store_true'
* '-a', '--adversarial_temperature', default=1.0, type=float
* '-b', '--batch_size', default=1024, type=int
* '-r', '--regularization', default=0.0, type=float
* '--test_batch_size', default=4, type=int, help='valid/test batch size'
* '--uni_weight', action='store_true',  help='Otherwise use subsampling weighting like in word2vec'
* '-lr', '--learning_rate', default=0.0001, type=float
* '-cpu', '--cpu_num', default=10, type=int
* '-randomSeed', default=0, type=int)
* parser.add_argument('-init', '--init_checkpoint', default=None, type=str)
* parser.add_argument('-save', '--save_path', default=None, type=str)parser.a
* dd_argument('--max_steps', default=100000, type=int)
* parser.add_argument('--warm_up_steps', default=None, type=int
* parser.add_argument('--save_checkpoint_steps', default=10000, type=int)
* parser.add_argument('--valid_steps', default=10000, type=int)
* parser.add_argument('--log_steps', default=100, type=int, help='train log every xx steps')
* parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')
* parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')
* parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')
* parser.add_argument('--print_on_screen', action='store_true', help='log on screen or not
* parser.add_argument('--ntriples_eval_train', type=int, default=200000, help='number of training triples to evaluate ventually'
* parser.add_argument('--neg_size_eval_train', type=int, default=500, help='number of negative samples when evaluating training triples')

### defaultdict

```python
from collections import defaultdict

train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)
```

defaultdict区别于普通的dict
对于普通的dict，调用不存在的键值对时会报错

```python
>>> a = dict()
>>> a['name'] = 'modige'
>>> print(a['name'])
modige
>>> print(a['age'])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyError: 'age'

```

但对于defaultdict

```python
>>> b = defaultdict(list)
>>> print(b['age'])
[]
>>> c = defaultdict(tuple)
>>> print(c['age'])
()
```

可以看出，但调用不存在的key时会返回一个形参类型的空结果

### tqdm

Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)。



使用方法

```python
    for i in tqdm(range(len(train_triples['head']))):
        head, relation, tail = train_triples['head'][i], train_triples['relation'][i], train_triples['tail'][i]
        head_type, tail_type = train_triples['head_type'][i], train_triples['tail_type'][i]
        train_count[(head, relation, head_type)] += 1
        train_count[(tail, -relation-1, tail_type)] += 1
        train_true_head[(relation, tail)].append(head)
        train_true_tail[(head, relation)].append(tail)
```

样式如下

100%|██████████| 4762678/4762678 [00:14<00:00, 335575.16it/s]

或



```python
from tqdm import tqdm
for i in tqdm(range(1000)):
 #do something
 pass
```

### argparse

* **https://docs.python.org/3/library/argparse.html**

命令行选项、参数和子命令的解析器

#### 打印现有参数

```pyhton
python a.py -h
#输出
usage: a.py [-h] echo ecdho

positional arguments:
  echo        dddd
  ecdho       dddd

optional arguments:
  -h, --help  show this help message and exit
```

#### 常用关键字

##### **help**

存放对参数的解释

#####  **action**

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--verbose", help="increase output verbosity",
                    action="store_true")
args = parser.parse_args()
if args.verbose:
    print("verbosity turned on")
```

给关键字action赋值"store_true"，这意味着，如果指定了选项，则将值 True 分配给 args.verbose。不指定它意味着 False。

##### type

指定参数的数据类型

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("square", help="display a square of a given number",
                    type=int)
args = parser.parse_args()
print(args.square**2)
```

##### 短指令

可以使用```-v```代替```--verbose```

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("-v", "--verbose", help="increase output verbosity",
                    action="store_true")
args = parser.parse_args()
if args.verbose:
    print("verbosity turned on")
```

##### default 

不指定参数时的默认值

```python
parser.add_argument('--valid_steps', default=10000, type=int)
```

执行时没有加```--valid_steps```使用默认值10000



```python
import argparse
def parse_args(args=None):
    parser = argparse.ArgumentParser(
        description='Training and Testing Knowledge Graph Embedding Models',
        usage='train.py [<args>] [-h | --help]'
    )

    parser.add_argument('--cuda', action='store_true', help='use GPU')
    parser.add_argument('--dataset', type=str, default='ogbl-biokg', help='dataset name, default to biokg')
    ...
    return parser.parse_args(args)
```

### 主程序（main函数）逻辑

1. 判断是否指定训练/测试/验证mode(缺一不可)，没有则报错并给出提示
2. 检查是否需要从断点处恢复模型
3. 设置模型保存路径
4. 设置randomseed  
5. 载入数据集到内存
6. 分割数据集
7. 设置评价协议（ ogb自带函数）



## 1.3 代码部分（model.py）

**参数 **

* ```model_name```模型
* ```nentity```实体数量
* ```nrelation```关系数量
* ```hidden_dim ```隐藏层维度
* ```epsilon```
* ```gamma```
* ```embedding_range```
* `entity_embedding`   实体特征矩阵
* `relation_embedding`关系特征矩阵

### 1.3.1 def forward()

```python
def forward(self, sample, mode='single'):
    '''
        Forward function that calculate the score of a batch of triples.
        In the 'single' mode, sample is a batch of triple.
        In the 'head-batch' or 'tail-batch' mode, sample consists two part.
        The first part is usually the positive sample.
        And the second part is the entities in the negative samples.
        Because negative samples and positive samples usually share two elements 
        in their triple ((head, relation) or (relation, tail)).
        '''   
正向传播函数计算一批三元组的打分。
single模式下样本是一批三元组
head-batch 和 tail-batch模式下，样本包含两部分：
第一部分通常是正例样本。
第二部分是阴性样本中的实体。
因为阴性样本和阳性样本三元组通常共享两种元素((head, relation) or (relation, tail)).

```

#### torch.index_select

```
torch.index_select(input, dim, index, out=None) → Tensor
```

​              沿着指定维度对输入进行切片，取`index`中指定的相应项(`index`为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量_Tensor_有相同的维度(在指定轴上)。

注意： 返回的张量不与原始张量共享内存空间。

参数:

- input (Tensor) – 输入张量
- dim (int) – 索引的轴
- index (LongTensor) – 包含索引下标的一维张量
- out (Tensor, optional) – 目标张量

例子：

```python
>>> x = torch.randn(3, 4)
>>> x

 1.2045  2.4084  0.4001  1.1372
 0.5596  1.5677  0.6219 -0.7954
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

>>> indices = torch.LongTensor([0, 2])
>>> torch.index_select(x, 0, indices)

 1.2045  2.4084  0.4001  1.1372
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 2x4]

>>> torch.index_select(x, 1, indices)

 1.2045  0.4001
 0.5596  0.6219
 1.3635 -0.5414
[torch.FloatTensor of size 3x2]

import torch
# 创建1D张量
a = torch.arange(0, 9)
print(a)

# 获取1D张量的第1个维度且索引号为2和3的张量子集
print(torch.index_select(a, dim = 0, index = torch.tensor([2, 3])))
# 创建2D张量
b = torch.arange(0, 9).view([3, 3])
print(b)

# 获取2D张量的第2个维度且索引号为0和1的张量子集(第一列和第二列)
print(torch.index_select(b, dim = 1, index = torch.tensor([0, 1])))
# 创建3D张量
c = torch.arange(0, 9).view([1, 3, 3])
print(c)
# 获取3D张量的第1个维度且索引号为0的张量子集
print(torch.index_select(c, dim = 0, index = torch.tensor([0])))
```



#### 调用打分函数

```python
model_func = {
    'TransE': self.TransE,
    'DistMult': self.DistMult,
    'ComplEx': self.ComplEx,
    'RotatE': self.RotatE,
    'AutoSF': self.AutoSF,
}

if self.model_name in model_func:
    # 调用函数的方法
    # model_func是一个字典 键是字符串，值是函数名
    score = model_func[self.model_name](head, relation, tail, mode)
```



### sample格式

```
n行三列

h,r,t
h,r,t
h,r,t
h,r,t
。。。
```

实体与关系嵌入维度相同，默认为hidden_dim

### Tuned hyper-parameters

| name                    | biokg                          | wikikg2                        |
| ----------------------- | ------------------------------ | ------------------------------ |
| learning rate           | [1e-4, 3e-4, 1e-3, 3e-3, 1e-2] | [1e-4, 3e-4, 1e-3, 3e-3, 1e-2] |
| L2 regularization       | [1e-7, 3e-7, 1e-8, 3e-8, 1e-6] | [1e-7, 3e-7, 1e-8, 3e-8, 1e-6] |
| gamma                   | [50, 100, 200]                 | [50, 100, 200]                 |
| batch_size              | [1024, 2048]                   | [1024, 2048]                   |
| embedding dimension     | [1000, 2000]                   | [100, 200]                     |
| #negative samples       | [64, 128, 256, 512, 1024]      | [64, 128, 256, 512, 1024]      |
| adversarial temperature | [0.2, 0.5, 1, 1.5, 2, 3]       | [0.2, 0.5, 1, 1.5, 2, 3]       |



### 1.3.2 init函数

初始化模型参数

#### torch.nn.Parameter()

`Variable`的一种，常被用于模块参数(`module parameter`)。

`Parameters` 是 `Variable` 的子类。`Paramenters`和`Modules`一起使用的时候会有一些特殊的属性，即：当`Paramenters`赋值给`Module`的属性的时候，他会自动的被加到 `Module`的 参数列表中(即：会出现在 `parameters() 迭代器中`)。将`Varibale`赋值给`Module`属性则不会有这样的影响。 这样做的原因是：我们有时候会需要缓存一些临时的状态(`state`), 比如：模型中`RNN`的最后一个隐状态。如果没有`Parameter`这个类的话，那么这些临时变量也会注册成为模型变量。

`Variable` 与 `Parameter`的另一个不同之处在于，`Parameter`不能被 `volatile`(即：无法设置`volatile=True`)而且默认`requires_grad=True`。`Variable`默认`requires_grad=False`。

参数说明:

- data (Tensor) – parameter tensor.
- requires_grad (bool, optional) – 默认为`True`，在`BP`的过程中会对其求微分。

#### torch.nn.init.uniform

```
torch.nn.init.uniform(tensor, a=0, b=1)
```

从均匀分布U(a, b)中生成值，填充输入的张量或变量

**参数：**

- **tensor** - n维的torch.Tensor
- **a** - 均匀分布的下界
- **b** - 均匀分布的上界

**例子**

```python
>>> w = torch.Tensor(3, 5)
>>> nn.init.uniform(w)
```



#### torch.norm

```
torch.norm(input, p=2) → float
```

返回输入张量`input` 的p 范数。

参数：

- input (Tensor) – 输入张量
- p (float,optional) – 范数计算中的幂指数值

例子：

```
>>> a = torch.randn(1, 3)
>>> a

-0.4376 -0.5328  0.9547
[torch.FloatTensor of size 1x3]

>>> torch.norm(a, 3)
1.0338925067372466
torch.norm(input, p, dim, out=None) → Tensor
```

返回输入张量给定维`dim` 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1.

参数：

- input (Tensor) – 输入张量
- p (float) – 范数计算中的幂指数值
- dim (int) – 缩减的维度
- out (Tensor, optional) – 结果张量

例子：

```
>>> a = torch.randn(4, 2)
>>> a

-0.6891 -0.6662
 0.2697  0.7412
 0.5254 -0.7402
 0.5528 -0.2399
[torch.FloatTensor of size 4x2]

>>> torch.norm(a, 2, 1)

 0.9585
 0.7888
 0.9077
 0.6026
[torch.FloatTensor of size 4x1]

>>> torch.norm(a, 0, 1)

 2
 2
 2
 2
[torch.FloatTensor of size 4x1]
```

#### unsqueeze()

```python
>>> a = torch.rand(2,3)
>>> a
tensor([[0.6188, 0.8565, 0.8087],
        [0.6824, 0.0179, 0.1942]])
>>> a.unsqueeze(1)
tensor([[[0.6188, 0.8565, 0.8087]],
        [[0.6824, 0.0179, 0.1942]]])

```

**经过这种操作，本来的列存放数字改为列存放列表**

```
1
1
1
1
变为
[1,2,3]
[1,3,2]
[1,2,3]
[1,2,3]
```

#### torch.cuda()

将模型从cpu传送到GPU

```pyhton
if args.cuda:
	kge_model = kge_model.cuda()
```

## 1.4 代码部分 （dataloder.py）

#### BidirectionalOneShotIterator

将数据集中的数据通过yield生成迭代器送入内存



# KGB Rotate

* 数据集 FB15K
* 显存不足无法复现

报错信息

```bash
RuntimeError: CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 10.76 GiB total capacity; 8.44 GiB already allocated; 661.44 MiB free; 9.14 GiB reserved in total by PyTorc
```

使用nvidia-smi发现并没有进程在运行，显存也未被占用

**这说明为程序分配8.44GiB显存后无法继续分配报错，进而程序停止，就是说显卡内存不足**

* TransE模型可以试试



# 日记

## 1-11

* 运行pairRE提供的代码，提示没有ogb包



## 1-12 

解决昨天的报错
昨天以为装上了，实际装obg失败了，原因是卡在了torch安装这一步
今天单独安装torch ，因为是在windows上配置环境，在安装torch时按选择没有cuda的cpu-only版本
命令

```
conda install pytorch torchvision torchaudio cpuonly -c pytorch

```

网上说一般人学习只用cpu-only的torch版本就可以，希望没问题
pairre模型用的是16G显存的显卡，试试我的amd cpu顶不顶
现在在下数据集，希望能成功，
运行失败，源代码调用了cuda，本机没有

##  3-1

* Auto-SF源码 run.py

## 3-2 

* run.py   319行，do_train部分
* model.py

### 思考

OGB给出的实例文件中包含模型如下

* * TransE 
  * DistMult
  * ComplEx
  * RotatE

  等主流模型，这意味着对比实验不需要自己实现，实现自己的想法时只需要将自己的模型加到model.py中即可

  现在的问题是看懂各个参数的意义，比如mode、数据评估eval等，这样才知道如何通过现有代码以最小的改动实现自己的想法

