

# 文档

## 1.2代码部分(run.py)

### 命令行参数

* '--cuda', action='store_true', help='use GPU'
* '--do_train', action='store_true'
* '--do_valid', action='store_true'
* '--do_test', action='store_true'
* '--evaluate_train', action='store_true', help='Evaluate on training data'
* '--dataset', type=str, default='ogbl-biokg', help='dataset name, default to biokg'
* '--model', default='TransE', type=str)
* '-de', '--double_entity_embedding', action='store_true')
* '-dr', '--double_relation_embedding', action='store_true')
* '-n', '--negative_sample_size', default=128, type=int)
* '-d', '--hidden_dim', default=500, type=int)
* '-g', '--gamma', default=12.0, type=float
* '-adv', '--negative_adversarial_sampling', action='store_true'
* '-a', '--adversarial_temperature', default=1.0, type=float
* '-b', '--batch_size', default=1024, type=int
* '-r', '--regularization', default=0.0, type=float
* '--test_batch_size', default=4, type=int, help='valid/test batch size'
* '--uni_weight', action='store_true',  help='Otherwise use subsampling weighting like in word2vec'
* '-lr', '--learning_rate', default=0.0001, type=float
* '-cpu', '--cpu_num', default=10, type=int
* '-randomSeed', default=0, type=int)
* parser.add_argument('-init', '--init_checkpoint', default=None, type=str)
* parser.add_argument('-save', '--save_path', default=None, type=str)parser.a
* dd_argument('--max_steps', default=100000, type=int)
* parser.add_argument('--warm_up_steps', default=None, type=int
* parser.add_argument('--save_checkpoint_steps', default=10000, type=int)
* parser.add_argument('--valid_steps', default=10000, type=int)
* parser.add_argument('--log_steps', default=100, type=int, help='train log every xx steps')
* parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')
* parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')
* parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')
* parser.add_argument('--print_on_screen', action='store_true', help='log on screen or not
* parser.add_argument('--ntriples_eval_train', type=int, default=200000, help='number of training triples to evaluate ventually'
* parser.add_argument('--neg_size_eval_train', type=int, default=500, help='number of negative samples when evaluating training triples')

### defaultdict

```python
from collections import defaultdict

train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)
```

defaultdict区别于普通的dict
对于普通的dict，调用不存在的键值对时会报错

```python
>>> a = dict()
>>> a['name'] = 'modige'
>>> print(a['name'])
modige
>>> print(a['age'])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyError: 'age'

```

但对于defaultdict

```python
>>> b = defaultdict(list)
>>> print(b['age'])
[]
>>> c = defaultdict(tuple)
>>> print(c['age'])
()
```

可以看出，但调用不存在的key时会返回一个形参类型的空结果

### tqdm

Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)。



使用方法

```python
    for i in tqdm(range(len(train_triples['head']))):
        head, relation, tail = train_triples['head'][i], train_triples['relation'][i], train_triples['tail'][i]
        head_type, tail_type = train_triples['head_type'][i], train_triples['tail_type'][i]
        train_count[(head, relation, head_type)] += 1
        train_count[(tail, -relation-1, tail_type)] += 1
        train_true_head[(relation, tail)].append(head)
        train_true_tail[(head, relation)].append(tail)
```

样式如下

100%|██████████| 4762678/4762678 [00:14<00:00, 335575.16it/s]

或



```python
from tqdm import tqdm
for i in tqdm(range(1000)):
 #do something
 pass
```

### argparse

* **https://docs.python.org/3/library/argparse.html**

命令行选项、参数和子命令的解析器

#### 打印现有参数

```pyhton
python a.py -h
#输出
usage: a.py [-h] echo ecdho

positional arguments:
  echo        dddd
  ecdho       dddd

optional arguments:
  -h, --help  show this help message and exit
```

#### 常用关键字

##### **help**

存放对参数的解释

#####  **action**

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--verbose", help="increase output verbosity",
                    action="store_true")
args = parser.parse_args()
if args.verbose:
    print("verbosity turned on")
```

给关键字action赋值"store_true"，这意味着，如果指定了选项，则将值 True 分配给 args.verbose。不指定它意味着 False。

##### type

指定参数的数据类型

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("square", help="display a square of a given number",
                    type=int)
args = parser.parse_args()
print(args.square**2)
```

##### 短指令

可以使用```-v```代替```--verbose```

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("-v", "--verbose", help="increase output verbosity",
                    action="store_true")
args = parser.parse_args()
if args.verbose:
    print("verbosity turned on")
```

#### default 

不指定参数时的默认值

```python
parser.add_argument('--valid_steps', default=10000, type=int)
```

执行时没有加```--valid_steps```使用默认值10000



```python
import argparse
def parse_args(args=None):
    parser = argparse.ArgumentParser(
        description='Training and Testing Knowledge Graph Embedding Models',
        usage='train.py [<args>] [-h | --help]'
    )

    parser.add_argument('--cuda', action='store_true', help='use GPU')
    parser.add_argument('--dataset', type=str, default='ogbl-biokg', help='dataset name, default to biokg')
    ...
    return parser.parse_args(args)
```

#### class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)[source]

实现Adam算法。

它在[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)中被提出。

**参数：**

- params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
- lr (`float`, 可选) – 学习率（默认：1e-3）
- betas (Tuple[`float`, `float`], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）
- eps (`float`, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
- weight_decay (`float`, 可选) – 权重衰减（L2惩罚）（默认: 0）

#### step(closure) [source]

进行单次优化 (参数更新).

**参数：**

- closure (`callable`) – 一个重新评价模型并返回loss的闭包，对于大多数参数来说是可选的。

### DataLoader

```python
from torch.utils.data import DataLoader
```

```
Data loader. Combines a dataset and a sampler, and provides an iterable over
the given dataset.
```

数据载入器，融合数据集与取样器并提供一个给定数据集的iterable

**参数：**

- **dataset** (*Dataset*) – 加载数据的数据集。
- **batch_size** (*int*, optional) – 每个batch加载多少个样本(默认: 1)。
- **shuffle** (*bool*, optional) – 设置为`True`时会在每个epoch重新打乱数据(默认: False).
- **sampler** (*Sampler*, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略`shuffle`参数。
- **num_workers** (*int*, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)
- **collate_fn** (*callable*, optional) –
- **pin_memory** (*bool*, optional) –
- **drop_last** (*bool*, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默认: False)

### 主程序（main函数）逻辑

1. 判断是否指定训练/测试/验证mode(缺一不可)，没有则报错并给出提示
2. 检查是否需要从断点处恢复模型
3. 设置模型保存路径
4. 设置randomseed  
5. 载入数据集到内存
6. 分割数据集
7. 设置评价协议（ ogb自带函数）



## 1.3 代码部分（model.py）

**参数 **

* ```model_name```模型
* ```nentity```实体数量
* ```nrelation```关系数量
* ```hidden_dim ```隐藏层维度
* ```epsilon```
* ```gamma```
* ```embedding_range```
* `entity_embedding`   实体特征矩阵
* `relation_embedding`关系特征矩阵

### 1.3.1 def forward()

model默认调用函数

```python
def forward(self, sample, mode='single'):
    '''
        Forward function that calculate the score of a batch of triples.
        In the 'single' mode, sample is a batch of triple.
        In the 'head-batch' or 'tail-batch' mode, sample consists two part.
        The first part is usually the positive sample.
        And the second part is the entities in the negative samples.
        Because negative samples and positive samples usually share two elements 
        in their triple ((head, relation) or (relation, tail)).
        '''   
正向传播函数计算一批三元组的打分。
single模式下样本是一批三元组
head-batch 和 tail-batch模式下，样本包含两部分：
第一部分通常是正例样本。
第二部分是阴性样本中的实体。
因为阴性样本和阳性样本三元组通常共享两种元素((head, relation) or (relation, tail)).

```

#### torch.index_select

```
torch.index_select(input, dim, index, out=None) → Tensor
```

​              沿着指定维度对输入进行切片，取`index`中指定的相应项(`index`为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量_Tensor_有相同的维度(在指定轴上)。

注意： 返回的张量不与原始张量共享内存空间。

参数:

- input (Tensor) – 输入张量
- dim (int) – 索引的轴
- index (LongTensor) – 包含索引下标的一维张量
- out (Tensor, optional) – 目标张量

例子：

```python
>>> x = torch.randn(3, 4)
>>> x

 1.2045  2.4084  0.4001  1.1372
 0.5596  1.5677  0.6219 -0.7954
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 3x4]

>>> indices = torch.LongTensor([0, 2])
>>> torch.index_select(x, 0, indices)

 1.2045  2.4084  0.4001  1.1372
 1.3635 -1.2313 -0.5414 -1.8478
[torch.FloatTensor of size 2x4]

>>> torch.index_select(x, 1, indices)

 1.2045  0.4001
 0.5596  0.6219
 1.3635 -0.5414
[torch.FloatTensor of size 3x2]

import torch
# 创建1D张量
a = torch.arange(0, 9)
print(a)

# 获取1D张量的第1个维度且索引号为2和3的张量子集
print(torch.index_select(a, dim = 0, index = torch.tensor([2, 3])))
# 创建2D张量
b = torch.arange(0, 9).view([3, 3])
print(b)

# 获取2D张量的第2个维度且索引号为0和1的张量子集(第一列和第二列)
print(torch.index_select(b, dim = 1, index = torch.tensor([0, 1])))
# 创建3D张量
c = torch.arange(0, 9).view([1, 3, 3])
print(c)
# 获取3D张量的第1个维度且索引号为0的张量子集
print(torch.index_select(c, dim = 0, index = torch.tensor([0])))
```



#### 调用打分函数

```python
model_func = {
    'TransE': self.TransE,
    'DistMult': self.DistMult,
    'ComplEx': self.ComplEx,
    'RotatE': self.RotatE,
    'AutoSF': self.AutoSF,
}

if self.model_name in model_func:
    # 调用函数的方法
    # model_func是一个字典 键是字符串，值是函数名
    score = model_func[self.model_name](head, relation, tail, mode)
```



### sample格式

```
n行三列

h,r,t
h,r,t
h,r,t
h,r,t
。。。
```

实体与关系嵌入维度相同，默认为hidden_dim

### Tuned hyper-parameters

| name                    | biokg                          | wikikg2                        |
| ----------------------- | ------------------------------ | ------------------------------ |
| learning rate           | [1e-4, 3e-4, 1e-3, 3e-3, 1e-2] | [1e-4, 3e-4, 1e-3, 3e-3, 1e-2] |
| L2 regularization       | [1e-7, 3e-7, 1e-8, 3e-8, 1e-6] | [1e-7, 3e-7, 1e-8, 3e-8, 1e-6] |
| gamma                   | [50, 100, 200]                 | [50, 100, 200]                 |
| batch_size              | [1024, 2048]                   | [1024, 2048]                   |
| embedding dimension     | [1000, 2000]                   | [100, 200]                     |
| #negative samples       | [64, 128, 256, 512, 1024]      | [64, 128, 256, 512, 1024]      |
| adversarial temperature | [0.2, 0.5, 1, 1.5, 2, 3]       | [0.2, 0.5, 1, 1.5, 2, 3]       |



### 1.3.2 init函数

初始化模型参数

#### torch.nn.Parameter()

`Variable`的一种，常被用于模块参数(`module parameter`)。

`Parameters` 是 `Variable` 的子类。`Paramenters`和`Modules`一起使用的时候会有一些特殊的属性，即：当`Paramenters`赋值给`Module`的属性的时候，他会自动的被加到 `Module`的 参数列表中(即：会出现在 `parameters() 迭代器中`)。将`Varibale`赋值给`Module`属性则不会有这样的影响。 这样做的原因是：我们有时候会需要缓存一些临时的状态(`state`), 比如：模型中`RNN`的最后一个隐状态。如果没有`Parameter`这个类的话，那么这些临时变量也会注册成为模型变量。

`Variable` 与 `Parameter`的另一个不同之处在于，`Parameter`不能被 `volatile`(即：无法设置`volatile=True`)而且默认`requires_grad=True`。`Variable`默认`requires_grad=False`。

参数说明:

- data (Tensor) – parameter tensor.
- requires_grad (bool, optional) – 默认为`True`，在`BP`的过程中会对其求微分。

#### torch.nn.init.uniform

```
torch.nn.init.uniform(tensor, a=0, b=1)
```

从均匀分布U(a, b)中生成值，填充输入的张量或变量

**参数：**

- **tensor** - n维的torch.Tensor
- **a** - 均匀分布的下界
- **b** - 均匀分布的上界

**例子**

```python
>>> w = torch.Tensor(3, 5)
>>> nn.init.uniform(w)
```



#### torch.norm

```
torch.norm(input, p=2) → float
```

返回输入张量`input` 的p 范数。

参数：

- input (Tensor) – 输入张量
- p (float,optional) – 范数计算中的幂指数值

例子：

```
>>> a = torch.randn(1, 3)
>>> a

-0.4376 -0.5328  0.9547
[torch.FloatTensor of size 1x3]

>>> torch.norm(a, 3)
1.0338925067372466
torch.norm(input, p, dim, out=None) → Tensor
```

返回输入张量给定维`dim` 上每行的p 范数。 输出形状与输入相同，除了给定维度上为1.

参数：

- input (Tensor) – 输入张量
- p (float) – 范数计算中的幂指数值
- dim (int) – 缩减的维度
- out (Tensor, optional) – 结果张量

例子：

```
>>> a = torch.randn(4, 2)
>>> a

-0.6891 -0.6662
 0.2697  0.7412
 0.5254 -0.7402
 0.5528 -0.2399
[torch.FloatTensor of size 4x2]

>>> torch.norm(a, 2, 1)

 0.9585
 0.7888
 0.9077
 0.6026
[torch.FloatTensor of size 4x1]

>>> torch.norm(a, 0, 1)

 2
 2
 2
 2
[torch.FloatTensor of size 4x1]
```

#### unsqueeze()

```python
>>> a = torch.rand(2,3)
>>> a
tensor([[0.6188, 0.8565, 0.8087],
        [0.6824, 0.0179, 0.1942]])
>>> a.unsqueeze(1)
tensor([[[0.6188, 0.8565, 0.8087]],
        [[0.6824, 0.0179, 0.1942]]])

```

**经过这种操作，本来的列存放数字改为列存放列表**

```
1
1
1
1
变为
[1,2,3]
[1,3,2]
[1,2,3]
[1,2,3]
```

#### torch.cuda()

将模型从cpu传送到GPU

```pyhton
if args.cuda:
	kge_model = kge_model.cuda()
```

### 1.3.3 train_step

```python
train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)
```



```python
positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)
```

```
(tensor([[2, 1, 3],
        [0, 0, 1]]), tensor([[0, 0, 0, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1,
         2, 0, 1, 2, 0, 0, 0, 2, 2, 1, 1, 1, 0, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1,
         0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 1, 0, 1, 1,
         2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 2, 2, 2, 0, 0,
         1, 1, 2, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 0, 2,
         1, 2, 2, 2, 1, 0, 0, 0],
        [3, 2, 2, 2, 3, 0, 2, 2, 0, 2, 2, 2, 2, 2, 3, 3, 0, 0, 0, 0, 2, 2, 0, 3,
         3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2, 2, 0, 2, 2, 0, 3, 3, 0, 2, 2, 2, 2, 3,
         3, 2, 2, 3, 0, 2, 3, 2, 2, 2, 0, 3, 3, 0, 3, 0, 3, 0, 2, 3, 0, 3, 2, 3,
         0, 2, 0, 2, 0, 0, 3, 0, 3, 2, 0, 0, 2, 3, 0, 3, 2, 2, 0, 0, 0, 0, 0, 2,
         2, 2, 0, 2, 3, 3, 2, 0, 3, 0, 0, 2, 3, 3, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0,
         2, 2, 3, 0, 3, 3, 3, 3]]), tensor([0.3536, 0.3536]), 'tail-batch')
  正样本  负样本(与参数 negative_sample_size有关)   下采样权重    训练模式 
```







## 1.4 代码部分 （dataloder.py）



### TestDataset

#### __getitem__

**输入**：

* 正例三元组索引

**输出**：

* positive_sample.以实体，关系id表示的三元组
* negative_sample: 0-nentity的一维数组[0,1,2,.....]
* filter_bias:判断新生成的三元组是否存在于数据集，存在为0，不存在为-1，[0,-1,.......]
* mode:头批还是尾批

从正例样本，以

```python
    def __getitem__(self, idx):
        head, relation, tail = self.triples[idx]

        if self.mode == 'head-batch':
            tmp = [(0, rand_head) if (rand_head, relation, tail) not in self.triple_set
                   else (-1, head) for rand_head in range(self.nentity)]
            # 0表示存在于（head,relation,tail）数据集中,-1表示不存在
            # tmp长度为 nentity

            tmp[head] = (0, head)
            # 确保tmp[head]没错

        elif self.mode == 'tail-batch':
            tmp = [(0, rand_tail) if (head, relation, rand_tail) not in self.triple_set
                   else (-1, tail) for rand_tail in range(self.nentity)]
            tmp[tail] = (0, tail)
        else:
            raise ValueError('negative batch mode %s not supported' % self.mode)

        tmp = torch.LongTensor(tmp)
        # 列表转为tensor,  nentity * 2
        filter_bias = tmp[:, 0].float()
        # 0 或 -1组成的 tensor

        negative_sample = tmp[:, 1]
        # 长度为nentity的一维tensor

        positive_sample = torch.LongTensor((head, relation, tail))
        # (head_id,rel_id,tail_id)

        return positive_sample, negative_sample, filter_bias, self.mode
```



### BidirectionalOneShotIterator

将数据集中的数据通过yield生成迭代器送入内存

#### get_true_head_and_tail(triples)

* 输入：三元祖列表
* 输出：两个字典

将三元组装换成两个字典，分别存放

* 关系(h,r,?)对应的所有尾实体
* 关系(？,r,t)对应的所有头实体

```
 true_head = {(h,r):[t1,t2.....],
              (h2,r):[t1,....]，。。。
 
 }
 
 true_tail = {(r,t):[h1,h2,...]

 }
```

__getitem__(self, idx)

* 输入：三元组索引

* 输出：根据索引返回一个item

* 为一个正样本生成negative_sample_size个负样本

  ##### in1d(ar1, ar2, assume_unique=False, invert=False)

  测试一维向量ar1中的元素是否出现在ar2中，返回一个布尔型一维向量，出现的位置为True

  * assume_unique：若为True假设输入的数组无重复值，这可以加快查询速度

  * invert ：为True则将返回值取反

    

#### collate_fn(data)

* 输入：数据集
* 输出positive_sample, negative_sample, subsample_weight, mode

#### count_frequency(triples, start=4)

* 输入：三元组列表，默认次数
* 输出：次数字典

计算三元组中(h,r)和（r,t）出现的频率，默认为4，在关系-尾中使用关系的相反数能避免出现重复，还能区分头、尾批

```
count = {}
for head, relation, tail in triples:
    if (head, relation) not in count:
        count[(head, relation)] = start
    else:
        count[(head, relation)] += 1

    if (tail, -relation - 1) not in count:
        count[(tail, -relation - 1)] = start
    else:
        count[(tail, -relation - 1)] += 1
return count
```

### 样本

* 生成一批正样本positive_sample 128个真三元组
* 负样本128*128，对应每个三元组的128个替换头实体（或尾实体）

样本生成逻辑

一批正样本positive_sample，数量为batch_size

​	对应正样本中的每个三元组positive_sample[i],为其生成negative_sample_size个负样本

```****
positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)

negative_sample.shape: torch.Size([128, 128])
positive_sample.shape: torch.Size([128, 3])
```



# 训练流程

1. 数据准备
2. 数据分批并存入迭代器
3. 在训练步数循环中执行训练函数
4. 1.  打开训练模式
   2. 设置梯度
   3. 取出正样本（batch_size个真三元组）
   4. 取出负样本batch_size*negetive_sample_size个实体
   5. 计算正样本打分batch_size*1
   6. 计算负样本打分batch_size*negetive_sample_size
   7. 计算正样本打分在负样本打分中的排名
   8. 计算正样本损失
   9. 计算负样本损失
   10. 计算总损失
   11. 跟据损失调整模型参数
5. 保存模型
6. 评估模型

