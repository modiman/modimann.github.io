# 命令行启动

| dataset   | lr     | dr    | edim | rdim | input_d | hidden_d1 | hidden_d2 | label_smoothing |
| --------- | ------ | ----- | ---- | ---- | ------- | --------- | --------- | --------------- |
| FB15k     | 0.003  | 0.99  | 200  | 200  | 0.2     | 0.2       | 0.3       | 0.              |
| WN18      | 0.005  | 0.995 | 200  | 30   | 0.2     | 0.1       | 0.2       | 0.1             |
| FB15k-237 | 0.0005 | 1.0   | 200  | 200  | 0.3     | 0.4       | 0.5       | 0.1             |
| WN18RR    | 0.003  | 1.0   | 200  | 30   | 0.2     | 0.2       | 0.3       | 0.1             |

```
原命令
CUDA_VISIBLE_DEVICES=0 python main.py --dataset FB15k-237 --num_iterations 500 --batch_size 128
                                       --lr 0.0005 --dr 1.0 --edim 200 --rdim 200 --input_dropout 0.3 
                                       --hidden_dropout1 0.4 --hidden_dropout2 0.5 --label_smoothing 0.1

CUDA_VISIBLE_DEVICES=1 python main.py --dataset WN18RR --batch_size 512 --model_name TuckER --mode train  --lr 0.003 --rdim 30 --input_dropout 0.2 --hidden_dropout1 0.2 --hidden_dropout2 0.3


CUDA_VISIBLE_DEVICES=1 python main.py --dataset FB15k --batch_size 512 --model_name TuckER --mode train  --lr 0.003 --rdim 200 --input_dropout 0.2 --hidden_dropout1 0.2 --hidden_dropout2 0.3


CUDA_VISIBLE_DEVICES=1 python main.py --dataset FB15k-237 --batch_size 512 --model_name head_project --mode train  --lr 0.003 --edim 200 --rdim 400 --input_dropout 0.3 --hidden_dropout1 0.4 --hidden_dropout2 0.5

```



## 5-3张量变换

### 转置

```python
a = torch.randn(2,3,3)
>>>tensor([[[-1.6531,  0.9339,  0.9556],
         [ 0.7668,  1.0657, -1.3660],
         [ 0.0541,  0.3297,  0.2132]],
        [[-0.4406, -0.6034, -0.1519],
         [ 0.0684,  0.9717,  0.6225],
         [ 1.3838,  1.1323,  1.2500]]])
a.transpose(1,2)
Out[74]: 
tensor([[[-1.6531,  0.7668,  0.0541],
         [ 0.9339,  1.0657,  0.3297],
         [ 0.9556, -1.3660,  0.2132]],
        [[-0.4406,  0.0684,  1.3838],
         [-0.6034,  0.9717,  1.1323],
         [-0.1519,  0.6225,  1.2500]]])
```

### 展开

```python
a.view(2,-1)  # 将1,2轴展开为一维，按行拼接
Out[71]: 
tensor([[-1.6531,  0.9339,  0.9556,  0.7668,  1.0657, -1.3660,  0.0541,  0.3297,
          0.2132],
        [-0.4406, -0.6034, -0.1519,  0.0684,  0.9717,  0.6225,  1.3838,  1.1323,
          1.2500]])
```



## 5-4

* 代码修改完成

## 5-5

* 添加日志打印功能
* 添加模型保存功能
* FB15k237是fb15k的子集，那么在fb15k训练得到的模型应该具有在FB15k237上的泛化能力
* 下一步实验设计，在FB15k模型上测试FB15k237上的测试集

## 5-6

* 保存模型功能
* 查找表现最差的实体
* 查看去掉最差性能的10个对结果的影响，构思实验trick
* introdcution部分
* 

### t-SNE

一种效果优于PCA的聚类算法，封装在sklearn中。

可以用它将处于高维向量空间的数据点压缩成二维或三维，便于画图展示

**使用案例**

```python
import numpy as np
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import os
# 从保存的模型中恢复嵌入向量
data = np.load('entity_embedding.npy')
# 使用TSNE对嵌入向量做聚类分析，结果保留两个维度
X_tsne = TSNE(n_components=2,random_state=33).fit_transform(data)
# 设计要展示的图片大小
plt.figure(figsize=(10, 5))
plt.subplot(121)
# 绘图，两个维度分别作为x,y轴
plt.scatter(X_tsne[:, 0], X_tsne[:, 1],label="t-SNE")
plt.legend()
# 保存图片
plt.savefig('images/rotate237-tsne.png', dpi=120)
plt.show()
```

**matplotlib.pyplot.scatter**(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, *, data=None, **kwargs)

参数的解释：

x，y：表示的是大小为(n,)的数组，也就是我们即将绘制散点图的数据点

s:是一个实数或者是一个数组大小为(n,)，这个是一个可选的参数。

c:表示的是颜色，也是一个可选项。默认是蓝色'b',表示的是标记的颜色，或者可以是一个表示颜色的字符，或者是一个长度为n的表示颜色的序列等等，感觉还没用到过现在不解释了。但是c不可以是一个单独的RGB数字，也不可以是一个RGBA的序列。可以是他们的2维数组（只有一行）。

## 5-7

对应的tail超过n的(head,ralation) 数量 表征1-N关系

| n               | 700  | 600  | 500  | 400  | 300  | 200  | 100  |
| --------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| (head,ralation) | 1    | 2    | 4    | 6    | 14   | 28   | 105  |

/m/09c7w0	/location/location/contains

对应的head超过n的(ralation,tail) 数量  表征N-1关系

| n                | 700  | 600  | 500  | 400  | 300  | 200  | 100  |
| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| (ralation，tail) | 18   | 20   | 31   | 39   | 63   | 101  | 217  |

### Tucker原模型WN18RR结果

```
CUDA_VISIBLE_DEVICES=1 python main.py --dataset WN18RR --num_iterations 500 --lr 0.003 --dr 1.0 --rdim 30 --input_dropout 0.2 --hidden_dropout1 0.2 --hidden_dropout2 0.3  --batch_size 128

2022-05-07 15:25:07 INFO     500
2022-05-07 15:25:07 INFO     7.605196714401245
2022-05-07 15:25:07 INFO     0.00034900982621009606
2022-05-07 15:25:07 INFO     模型保存成功
2022-05-07 15:25:07 INFO     Validation:
2022-05-07 15:25:10 INFO     Hits @10: 0.5121951219512195
2022-05-07 15:25:10 INFO     Hits @3: 0.4698417930125247
2022-05-07 15:25:10 INFO     Hits @1: 0.43539881344759396
2022-05-07 15:25:10 INFO     Mean rank: 6209.889584706658
2022-05-07 15:25:10 INFO     Mean reciprocal rank: 0.461735131653968
2022-05-07 15:25:10 INFO     [0.5215379706445438, 0.4871456822676335, 0.4384173580089343, 6049.534460753031, 0.46583824185730044]
2022-05-07 15:25:10 INFO     Test:
2022-05-07 15:25:13 INFO     Hits @10: 0.5199425654116145
2022-05-07 15:25:13 INFO     Hits @3: 0.47654754307594127
2022-05-07 15:25:13 INFO     Hits @1: 0.4361837906828334
2022-05-07 15:25:13 INFO     Mean rank: 6127.832322910019
2022-05-07 15:25:13 INFO     Mean reciprocal rank: 0.4650587206998672
2022-05-07 15:25:13 INFO     [0.5215379706445438, 0.4871456822676335, 0.4384173580089343, 6049.534460753031, 0.46583824185730044]

```

* 之前的反馈函数没有发挥作用
* 聚类图绘制完成
* 模型修改完成

新的投影模型

```python
# 设计模型
import numpy as np
import torch
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import os

# 恢复模型

entities = torch.tensor(np.load('entity_embedding.npy'))
relations = torch.tensor(np.load('relation_embedding.npy'))
W = torch.tensor(np.load('W_embedding.npy'))

bn0 = torch.nn.BatchNorm1d(200)
bn1 = torch.nn.BatchNorm1d(200)

input_dropout = torch.nn.Dropout(0.3)
hidden_dropout1 = torch.nn.Dropout(0.4)
hidden_dropout2 = torch.nn.Dropout(0.5)

e1 = np.arange(5)
r = np.arange(5)
e1 = entities[e1]
head = e1
r = relations[r]
batch_size = e1.size(0)
de = e1.size(1)
dr = r.size(1)

w = W
w = w.view(1,dr,de * de)  # 实体平面展开成一维
w = w.repeat(r.size(0), 1, 1)  # 重复填充

head = bn0(e1)
head = input_dropout(head)


new_r = r.view(batch_size, 1, dr)  # 变换形状，便于计算-
features = torch.bmm(new_r, w)  # torch.Size([5, 1, 40000])
features = features.view(batch_size, de, de) # torch.Size([5, 200, 200])
features = hidden_dropout1(features)

head = e1.view(batch_size, 1, de) # torch.Size([5, 1, 200])


head = torch.bmm(head, features) # torch.Size([5, 1, 200])

head = head.view(batch_size, de) # torch.Size([5, 200])

head = bn1(head)
head = hidden_dropout2(head)

head.mean()

w_tail = W.transpose(1, 2) # torch.Size([200, 200, 200])
w_tail = w_tail.reshape(1, dr, de * de) 
w_tail = w_tail.repeat(batch_size,1,1)


features_tail = torch.bmm(new_r, w_tail)
features_tail = features_tail.view(r.size(0), e1.size(1), e1.size(1))#torch.Size([5, 200, 200])]


features_tail = hidden_dropout1(features_tail)
# features_tail = bn1(features_tail)

tail = bn0(entities)
tail = entities.unsqueeze(0)# torch.Size([1, 14541, 200])

tail = input_dropout(tail)

tail = tail.repeat(batch_size,1,1)# torch.Size([5, 14541, 200])


tail = torch.bmm(tail,features_tail)# torch.Size([5, 14541, 200])


# bn2 = torch.nn.BatchNorm1d(14541，200)
tail = torch.nn.functional.normalize(tail,2)
tail = hidden_dropout2(tail)
tail = tail.transpose(1,2)# torch.Size([5, 200, 14541])
head = head.view(batch_size,1,de)
# res = torch.nn.functional.cosine_similarity(head,tail)
# # res = torch.norm(head-tail,1,2)
res = torch.bmm(head,tail).squeeze(1)
# res
torch.sigmoid(res)
```

### 明天关注问题：三维张量的BatchNorm1d

```
tail = torch.nn.functional.normalize(tail,2)
# 这一句需要修改
```

另外，全部投影所有实体内存消耗太大，下一步考虑投影部分尾实体

## 5-9

分析测试集中数据特点

1. （头，关系）对应尾数量的平均值

```python
head_relation = {}
for head,relation,tail in data:
    if (head,relation) not in head_relation:
        head_relation[(head,relation)] = [tail]
    else:
        head_relation[(head,relation)].append(tail)
sum_tail = 0
for key in head_relation:
    sum_tail += len(head_relation[key])
print(sum_tail/len(head_relation))
>>>1.489844944310985
```

2. (关系，尾)对应头数量的平均值

```python
tail_relation = {}
for head,relation,tail in data:
    if (relation,tail) not in tail_relation:
        tail_relation[(relation,tail)] = [head]
    else:
        tail_relation[(relation,tail)].append(head)
sum = 0
for key in tail_relation:
    sum += len(tail_relation[key])
print(sum/len(tail_relation))
>>>2.2458026994403597
```

|      | head_pre_tail      | tail_per_head     |
| ---- | ------------------ | ----------------- |
| test | 2.2458026994403597 | 1.489844944310985 |

根据分析，测试集具有更多的n-1关系

之前的实验是将测试集划分为预测尾任务（head,relation,?），以及预测头任务（？，relation,tail），这样测试集的长度翻了一倍

现在将其分开，做两次测试，查看结果、

```
Number of data points: 20466
2022-05-09 15:25:04,914 INFO     Hits @10: 0.3930421186357862
2022-05-09 15:25:04,915 INFO     Hits @3: 0.24704387765073782
2022-05-09 15:25:04,916 INFO     Hits @1: 0.1490276556239617
2022-05-09 15:25:04,917 INFO     Mean rank: 208.27186553307925
2022-05-09 15:25:04,918 INFO     Mean reciprocal rank: 0.2296164051010573
2022-05-09 15:25:04,918 INFO     [0, 0, 0, 20000, 0]
2022-05-09 15:25:04,934 INFO     Test:
Number of data points: 20466
2022-05-09 15:25:13,095 INFO     Hits @10: 0.6090589270008795
2022-05-09 15:25:13,096 INFO     Hits @3: 0.4682888693442783
2022-05-09 15:25:13,096 INFO     Hits @1: 0.3355321020228672
2022-05-09 15:25:13,097 INFO     Mean rank: 115.38478452066843
2022-05-09 15:25:13,099 INFO     Mean reciprocal rank: 0.42788451620337564
2022-05-09 15:25:13,099 INFO     [0, 0, 0, 20000, 0]
2022-05-09 15:25:13,122 INFO     8.18712067604065

```

两个方向的结果相差悬殊，这里值得挖掘

原模型数据

```
022-05-09 20:42:39,205 INFO     Hits @10: 0.6420893188703215
2022-05-09 20:42:39,205 INFO     Hits @3: 0.4969705853610867
2022-05-09 20:42:39,206 INFO     Hits @1: 0.35883905013192613
2022-05-09 20:42:39,207 INFO     Mean rank: 123.40657676145803
2022-05-09 20:42:39,208 INFO     Mean reciprocal rank: 0.45433137392559453
2022-05-09 20:42:39,208 INFO     [0, 0, 0, 20000, 0]


2022-05-09 20:42:39,232 INFO     4.75752067565918
2022-05-09 20:43:46,076 INFO     Hits @10: 0.4422456757549106
2022-05-09 20:43:46,076 INFO     Hits @3: 0.2872080523795563
2022-05-09 20:43:46,077 INFO     Hits @1: 0.1673018665103098
2022-05-09 20:43:46,078 INFO     Mean rank: 206.79023746701847
2022-05-09 20:43:46,079 INFO     Mean reciprocal rank: 0.258595515612217
2022-05-09 20:43:46,079 INFO     [0, 0, 0, 20000, 0]
2022-05-09 20:43:46,103 INFO     4.961421966552734

2022-05-09 20:45:42,054 INFO     Hits @10: 0.542167497312616
2022-05-09 20:45:42,055 INFO     Hits @3: 0.3920893188703215
2022-05-09 20:45:42,056 INFO     Hits @1: 0.263070458321118
2022-05-09 20:45:42,058 INFO     Mean rank: 165.09840711423826
2022-05-09 20:45:42,061 INFO     Mean reciprocal rank: 0.3564634447689058
2022-05-09 20:45:42,061 INFO     [0, 0, 0, 20000, 0]
2022-05-09 20:45:42,092 INFO     8.973554611206055

```

### 原模型分别头尾批的方式

将数据增强成原来的两倍

头批中关系加上_reverse，实际上相当于新引入一个关系

现在考虑另一种标识头尾批的方式，不引入新关系，而是将关系编号乘以-1，以-1作为是否转置矩阵的标志

## 5-11

只预测尾实体

```
def load_data(self, data_dir, data_type="train", reverse=False):
    with open("%s%s.txt" % (data_dir, data_type), "r") as f:
    data = f.read().strip().split("\n")
    data = [i.split() for i in data]
    # if reverse:
    #     data += [[i[2], i[1]+"_reverse", i[0]] for i in data]
    return data
```

```
2022-05-11 13:48:45 INFO     Validation:
2022-05-11 13:48:49 INFO     Hits @10: 0.6401482748788138
2022-05-11 13:48:49 INFO     Hits @3: 0.49860279441117766
2022-05-11 13:48:49 INFO     Hits @1: 0.3626461362988309
2022-05-11 13:48:49 INFO     Mean rank: 134.91371542629028
2022-05-11 13:48:49 INFO     Mean reciprocal rank: 0.455993045635045
2022-05-11 13:48:49 INFO     [0.6335874132707906, 0.5346916837682009, 0.35522329717580375, 137.07001856738003, 0.4492732373620914]
2022-05-11 13:48:49 INFO     Test:
2022-05-11 13:48:54 INFO     Hits @10: 0.633294244112186
2022-05-11 13:48:54 INFO     Hits @3: 0.4945763705658165
2022-05-11 13:48:54 INFO     Hits @1: 0.35605394312518324
2022-05-11 13:48:54 INFO     Mean rank: 150.0566305091371
2022-05-11 13:48:54 INFO     Mean reciprocal rank: 0.4501522569425495
2022-05-11 13:48:54 INFO     [0.6335874132707906, 0.5346916837682009, 0.35522329717580375, 137.07001856738003, 0.4492732373620914]

```



只预测头实体

```
def load_data(self, data_dir, data_type="train", reverse=False):
    with open("%s%s.txt" % (data_dir, data_type), "r") as f:
    data = f.read().strip().split("\n")
    data = [i.split() for i in data]
    res = [[i[2], i[1], i[0]] for i in data]
    return res
```

```
Number of data points: 17535
2022-05-11 15:19:30,538 INFO     Hits @10: 0.4261191901910465
2022-05-11 15:19:30,539 INFO     Hits @3: 0.2784145993726832
2022-05-11 15:19:30,539 INFO     Hits @1: 0.16777872825777018
2022-05-11 15:19:30,540 INFO     Mean rank: 215.01802110065583
2022-05-11 15:19:30,542 INFO     Mean reciprocal rank: 0.25370475055014186
2022-05-11 15:19:30,542 INFO     [0.427098602560344, 0.3648490178833187, 0.1672041434574416, 223.08809733216066, 0.25279266915711374]
2022-05-11 15:19:30,549 INFO     Test:
Number of data points: 20466
2022-05-11 15:19:35,122 INFO     Hits @10: 0.4255838952408873
2022-05-11 15:19:35,122 INFO     Hits @3: 0.2774846086191733
2022-05-11 15:19:35,123 INFO     Hits @1: 0.16691097429883708
2022-05-11 15:19:35,124 INFO     Mean rank: 223.5528681716017
2022-05-11 15:19:35,126 INFO     Mean reciprocal rank: 0.2530676873348007
2022-05-11 15:19:35,126 INFO     [0.427098602560344, 0.3648490178833187, 0.1672041434574416, 223.08809733216066, 0.25279266915711374]
2022-05-11 15:19:35,139 INFO     4.589280366897583
2022-05-11 15:19:35,684 INFO     模型保存成功

```

现在考虑参考翻译模型，将数据集分为头尾两批







## 5-13

```python
 def load_data(self, data_dir, data_type="train", reverse=False):
        with open("%s%s.txt" % (data_dir, data_type), "r") as f:
            data = f.read().strip().split("\n")
            data = [i.split() for i in data]
            res = []
            res += [[i[0],i[0]+i[1],i[2]] for i in data]
            if reverse:
                res += [[i[2], i[2]+i[1], i[0]] for i in data]

        # 前半部分为头批，后半部分为尾批，头尾批使用一样的关系
        return res
```

结果

```
Number of data points: 35070
2022-05-13 13:36:11,797 INFO     Hits @10: 0.38571428571428573
2022-05-13 13:36:11,798 INFO     Hits @3: 0.26147704590818366
2022-05-13 13:36:11,799 INFO     Hits @1: 0.1592814371257485
2022-05-13 13:36:11,801 INFO     Mean rank: 1996.5351867693184
2022-05-13 13:36:11,803 INFO     Mean reciprocal rank: 0.235326408150945
2022-05-13 13:36:11,803 INFO     [0.42646340271670086, 0.31686699892504644, 0.15630802306264047, 704.48390012704, 0.24291676376681254]
2022-05-13 13:36:11,830 INFO     Test:
Number of data points: 40932
2022-05-13 13:36:20,899 INFO     Hits @10: 0.3853708589856347
2022-05-13 13:36:20,900 INFO     Hits @3: 0.2584530440730968
2022-05-13 13:36:20,901 INFO     Hits @1: 0.15203263949965798
2022-05-13 13:36:20,904 INFO     Mean rank: 2021.1220316622691
2022-05-13 13:36:20,906 INFO     Mean reciprocal rank: 0.2304368167823544
2022-05-13 13:36:20,906 INFO     [0.42646340271670086, 0.31686699892504644, 0.15630802306264047, 704.48390012704, 0.24291676376681254]
2022-05-13 13:36:20,941 INFO     9.110770463943481
2022-05-13 13:36:26,136 INFO     模型保存成功

```

### 数据增强

```python
def load_data(self, data_dir, data_type="train", reverse=False):
    with open("%s%s.txt" % (data_dir, data_type), "r") as f:
        data = f.read().strip().split("\n")
        data = [i.split() for i in data]
        res = []
        res += [[i[0]+i[1],i[1],i[2]] for i in data]
        if reverse:
            res += [[i[2]+i[1], i[1], i[0]] for i in data]
```



```
Number of data points: 35070
2022-05-13 18:51:36,811 INFO     Hits @10: 0.47527801539777587
2022-05-13 18:51:36,812 INFO     Hits @3: 0.33963501568291987
2022-05-13 18:51:36,813 INFO     Hits @1: 0.22147134302822927
2022-05-13 18:51:36,815 INFO     Mean rank: 2404.209210151126
2022-05-13 18:51:36,817 INFO     Mean reciprocal rank: 0.30686066207088875
2022-05-13 18:51:36,817 INFO     [0.46990129971660316, 0.38368513632365875, 0.21591908531222515, 442.683255154891, 0.30240434896968066]
2022-05-13 18:51:36,847 INFO     Test:
Number of data points: 40932
2022-05-13 18:52:34,158 INFO     Hits @10: 0.470976253298153
2022-05-13 18:52:34,159 INFO     Hits @3: 0.335996286523991
2022-05-13 18:52:34,161 INFO     Hits @1: 0.21789797713280562
2022-05-13 18:52:34,163 INFO     Mean rank: 2702.095328838073
2022-05-13 18:52:34,165 INFO     Mean reciprocal rank: 0.3036027941020435
2022-05-13 18:52:34,165 INFO     [0.46990129971660316, 0.38368513632365875, 0.21591908531222515, 442.683255154891, 0.30240434896968066]
2022-05-13 18:52:34,207 INFO     57.3599739074707

```

测试集分割，找出表现最好的子集。

## 5-17

TuckER 在训练时把训练集出现的实体标记为1，没出现的标记为0,这与链路预测的目标是相违背的，因为训练集中没出现的可能在测试集中出现，而在训练时由于标记为0而拉远了

## 5-18

原来是把真三元组的target置为1，评估时过滤掉

现在扩大范围，把在relation的尾实体置为1，评估时过滤掉

```python
    def get_relation_tail(self,data):
        relation_tail = defaultdict(list)
        for triple in data:
            relation_tail[triple[1]].append(triple[2])
        return relation_tail
    #修改版
    def get_batch(self, er_vocab, er_vocab_pairs, idx,relation_tail):
        batch = er_vocab_pairs[idx:idx+self.batch_size] # 切片操作
        targets = np.zeros((len(batch), len(d.entities))) #nentity分类任务


        for idx, pair in enumerate(batch):    # idx为索引，pair为batch元素
            targets[idx, er_vocab[pair]] = 1.    # 将targets矩阵正样本对应位置赋值为1
            targets[idx,relation_tail[pair[1]]] = 1
        targets = torch.FloatTensor(targets)
        if self.cuda:
            targets = targets.cuda()

        return np.array(batch), targets
# 把真三元组涉及到的实体预测值改为0，这样排名的时候自动排到末位
#原始filt
# filt = er_vocab[(data_batch[j][0], data_batch[j][1])]

#修改后filt

filt = relation_tail[r_idx[j]]
target_value = predictions[j,e2_idx[j]].item()
predictions[j, filt] = 0.0
predictions[j, e2_idx[j]] = target_value



```

## 5-22

向量乘以对角矩阵等价于向量与对角矩阵对角线元素的对位相乘

Hamilton product

修改dismult

原打分函数

```python
    def DistMult(self, head, relation, tail, mode):
        if mode == 'head-batch':
            score = head * (relation * tail)
        else:
            score = (head * relation) * tail

        score = score.sum(dim=2)
        return score
```

根据SeAttrE修改后的模型k=2

```python
    def SeAttrE(self, head, relation, tail, mode):
        re1, re2 = relation.chunk(2, -1)
        if mode == 'head-batch':
            score = head * (re1 * tail + re2 * tail)
        else:
            score = (head * re1 + head * re2) * tail
        score = score.sum(dim=2)
        return score
```

据SeAttrE表述，dismult是SeAttrE k=1时的特例，现在将k设置为2![image-20220522213656852](E:\gitee\modige\_posts\image-20220522213656852.png)

由于向量乘以对角矩阵的本质就是向量与对角矩阵对角元素组成的向量的哈密尔顿积，则当k=2时就是由一个关系向量改为两个关系向量

{'MRR': 0.3236813120758703, 'MR': 148.949500998004, 'HITS@1': 0.22897063016823496, 'HITS@3': 0.35508982035928144, 'HITS@10': 0.518562874251497}
2022-05-22 22:08:49,548 INFO     Valid MRR at step 99999: 0.323681
2022-05-22 22:08:49,548 INFO     Valid MR at step 99999: 148.949501
2022-05-22 22:08:49,548 INFO     Valid HITS@1 at step 99999: 0.228971
2022-05-22 22:08:49,548 INFO     Valid HITS@3 at step 99999: 0.355090
2022-05-22 22:08:49,548 INFO     Valid HITS@10 at step 99999: 0.518563
2022-05-22 22:08:49,548 INFO     Evaluating on Test Dataset...
2022-05-22 22:08:50,063 INFO     Evaluating the model... (0/2560)
2022-05-22 22:09:13,308 INFO     Evaluating the model... (1000/2560)
2022-05-22 22:09:37,245 INFO     Evaluating the model... (2000/2560)
{'MRR': 0.3174775948549614, 'MR': 159.9930128017199, 'HITS@1': 0.22234437603830742, 'HITS@3': 0.3481872373692954, 'HITS@10': 0.51363236587511}





 Start Training...
2022-05-22 22:23:15,689 INFO     init_step = 99999
2022-05-22 22:23:15,689 INFO     batch_size = 1024
2022-05-22 22:23:15,689 INFO     negative_adversarial_sampling = 0
2022-05-22 22:23:15,689 INFO     hidden_dim = 2000
2022-05-22 22:23:15,689 INFO     gamma = 12.000000
2022-05-22 22:23:15,689 INFO     negative_adversarial_sampling = False
2022-05-22 22:23:15,689 INFO     Evaluating on Test Dataset...
2022-05-22 22:23:16,160 INFO     Evaluating the model... (0/2560)
2022-05-22 22:23:38,818 INFO     Evaluating the model... (1000/2560)
2022-05-22 22:24:01,768 INFO     Evaluating the model... (2000/2560)
2022-05-22 22:24:14,573 INFO     {'MRR': 0.30867433718395054, 'MR': 175.00596110622496, 'HITS@1': 0.22080523795563373, 'HITS@3': 0.33755985536988176, 'HITS@10': 0.4854148343594254}

## 5-23

```
            


```

## 之前模型效果差的原因之一是训练与测试用的不是一个forward

### 引入两个r向量

```python
    def head_and_tail(self, e1_idx, r_idx):

        e1 = self.E(e1_idx)
        x = self.bn0(e1)
        x = self.input_dropout(x)

        x = x.view(-1, 1, e1.size(1))  # [1024,1,200]

        r_head,r_tail = self.R(r_idx).chunk(2,-1)  # [1024,200]

        W_head = torch.mm(r_head, self.W.view(r_head.size(1), -1))  # W[200,200,200]
        W_head = W_head.view(-1, e1.size(1), e1.size(1))
        W_head = self.hidden_dropout1(W_head)

        W_tail = torch.mm(r_tail, self.W.view(r_tail.size(1), -1))  # W[200,200,200]
        W_tail = W_tail.view(-1, e1.size(1), e1.size(1))
        W_tail = self.hidden_dropout1(W_tail)


        x1 = torch.bmm(x, W_head)
        x2 = torch.bmm(x,W_tail)

        x = x1+x2

        x = x.view(-1, e1.size(1))
        x = self.bn1(x)
        x = self.hidden_dropout2(x)
        x = torch.mm(x, self.E.weight.transpose(1, 0))
        pred = torch.sigmoid(x)
        return pred



```

### 效果

```
Validation:
2022-05-23 17:38:05 INFO     Hits @10: 0.537610493299116
2022-05-23 17:38:05 INFO     Hits @3: 0.3890504704875962
2022-05-23 17:38:05 INFO     Hits @1: 0.26592529227259765
2022-05-23 17:38:05 INFO     Mean rank: 154.93798118049614
2022-05-23 17:38:05 INFO     Mean reciprocal rank: 0.35605798561934926
2022-05-23 17:38:05 INFO     [0.5335190071337829, 0.5190804260725105, 0.25869735170526725, 167.5781051500049, 0.35068541887642996]
2022-05-23 17:38:05 INFO     Test:
2022-05-23 17:38:15 INFO     Hits @10: 0.534178637740643
2022-05-23 17:38:15 INFO     Hits @3: 0.38573732043389036
2022-05-23 17:38:15 INFO     Hits @1: 0.25943027460177853
2022-05-23 17:38:15 INFO     Mean rank: 167.04651617316526
2022-05-23 17:38:15 INFO     Mean reciprocal rank: 0.3513226018744017
2022-05-23 17:38:15 INFO     [0.5335190071337829, 0.5190804260725105, 0.25869735170526725, 167.5781051500049, 0.35068541887642996]
2022-05-23 17:38:15 INFO     10.366660833358765

```

### 使用平均x

```
Number of data points: 35070
2022-05-23 22:05:19,255 INFO     Hits @10: 0.5376675220986599
2022-05-23 22:05:19,256 INFO     Hits @3: 0.38907898488736814
2022-05-23 22:05:19,257 INFO     Hits @1: 0.26592529227259765
2022-05-23 22:05:19,259 INFO     Mean rank: 154.94145993726832
2022-05-23 22:05:19,261 INFO     Mean reciprocal rank: 0.35606135195010735
2022-05-23 22:05:19,261 INFO     [0.5334701456073487, 0.5190804260725105, 0.25867292094205024, 167.5821117951725, 0.3506551023837021]
2022-05-23 22:05:19,281 INFO     Test:
Number of data points: 40932
2022-05-23 22:05:29,544 INFO     Hits @10: 0.534178637740643
2022-05-23 22:05:29,545 INFO     Hits @3: 0.3857128896706733
2022-05-23 22:05:29,546 INFO     Hits @1: 0.25938141307534446
2022-05-23 22:05:29,549 INFO     Mean rank: 167.0465650346917
2022-05-23 22:05:29,551 INFO     Mean reciprocal rank: 0.3513034515108644
2022-05-23 22:05:29,551 INFO     [0.5334701456073487, 0.5190804260725105, 0.25867292094205024, 167.5821117951725, 0.3506551023837021]
2022-05-23 22:05:29,585 INFO     10.303548812866211

```



### 转置核张量

```python
    def head_project(self,e1_idx, r_idx):
        # 转置核张量，头尾分别嵌入
        e1 = self.E(e1_idx)
        x = self.bn0(e1)
        x = self.input_dropout(x)
        x = x.view(-1, 1, e1.size(1))  # [1024,1,200]
        r_head, r_tail = self.R(r_idx).chunk(2, -1)  # [1024,200]

        W_head = torch.mm(r_head, self.W.view(r_head.size(1), -1))  # W[200,200,200]
        W_head = W_head.view(-1, e1.size(1), e1.size(1))
        W_head = self.hidden_dropout1(W_head)

        W_tail = torch.mm(r_tail, self.W.transpose(1,2).reshape(r_tail.size(1), -1))  # W[200,200,200]
        W_tail = W_tail.view(-1, e1.size(1), e1.size(1))
        W_tail = self.hidden_dropout1(W_tail)

        x1 = torch.bmm(x, W_head)
        x2 = torch.bmm(x, W_tail)

        x = (x1 + x2) / 2

        x = x.view(-1, e1.size(1))
        x = self.bn1(x)
        x = self.hidden_dropout2(x)
        x = torch.mm(x, self.E.weight.transpose(1, 0))
        pred = torch.sigmoid(x)
        return pred
```

### 实验效果

```
2022-05-23 23:11:51,023 INFO     Hits @10: 0.5433989164528087
2022-05-23 23:11:51,024 INFO     Hits @3: 0.3949244368406045
2022-05-23 23:11:51,025 INFO     Hits @1: 0.2672084402623325
2022-05-23 23:11:51,027 INFO     Mean rank: 156.0013116623895
2022-05-23 23:11:51,029 INFO     Mean reciprocal rank: 0.3591221408096626
2022-05-23 23:11:51,029 INFO     [0.5382830059611062, 0.5301475618098309, 0.2616779048177465, 168.49486953972442, 0.3545412380224741]
2022-05-23 23:11:51,049 INFO     Test:
Number of data points: 40932
2022-05-23 23:12:01,414 INFO     Hits @10: 0.5384295905404085
2022-05-23 23:12:01,416 INFO     Hits @3: 0.3874963353855174
2022-05-23 23:12:01,417 INFO     Hits @1: 0.26136030489592493
2022-05-23 23:12:01,419 INFO     Mean rank: 170.83980748558585
2022-05-23 23:12:01,421 INFO     Mean reciprocal rank: 0.3536654196353958
2022-05-23 23:12:01,422 INFO     [0.5382830059611062, 0.5301475618098309, 0.2616779048177465, 168.49486953972442, 0.3545412380224741]

```

### 一千步（head_and_tail）

```
2022-05-24 12:35:55,542 INFO     Validation:
Number of data points: 35070
2022-05-24 12:36:04,522 INFO     Hits @10: 0.545566010835472
2022-05-24 12:36:04,523 INFO     Hits @3: 0.3962360992301112
2022-05-24 12:36:04,524 INFO     Hits @1: 0.27174222982606216
2022-05-24 12:36:04,526 INFO     Mean rank: 155.24616481323068
2022-05-24 12:36:04,528 INFO     Mean reciprocal rank: 0.36285528652855675
2022-05-24 12:36:04,528 INFO     [0.5429004202091273, 0.5190804260725105, 0.2641942734291019, 165.78576663734975, 0.35703617554911793]
2022-05-24 12:36:04,548 INFO     Test:
Number of data points: 40932
2022-05-24 12:36:14,901 INFO     Hits @10: 0.5411658360207173
2022-05-24 12:36:14,902 INFO     Hits @3: 0.3910632268152057
2022-05-24 12:36:14,903 INFO     Hits @1: 0.262532981530343
2022-05-24 12:36:14,905 INFO     Mean rank: 171.3871543047005
2022-05-24 12:36:14,908 INFO     Mean reciprocal rank: 0.35608133559029365
2022-05-24 12:36:14,908 INFO     [0.5429004202091273, 0.5190804260725105, 0.2641942734291019, 165.78576663734975, 0.35703617554911793]
2022-05-24 12:36:14,935 INFO     10.38675045967102

```



### 原模型1000步

路径注意力

```
022-05-25 13:55:23,890 INFO     Hits @10: 0.3909894496720844
2022-05-25 13:55:23,891 INFO     Hits @3: 0.24833190761334473
2022-05-25 13:55:23,892 INFO     Hits @1: 0.17014542343883662
2022-05-25 13:55:23,894 INFO     Mean rank: 302.27978329056174
2022-05-25 13:55:23,896 INFO     Mean reciprocal rank: 0.24018179655392047
2022-05-25 13:55:23,896 INFO     [0.3951187335092348, 0.27731359327665395, 0.170282419622789, 287.15403596208347, 0.24175001613232486]
2022-05-25 13:55:23,914 INFO     Test:
Number of data points: 40932
2022-05-25 13:55:34,124 INFO     Hits @10: 0.3864458125671846
2022-05-25 13:55:34,126 INFO     Hits @3: 0.24552917033128116
2022-05-25 13:55:34,127 INFO     Hits @1: 0.1656161438483338
2022-05-25 13:55:34,129 INFO     Mean rank: 308.67064888107103
2022-05-25 13:55:34,131 INFO     Mean reciprocal rank: 0.2360798388570739
2022-05-25 13:55:34,131 INFO     [0.3951187335092348, 0.27731359327665395, 0.170282419622789, 287.15403596208347, 0.24175001613232486]

```



### head_project1000步

```
2022-05-24 14:48:53,839 INFO     Hits @10: 0.5482178500142572
2022-05-24 14:48:53,840 INFO     Hits @3: 0.39689193042486454
2022-05-24 14:48:53,841 INFO     Hits @1: 0.271285999429712
2022-05-24 14:48:53,843 INFO     Mean rank: 164.71978899344168
2022-05-24 14:48:53,845 INFO     Mean reciprocal rank: 0.36329347962577396
2022-05-24 14:48:53,845 INFO     [0.5447327274504056, 0.5301475618098309, 0.26653962669793807, 168.49486953972442, 0.3595116415349159]
2022-05-24 14:48:53,865 INFO     Test:
Number of data points: 40932
2022-05-24 14:49:04,315 INFO     Hits @10: 0.5456610964526531
2022-05-24 14:49:04,316 INFO     Hits @3: 0.3919183035278022
2022-05-24 14:49:04,318 INFO     Hits @1: 0.2633880582429395
2022-05-24 14:49:04,320 INFO     Mean rank: 176.0369637447474
2022-05-24 14:49:04,322 INFO     Mean reciprocal rank: 0.3571727879791276
2022-05-24 14:49:04,322 INFO     [0.5447327274504056, 0.5301475618098309, 0.26653962669793807, 168.49486953972442, 0.3595116415349159]
2022-05-24 14:49:04,351 INFO     10.485853910446167

```

## 

# WN18RR

## TuckER 500步

```
Number of data points: 6068
2022-05-24 16:15:41,852 INFO     Hits @10: 0.5087343441001978
2022-05-24 16:15:41,852 INFO     Hits @3: 0.4673698088332235
2022-05-24 16:15:41,852 INFO     Hits @1: 0.43045484508899146
2022-05-24 16:15:41,853 INFO     Mean rank: 6263.144858272907
2022-05-24 16:15:41,853 INFO     Mean reciprocal rank: 0.4573567916596436
2022-05-24 16:15:41,853 INFO     [0.518985322271857, 0.48851308232291, 0.43059987236758135, 6472.665762603701, 0.4605156963884986]
2022-05-24 16:15:41,866 INFO     Test:
Number of data points: 6268
2022-05-24 16:15:44,948 INFO     Hits @10: 0.5193044033184429
2022-05-24 16:15:44,948 INFO     Hits @3: 0.473356732610083
2022-05-24 16:15:44,948 INFO     Hits @1: 0.43251435864709636
2022-05-24 16:15:44,949 INFO     Mean rank: 6460.4098596043395
2022-05-24 16:15:44,949 INFO     Mean reciprocal rank: 0.4613729449439943
2022-05-24 16:15:44,949 INFO     [0.518985322271857, 0.48851308232291, 0.43059987236758135, 6472.665762603701, 0.4605156963884986]

```

现在平等比较所有候选样本打分是否合理？参考其他模型中的负采样，能够对候选样本进行某种运算再求损失



```
CUDA_VISIBLE_DEVICES=1 python main.py --dataset FB15k-237 --batch_size 512 --model_name head_project --mode train --num_iterations 1000 --edim 350 --rdim 350 

```

|              | mrr    | mr   | hit@1  | hit@3  | hit@10 |
| ------------ | ------ | ---- | ------ | ------ | ------ |
| ours(50edim) | 363.44 | 362. | 0.1736 | 0.2628 | 0.375  |
|              |        |      |        |        |        |
|              |        |      |        |        |        |

## 5-27

### 学习笔记

```
我希望一个CNN不仅仅告诉我它在某张图像上检测到一只小鸟，我还要CNN明确的告诉我，它用第一个filter去监测鸟头，第二个filter去检测鸟尾巴。因为这两个filter被这张图像触发，所以判断出图像中有一只小鸟。进一步，当我知道鸟的分类得分是0.7，我还希望CNN给出鸟头部分贡献了0.3的分数，鸟尾贡献了0.2。当CNN内部逻辑足够条理清晰，我们是否还需要通过大数据进行端对端的训练？我们能否在语义层面直接debug CNN呢？

作者：Qs.Zhang张拳石
链接：https://zhuanlan.zhihu.com/p/30074544
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
```

