



## 多线程爬取

![](C:\Users\modige\AppData\Roaming\Typora\typora-user-images\image-20211120204556685.png)



| 任务  | 任务描述                   |
| ----- | -------------------------- |
| 任务1 | 根据url爬取网页源代码      |
| 任务2 | 获取网页源代码中的所有词条 |
| 任务3 | 解析网页源代码中的有效信   |
|       |                            |



# 数据结构

| 数据库  | 类型       | 名称       | 作用                                   |
| ------- | ---------- | ---------- | -------------------------------------- |
| Redis   | 集合set    | words:     | 存放已经出现过的词条                   |
| Redis   | 集合set    | url_wait   | 等待获取源网页的词条                   |
| Redis   | 集合set    | visited    | 已经获取源网页且解析出结构化信息的网页 |
| MongoDB | collection | wiki_pages | 网页源代码                             |
| MongoDB | collection | wiki_info  | 网页结构化信息                         |
|         |            |            |                                        |

## 数据库的四大特性





## 需求

三个任务可以并行完成

爬取一个词条需要1s,过慢，如何通过多线程等手段加快爬取速度

## 爬取页面新方案

1. 从url_wait中弹出100条url

2. 爬取这100条url的页面，爬取失败的页面重新装入url_wait,等待下一次爬取

**这样做是为了利用redis的原子性，多个程序访问数据库时避免出现被重复爬取的url**

（程序A取出Redis中的100条url之前，程序B不能开始取URL）

**循环条件可以尽量设置得宽松，当出错超过10或20次时跳出循环**

## 解析页面新方案

需要解析的url即wiki_pages与wiki_info的差集

解析url与结构化信息可以同时进行

每半小时扫描一遍MongoDB，找出URL，进行解析



## 解析

# 附录：基础知识

## 数据库四大特性ACID

**ACID**，是指[数据库管理系统](https://zh.wikipedia.org/wiki/数据库管理系统)（[DBMS](https://zh.wikipedia.org/wiki/DBMS)）在写入或更新资料的过程中，为保证[事务](https://zh.wikipedia.org/wiki/数据库事务)（transaction）是正确可靠的，所必须具备的四个特性：[原子性](https://zh.wikipedia.org/w/index.php?title=原子性&action=edit&redlink=1)（atomicity，或称不可分割性）、[一致性](https://zh.wikipedia.org/wiki/一致性_(数据库))（consistency）、[隔离性](https://zh.wikipedia.org/wiki/隔離性)（isolation，又称独立性）、[持久性](https://zh.wikipedia.org/w/index.php?title=持久性&action=edit&redlink=1)（durability）。

* 原子性：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被[回滚](https://zh.wikipedia.org/wiki/回滚_(数据管理))（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。
* 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设[约束](https://zh.wikipedia.org/wiki/数据完整性)、[触发器](https://zh.wikipedia.org/wiki/触发器_(数据库))、[级联回滚](https://zh.wikipedia.org/wiki/级联回滚)等。
* [事务隔离](https://zh.wikipedia.org/wiki/事務隔離)（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
* 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

## 进程

​       进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是[操作系统](https://baike.baidu.com/item/操作系统)结构的基础。在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。

## **线程**（英语：thread）

​         线程是[操作系统](https://zh.wikipedia.org/wiki/操作系统)能够进行运算[调度](https://zh.wikipedia.org/wiki/调度)的最小单位。大部分情况下，它被包含在[进程](https://zh.wikipedia.org/wiki/进程)之中，是[进程](https://zh.wikipedia.org/wiki/进程)中的实际运作单位。一条线程指的是[进程](https://zh.wikipedia.org/wiki/进程)中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。

## 

## 爬取页面

```python
## 爬取wiki优化版
## 两台机器并行
## 一台爬取原网页（有马上6的winddows系统)，另一台处理文本，不能联网的ubuntu主机
import time

import pymongo
import redis
import requests
from  bs4 import BeautifulSoup
class WikiPage:
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
                          ' AppleWebKit/537.36 (KHTML, like Gecko)'
                          ' Chrome/95.0.4638.69 Safari/537.36 Edg/95.0.1020.44'}
    r = redis.StrictRedis(host='localhost', port=6379, db=1, decode_responses=True)
    myclient = pymongo.MongoClient("mongodb://localhost:27017/")
    mongodb = myclient["wiki"]
    collection = mongodb.wiki_pages
    index = 0
    ## 从wait_url中取出100条url
    def get_wait_url(self):
        url = self.r.spop('url_wait', 100)
        return url
    def get_html(self,url):
        html = requests.get(url, self.headers, proxies={"http": "127.0.0.1:7890", "https": "127.0.0.1:7890"})
        html.encoding = 'utf-8'
        page = dict()
        page['url'] = url
        page['html'] = html.text
        self.collection.insert_one(page)


if __name__ == '__main__':
    index = 1
    count = 0
    while count < 50:
        w = WikiPage()
        urls = w.get_wait_url()
        print(len(urls))
        for url in urls:
            try:
                w.get_html('https://zh.wikipedia.org'+url)
                print(str(index)+"：已完成")
                index += 1
            except Exception as e:
                w.r.sadd('wait_url',url)
                print(e)
                time.sleep(60)
                with open('wrong.txt','a') as f:
                    f.write(str(e))
        count += 1


```

## 解析网页

```python
## 检查MongoDB中是否有重复词条
import time

import pymongo
import redis
from bs4 import BeautifulSoup

from langconv import Converter


class AAA:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36 Edg/95.0.1020.44'}
        self.r = redis.StrictRedis(host='localhost', port=6379, db=1, decode_responses=True)
        self.myclient = pymongo.MongoClient("mongodb://localhost/")
        self.mongodb = self.myclient["wiki"]
        self.wiki_pages = self.mongodb.wiki_pages
        self.wiki_info = self.mongodb.wiki_info
        self.index = 1
    # 检查Redis中存放等待被查询的页面是否已经被查询过了
    # 即url_wait 与 MongoDB中的wiki_pages是否有交集
    # 若有，从wait_url中删除这些重复的，并返回

    def check_url_wait_repeat(self):
        url_wait = self.r.smembers('url_wait') #  返回一个set
        url_visited = self.wiki_pages.find()
        repeat = []  # 存放重复URL

        for i in url_visited:
            if i['url'] in url_wait:
                repeat.append(i['url'])
        if len(repeat)>0:
            for url in repeat:
                self.r.srem('url_wait',url)
                print(url+"已删除")
    ## 返回已经被抓取界面但还没被解析出结构化信息的url
    # 即wiki_page 与 wiki_info 中url的差集
    def get_unparse_url(self):
        wiki_info_url = []
        unparse_url = []
        for item in  self.wiki_info.find():
            wiki_info_url.append(item['url'])
        for item in self.wiki_pages.find():
            if item['url'] not in wiki_info_url:
                unparse_url.append(item['url'])
        return unparse_url
    # 解析出wiki页面中的结构化信息
    def get_content_in_page(self,url):
        html = self.wiki_pages.find_one({'url':url})
        soup = BeautifulSoup(html['html'], "html.parser")
        # 得到需要的内容
        entity = dict()
        entity['url'] = url
        # 标题
        title =soup.select('#firstHeading')
        if title:
            title = title[0].get_text()
        entity['title']  = Converter("zh-hans").convert(title).replace('\xe6',"")
        # 摘要
        tag_p = soup.findAll('p')
        text = ''
        for t in tag_p:
            text = text +Converter("zh-hans").convert(t.get_text()).replace('\xe6',"")
        entity['text'] = text
        # 分类
        res = soup.select('#mw-normal-catlinks > ul > li > a')
        cate = []
        for r in res:
            cate.append(Converter("zh-hans").convert(r.get_text()).replace('\xe6',""))
        entity['category'] = cate
        try:
            result = self.wiki_info.insert_one(entity)
        except Exception as e:
            print(e)
            print("get_content_in_page出错")

    def get_url_in_page(self,url):
        # 得到一个页面中的所有指向wiki的url，并存入wait_url
        html = self.wiki_pages.find_one({'url': url})
        html = html['html']
        # html.encoding = 'utf-8'
        soup = BeautifulSoup(html, "html.parser")
        names = soup.findAll('a')
        for name in names:
            try:
                href = name.get('href')
                if href is not None and href.startswith('/wiki/%'):
                    ## 三个字任务各一条队列
                    if (self.r.sadd('words', href)) == 1:
                        self.r.sadd('url_wait',href)
                        print('已完成,url_wait长度为'+str(self.r.scard('url_wait')))
            except Exception as e:
                print(e)
                print("get_url_in_page错误")



if __name__ == '__main__':
    count = 1
    while count <100:
        print("第"+str(count)+"次扫描")
        u = AAA()
        index = 1
        unparse_url = u.get_unparse_url()
        for url in unparse_url:
            try:
                print("==========开始收集页面中url=============")
                u.get_url_in_page(url)
            except Exception as e:
                print(e)
            try:
                print("==========开始解析页面中的结构化信息=============")
                u.get_content_in_page(url)
            except Exception as e:
                print(e)
            print("已完成"+str(index)+"/"+str(len(unparse_url)))
            index += 1
    # 半小时扫描一次
    time.sleep(1800)


```



