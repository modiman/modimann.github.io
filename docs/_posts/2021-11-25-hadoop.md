# 尚硅谷Hadoop教程

## 环境准备

### 1. 虚拟机安装

最初使用vmware15.5,打开虚拟机后电脑蓝屏

按照网上的方法逐步修改，分别

* 关闭hV服务
* 打开虚拟机服务等操作

其中遇到的错误：打开本地计算机策略时找不到gpedit.msc，解决办法

```bash
@echo off

pushd "%~dp0"

dir /b C:\Windows\servicing\Packages\Microsoft-Windows-GroupPolicy-ClientExtensions-Package~3*.mum >List.txt

dir /b C:\Windows\servicing\Packages\Microsoft-Windows-GroupPolicy-ClientTools-Package~3*.mum >>List.txt

for /f %%i in ('findstr /i . List.txt 2^>nul') do dism /online /norestart /add-package:"C:\Windows\servicing\Packages\%%i"

pause
```

将上述代码保存为xx.cmd，以管理员身份运行，此后可以找到gpedit.msc

但又发现没有设备守护或Device Guard，说明问题不在这

于是到官网下载最新的vmware16.2,虚拟机可以运行

### 2.安装centos

可以按默认操作

### 3.配置网络

三处需要配置

1. 虚拟机顶部菜单栏中--》编辑--》虚拟网络编辑器--》选择VMnet8--》子网设置为192.168.10.0 

2. windows-->更改网络适配器选项--》VMnet8-->右键--》属性--》IPV4-->使用下面的ip地址--》192.168.10.1--》DNS：192.168.10.2

3. centos系统

   * vi /etc/sysconfig/network-scripts/ifcfg-ens33

   添加以下内容

   ```
   IPADDR= 192.168.10.100
   GATEWAY=192.168.10.2
   DNS1=192.168.10.2
   DNS2=8.8.8.8
   ```

   **发现没有用**

于是在图形化界面中手动设置ip

设置--》网络--》ip

填写上述信息

此时可以执行以下信息

```bash
ip addr|grep 192
ping www.baidu.com
ping 172.21.7.172 #本机
```

都成功后说明网络配置成功

### 远程连接虚拟机

下载xshell

连接上面设置好的centos虚拟机

使用方法与的地方接服务器相同。，虚拟机ip为192.168.10.100

### 主机映射虚拟机

找到windows下的 C://windows/system32/drivers/etc/hosts

粘贴以下内容

```
192.168.10.100 hadoop100
192.168.10.101 hadoop101
192.168.10.102 hadoop102
192.168.10.103 hadoop103
192.168.10.104 hadoop104
192.168.10.105 hadoop105
192.168.10.106 hadoop106
192.168.10.107 hadoop107
192.168.10.108 hadoop108
```

结束之后在xshell连接虚拟机时可以把原来的主机一项中的ip192.168.10.100替换为hadoop100

### 给centos安装必要的依赖

```bash
yum install -y epel-release
#关闭防火墙
systemctl stop firewalld
systemctl disable firewalld.service
```

## 跟着教程做，出问题的地方

### 启动hadoop集群时出现问题

```bash
[root@hadoop102 bin]# start-dfs.sh
Starting namenodes on [hadoop102]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop104]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.

```

**此处是因为使用root用户而非hadoop用户**

切换至hadoop用户

报错

```bash
[root@hadoop102 hadoop-3.3.0]# su hadoop
[hadoop@hadoop102 hadoop-3.3.0]$ start-dfs.sh
Starting namenodes on [hadoop102]
hadoop102: Warning: Permanently added 'hadoop102,192.168.10.102' (ECDSA) to the list of known hosts.
hadoop102: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
Starting datanodes
hadoop102: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
hadoop104: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
hadoop103: WARNING: /opt/module/hadoop-3.3.0/logs does not exist. Creating.
hadoop103: mkdir: 无法创建目录"/opt/module/hadoop-3.3.0/logs": 权限不够
hadoop103: ERROR: Unable to create /opt/module/hadoop-3.3.0/logs. Aborting.
Starting secondary namenodes [hadoop104]
hadoop104: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
```

这有两个错误

#### 错误一：hadoop102:Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).

这是因为在免密使用ssh那一步中，将公钥转发给了103/104却没转发给自己，导致拒绝访问

解决办法：将公钥粘贴给自己

```bash
ssh-copy-id hadoop102
```

#### 错误二：hadoop103: mkdir: 无法创建目录"/opt/module/hadoop-3.3.0/logs": 权限不够

给hadoop103和hadoop104上的/opt/module/hadoop-3.3.0文件夹更高的权限

```
sudo chmod -R 777 /opt/module/hadoop-3.3.0
```

至此可以启动

# 下一步可以进行JavaAPI 操作

# Windows下安装

* 尚硅谷教程

1. 将安装包放在合适的位置并添加环境变量

```
D:\hadoop-2.7.2\bin
```

2. 添加JAVA_HOME环境变量

```
JAVA_HOME
C:\PROGRA~1\Java\jdk1.8.0_131
```

由于C:\Program Files\Java\jdk1.8.0_121路径中有空格会导致编译出错，因此要改成上面的形式

3. 创建一个Maven工程HdfsClientDemo
4. 导入相应的依赖坐标+日志添加

```xml
 <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>RELEASE</version>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.8.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
        <groupId>jdk.tools</groupId>
            <artifactId>jdk.tools</artifactId>
            <version>1.8</version>
            <scope>system</scope>
            <systemPath>${JAVA_HOME}/lib/tools.jar</systemPath>
        </dependency>
    </dependencies>

```

5. 创建包名com.atguigu.hdfs
6. 创建HdfsClient类

```java
package com.atguigu.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.junit.Test;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

public class HdfsClient{
    @Test
    public void testMkdirs() throws IOException, InterruptedException, URISyntaxException {

        // 1 获取文件系统
        Configuration configuration = new Configuration();
        // 配置在集群上运行
        // configuration.set("fs.defaultFS", "hdfs://hadoop102:8020");
        // FileSystem fs = FileSystem.get(configuration);

        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "hadoop");?/对应系统用户名

        // 2 创建目录
        fs.mkdirs(new Path("/1108/daxian/banzhang"));

        // 3 关闭资源
        fs.close();
    }
}

```

**补充步骤**

* 添加主机

打开

```
C:\Windows\System32\drivers\etc\hosts
```

添加

```
127.0.0.1 hadoop102
```

* 启动服务

hadoop-2.7.2/sbin下打开cmd

```bash
hadoop namenode -format
start-all.cmd
```

