# FB15k237数据集

## fb15k数据集分析

| 类型                                                         | 数量   |
| ------------------------------------------------------------ | ------ |
| 实体                                                         |        |
| 关系                                                         |        |
| 训练集                                                       | 272115 |
| 测试集                                                       |        |
| 验证集                                                       |        |
| 出现次数最多关系	/award/award_nominee/award_nominations./award/award_nomination/award_nominee | 15989  |
| 出现次数最多头实体 /m/09c7w0                                 | 1324   |
| 出现次数最多尾实体 /m/09c7w0                                 | 6288   |
| 1-1关系                                                      |        |
| 1-n关系                                                      |        |
| n-1关系                                                      |        |
| n-n关系                                                      |        |

* 1-1头实体尾实体都不能重复

* 1-n 尾实体可以重复

* n-1头实体可以重复

* n-n头尾都能重复

这有很多一对多、多对一关系三元组，怎么解决



# 问题

## 为什么初始损失大且训练不会降低？

起始损失跟gamma有关，后续不改变说明反向传播没有工作

### 推测

1. **没有使用optimzier.step()**
2. 没有保存模型，导致每次都会初始化模型参数

源代码日志

```
2022-03-23 14:05:33,381 INFO     Model: TransE
2022-03-23 14:05:33,382 INFO     Data Path: ./dataset/GoT.csv
2022-03-23 14:05:33,382 INFO     #entity: 2049
2022-03-23 14:05:33,382 INFO     #relation: 10
2022-03-23 14:05:33,382 INFO     #train: 2500
2022-03-23 14:05:33,382 INFO     #valid: 723
2022-03-23 14:05:33,382 INFO     #test: 0
2022-03-23 14:05:33,387 INFO     Model Parameter Configuration:
2022-03-23 14:05:33,387 INFO     Parameter gamma: torch.Size([1]), require_grad = False
2022-03-23 14:05:33,388 INFO     Parameter embedding_range: torch.Size([1]), require_grad = False
2022-03-23 14:05:33,388 INFO     Parameter entity_embedding: torch.Size([2049, 400]), require_grad = True
2022-03-23 14:05:33,388 INFO     Parameter relation_embedding: torch.Size([10, 400]), require_grad = True
```

其中  **Parameter entity_embedding: torch.Size([2049, 400]), require_grad = True**以及**Parameter relation_embedding: torch.Size([10, 400]), require_grad = True**两个参数的梯度应该是问题所在，因为没有梯度信息则无法更新模型参数

**将train_step函数完全替换后没有效果，说明问题不在train_step函数**

重点排查模型参数

损失主要来自正例，会不会是正例样本初始化有问题，还是参数一直没有更新？

破案了，妈的学习率设置错误了

设置成0.0001

跟个小丑一样分析半天，搞笑

### 速度

之前跑的慢是因为在我自己电脑上速度会慢

放在实验室电脑上速度上升

### 日志

日志不打印的问题是因为调用set_logger函数的语句放错位置的

## 为什么初始正负例损失相差悬殊？

## 第一次调通代码

![image-20220323145501121](https://gitee.com/modige/modige/raw/master/_posts/imgs/image-20220323145501121.png)



#### FB15k-237

| Model         | 结束时损失 | MRR       | Hits@1   | Hits@3   | Hits@10  | epoch     | dim    |
| ------------- | ---------- | --------- | -------- | -------- | -------- | --------- | ------ |
| TransE        | 0.204731   | 0.15      | .0.09    | .16      | .27      | 50        | 10     |
| TransE        | 0.027393   | 0.23      | 0.15     | 0.25     | 0.39     | 100       | 100    |
| TransE(tc181) | 0.169015   | 0.32 4486 | 0.222327 | 0.366838 | 0.528172 | 60000     | 500    |
|               |            | 0.15      | .09      | .16      | 0.26     | 50        | 100    |
|               |            | 0.23      | 0.14     | 0.26     | 0.41     | １００    | ３５０ |
| RotatE(tc181) | .145340    | 0.237154  | 0.121129 | .290647  | .455917  | 10000step | 350    |
| RotatE(tc181) | .111790    | .270274   | .161848  | .316938  | .482064  | 20000step |        |
|               |            |           |          |          |          |           |        |
|               |            |           |          |          |          |           |        |

PairRE

{'MRR': 0.25834942984592496, 'MR': 273.7997166031467, 'HITS@1': 0.1677171894849995, 'HITS@3': 0.28642626795661097, 'HITS@10': 0.4416837682009186}

{'MRR': 0.2673943598279943, 'MR': 268.16407700576565, 'HITS@1': 0.17419134173751588, 'HITS@3': 0.29678491156063713, 'HITS@10': 0.4583455487149419}

{'MRR': 0.26299313156417525, 'MR': 273.3866412586729, 'HITS@1': 0.167424020326395, 'HITS@3': 0.29409752760676244, 'HITS@10': 0.4593227792436236}

{'MRR': 0.30461205165120214, 'MR': 186.924582233949, 'HITS@1': 0.2124743476986221, 'HITS@3': 0.33660705560441706, 'HITS@10': 0.48964135639597384}

{'MRR': 0.3003582101458231, 'MR': 211.85128994429786, 'HITS@1': 0.2030685038600606, 'HITS@3': 0.3377553014756181, 'HITS@10': 0.49337926316818137}



## 最佳性能

### TransE

```bash
bash run.sh train TransE FB15k-237 0 0 1024 256 1000 9.0 1.0 0.00005 100000 16

# 实验结果
# 2022-04-13 10:30:31,525 INFO     {'MRR': 0.3293643799871905, 'MR': 158.98311947533503, 'HITS@1': 0.23036783575705733, 'HITS@3': 0.36991730824066155, 'HITS@10': 0.5270031365839749}

# 2022-04-13 10:39:06,283 INFO     {'MRR': 0.32846751208792435, 'MR': 157.20789848873682, 'HITS@1': 0.22723125178214998, 'HITS@3': 0.37005988023952097, 'HITS@10': 0.5283147989734817}

# 2022-04-13 10:48:23,038 INFO     {'MRR': 0.3289050374296591, 'MR': 156.7785856857713, 'HITS@1': 0.2281437125748503, 'HITS@3': 0.3712004562303963, 'HITS@10': 0.5287995437696037}


#  Evaluating on Valid Dataset...
2022-04-13 10:37:04,106 INFO     Evaluating the model... (0/2192)
2022-04-13 10:37:25,816 INFO     Evaluating the model... (1000/2192)
2022-04-13 10:37:47,318 INFO     Evaluating the model... (2000/2192)
2022-04-13 10:37:56,001 INFO     Valid MRR at step 70000: 0.335868
2022-04-13 10:37:56,001 INFO     Valid MR at step 70000: 162.783775
2022-04-13 10:37:56,001 INFO     Valid HITS@1 at step 70000: 0.237753
2022-04-13 10:37:56,001 INFO     Valid HITS@3 at step 70000: 0.375877
2022-04-13 10:37:56,001 INFO     Valid HITS@10 at step 70000: 0.531765

2022-04-13 10:39:11,754 INFO     Evaluating the model... (0/2560)
2022-04-13 10:39:49,434 INFO     Evaluating the model... (1000/2560)
2022-04-13 10:40:05,408 INFO     Evaluating the model... (2000/2560)
2022-04-13 10:40:14,022 INFO     Test MRR at step 70000: 0.329488
2022-04-13 10:40:14,022 INFO     Test MR at step 70000: 174.077690
2022-04-13 10:40:14,022 INFO     Test HITS@1 at step 70000: 0.230749
2022-04-13 10:40:14,022 INFO     Test HITS@3 at step 70000: 0.368025
2022-04-13 10:40:14,022 INFO     Test HITS@10 at step 70000: 0.527656

```

* 维度为200 批数量为256

```
2022-04-13 19:11:00,181 INFO     {'MRR': 0.29445355537490947, 'MR': 219.81822070145424, 'HITS@1': 0.20079840319361278, 'HITS@3': 0.3301682349586541, 'HITS@10': 0.47775876817792984}
```

* 维度为200 批数量为1024

{'MRR': 0.3125382294787655, 'MR': 187.7504397537379, 'HITS@1': 0.21459982409850484, 'HITS@3': 0.34906674484510897, 'HITS@10': 0.5064252907260823}

* 维度为300 批数量为256

{'MRR': 0.3163831109493696, 'MR': 185.29365777386886, 'HITS@1': 0.2174337926316818, 'HITS@3': 0.3558340662562298, 'HITS@10': 0.5122398123717385}

* 维度为500 批数量为256

{'MRR': 0.3204545927970868, 'MR': 173.10558975862406, 'HITS@1': 0.21990129971660316, 'HITS@3': 0.3602560343985146, 'HITS@10': 0.5188849799667742}



| 模型   | 关系 | mrr   | 1     | 3     | 10    | n_size | t_size | 日期 |
| ------ | ---- | ----- | ----- | ----- | ----- | ------ | ------ | ---- |
| pairre | all  | 0.304 | 0.212 | 0.336 | 0.489 | 128    |        | 4-9  |
| pairre | 全   | 0.263 | 0.167 | 0.294 | 0.459 | 128    | 64     |      |
| pair   | 1-1  | 145   | 065   | 161   | 310   | 128    | 64     |      |
| pair   | 1-1  | 145   | 065   | 161   | 310   | 128    | 128    |      |
| pairre | 1-n  | 0.229 | 0.140 | 0.250 | 0.416 | 128    | 128    | 3-31 |
| pairre | n-1  | 0.289 | 0.196 | 0.329 | 0.470 | 128    | 128    | 3-31 |
| pairre | n-n  | 0.337 | 0.213 | 0.369 | 0.616 | 128    | 128    | 3-31 |



TripleRE

{'MRR': 0.2644587104727334, 'MR': 259.6165836020717, 'HITS@1': 0.1904866608032835, 'HITS@3': 0.2851314375061077, 'HITS@10': 0.4121225447082967}

{'MRR': 0.29859315093045563, 'MR': 201.90515977719144, 'HITS@1': 0.20741717971269422, 'HITS@3': 0.3295465650346917, 'HITS@10': 0.4827763119319848}

#### 这是为何？

可以看到1-n关系明显比1-1关系的效果更好

原以为1-1的表现会更好，但事实并非如此

| 模型   | 关系 | mrr  | 1    | 3    | 10   | n_size | t_size | 日期 |
| ------ | ---- | ---- | ---- | ---- | ---- | ------ | ------ | ---- |
| TransE | 1-1  | 141  | 062  | 159  | 308  | 128    | 128    | 4-1  |
| TransE | 1-n  | 219  | 127  | 246  | 407  | 128    | 128    | 4-1  |
| TransE | n-1  | 301  | 213  | 339  | 468  | 128    | 128    | 4-1  |
| TransE | n-n  | 300  | 179  | 334  | 560  | 128    | 128    | 4-1  |
| TransE | all  | 262  | 169  | 295  | 449  | 128    | 128    | 4-1  |
|        |      |      |      |      |      |        |        |      |
|        |      |      |      |      |      |        |        |      |
|        |      |      |      |      |      |        |        |      |



## 训练到十万步之后损失降不下去 

调整学习率是否有用？

Train loss at step 9000: 0.211143

{'MRR': 0.2920551973192261, 'MR': 189.5815743183817, 'HITS@1': 0.1912440144630118, 'HITS@3': 0.3295954265611258, 'HITS@10': 0.4937945861428711}

推测是损失计算方式

```python
uni_weight = True
if uni_weight:
    positive_sample_loss = - positive_score.mean()
    negative_sample_loss = - negative_score.mean()
else:
    positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()
    negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()

```

改为



## RankAE 

新模型，以矩阵表示模型

20维向量训练三万步以后效果

{'MRR': 0.028416315315879667, 'MR': 6237.293902081501, 'HITS@1': 0.016954949672627773, 'HITS@3': 0.030978207759210397, 'HITS@10': 0.05186651030978208}

### 验证集

2022-04-07 09:31:11,719 INFO     {'MRR': 0.29941076943098266, 'MR': 335.5692614770459, 'HITS@1': 0.2219560878243513, 'HITS@3': 0.3272597661819219, 'HITS@10': 0.4531793555745652}

### 测试集



### 4-4进展

模型超参：

```python
class Args:
# 初始化模型参数
    cuda = False
    do_train=False
    do_valid = True
    do_test = True
    evaluate_train = True
    countries = False
    data_path = './data/FB15k-237'
    model = 'RankAE'
    double_entity_embedding = False
    double_relation_embedding = False
    negative_sample_size = 128

    relation_type = 'all'

    hidden_dim  = 200
    gamma = 12.0
    negative_adversarial_sampling = True
    adversarial_temperature = 1.0
    batch_size = 256

    regularization = 0.0

    test_batch_size = 128

    uni_weight = True
    learning_rate = 0.001

    cpu_num = 10
    init_checkpoint = True
    # 模型保存在新路径下
    save_path ='models/RankAE_fb237'

    max_steps = 150000
    warm_up_steps = None

    save_checkpoint_steps = 1000

    valid_steps = 2000
    log_steps = 500
    test_log_steps = 10
    nentity = 0
    nrelation = 0
```

| 模型        | 关系 | mrr    | 1    | 3     | 10    | n_size | t_size | 日期 | MR   |
| ----------- | ---- | ------ | ---- | ----- | ----- | ------ | ------ | ---- | ---- |
| TransE(400) | all  | 316    | 215  | 357   | 517   |        |        | 4-7  |      |
| RankAE      | all  | 213    | 162  | 234   | 313   | 128    | 128    | 4-4  |      |
| RankAE      | 1-1  | 121    | 066  | 134   | 236   | 128    | 128    |      |      |
| RankAE      | 1-n  | 055    | 028  | 056   | 105   | 128    | 128    |      |      |
| RankAE      | n-1  | 304    | 247  | 334   | 402   | 128    | 128    |      |      |
| RankAE      | n-n  | 220    | 158  | 229   | 344   | 128    | 128    |      |      |
| RankAE      | all  | 0.295  | 0.21 | 0.322 | 0.448 | 128    | 128    | 4-7  |      |
| RankAE      | n-1  | 0.2669 | 0.21 | 0.294 | 0.367 | 128    | 128    | 4-7  |      |

2022-04-07 14:37:00,042 INFO     {'MRR': 0.266950924860459, 'MR': 3145.6217239370994, 'HITS@1': 0.21479324403028538, 'HITS@3': 0.2945835760046593, 'HITS@10': 0.36779266161910307}

2022-04-07 16:12:03,235 INFO     {'MRR': 0.28351809339207257, 'MR': 2957.027897495632, 'HITS@1': 0.22900407687827606, 'HITS@3': 0.3140361094933023, 'HITS@10': 0.38642981945253346}



Evaluating the model... (240/274)
2022-04-07 21:16:16,611 INFO     {'MRR': 0.23005628703507086, 'MR': 2160.3810664385514, 'HITS@1': 0.1803250641573995, 'HITS@3': 0.2510692899914457, 'HITS@10': 0.3287425149700599}

损失 0.07

### 4-9

#### learn_rate

* 前3万步 0.001
* 前9万步   0.0001
* 前20万步   0.00001



```python
# 打分函数
# negative_score = F.logsigmoid(-negative_score).mean(dim=1)
#
# positive_score = F.logsigmoid(positive_score).squeeze(dim = 1)
```

{'MRR': 0.21381728937157335, 'MR': 2013.2044854881267, 'HITS@1': 0.1604856835727548, 'HITS@3': 0.23602560343985146, 'HITS@10': 0.3199941366168279}

**问题在于损失下降太快，几万步之后就降到了0.0x**

* 保持0.0001

#### 打分函数

```python
# 打分函数# 
negative_score = F.logsigmoid(-negative_score).mean(dim=1)## 
positive_score = F.logsigmoid(positive_score).squeeze(dim = 1)
```

{'MRR': 0.20764015134366237, 'MR': 2351.0924704387767, 'HITS@1': 0.15432913124206, 'HITS@3': 0.22957588195055215, 'HITS@10': 0.31398416886543534}

**考虑将样本打分求平均改为求和**

#### 打分函数

```python
negative_score = (F.softmax(negative_score * 1.0, dim=1).detach()
                  * F.logsigmoid(-negative_score)).sum(dim=1)
positive_score = F.logsigmoid(positive_score).squeeze(dim = 1)

```

```
2022-04-09 17:55:34 INFO     test MRR at step 199999: 0.214485
2022-04-09 17:55:34 INFO     test MR at step 199999: 2587.059587
2022-04-09 17:55:34 INFO     test HITS@1 at step 199999: 0.165470
2022-04-09 17:55:34 INFO     test HITS@3 at step 199999: 0.234975
2022-04-09 17:55:34 INFO     test HITS@10 at step 199999: 0.312372
```

#### 正则化

PairRE中的正则化在score函数中实现

```
||head|| = ||tail|| = 1
```

实现方式

```python
    def PairRE(self, head, relation, tail, mode):


        re_head, re_tail = torch.chunk(relation, 2, dim=2)

        head = F.normalize(head, 2, -1)
        tail = F.normalize(tail, 2, -1)

        score = head * re_head - tail * re_tail

        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
        return score
```



```python
# _*_ coding=utf-8 _*_
#  @Time    :2022/4/10 20:55
#  @Author  :modige
#  @Description :测试张量乘法

import torch
import torch.nn.functional as ff

def get_distant(head):
    return torch.norm(head,p=1,dim=-1)

if __name__ == '__main__':
    # 初始化张量
    # 一批有256个样本，每个正样本对应4个负样本，隐藏层维度为20
    head = torch.rand(256, 4, 20)
    relation = torch.rand(256, 20, 20)
    tail = torch.rand(256, 4, 20)
    # 第一范数正则化

    head = ff.normalize(head, p=1, dim=-1)
    tail = ff.normalize(tail, p=1, dim=-1)

    # 使用softmax 处理relation,对列求和值为1
    relation = torch.softmax(relation, dim=-2)
    # 不考虑关系时的距离
    print(get_distant(head-tail))
    # 考虑关系时的距离
    print(get_distant(torch.matmul(head,relation[0])-tail))


```

#### 我的正则化

```
|head| = |tail| = 1
relation矩阵每一列的和为1
随机初始化嵌入向量（如果采用均匀分布的方式初始化，会导致softmax或noralize的结果也均匀分布，这样向量与矩阵的乘积始终为1，无法调节模型 ）
```

具体实施，

```python
    def RankAE(self,head,relation,tail,mode,bias):
        # logging.info(head.shape)
        # logging.info(relation.squeeze(1).shape)
        # 将relation由四维压缩至三维
        head = F.normalize(head,p=1,dim=-1)
        tail = F.normalize(tail,p=1,dim=-1)
        relation = relation.squeeze(1)

        relation = F.softmax(relation,dim=-2)
        # print(torch.norm(torch.matmul(head,relation)-tail, p=1, dim=-1))
        # print(head.sum())
        # print(tail.shape)
        # print(torch.norm(head-tail,p=1,dim=-1))



        if mode == 'head-batch':

            score = torch.matmul(tail,relation) - head
            # score = F.softmax(torch.matmul(tail, relation),dim=2) - F.softmax(head,dim=2)
        else:
            score = torch.matmul(head,relation) - tail

            # score = F.softmax(torch.matmul(head, relation),dim=2 ) - F.softmax(tail,dim=2)

        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
        # print(torch.norm(score, p=1, dim=-1))
        # logging.info(torch.norm(score, p=1, dim=2))
        # print(score)

        # logging.info(score)
        return score

```

```python
        self.entity_embedding = nn.Parameter(torch.rand(nentity, self.entity_dim))
		self.relation_embedding = nn.Parameter(torch.rand(nrelation, self.relation_dim,self.relation_dim)

            )
```

### 4-11

模仿PairRE写一个打分函数

```python
score = torch.matmul(tail, re_tail) - torch.matmul(head, re_head)
```

##### 不同处理的结果

1. 未对关系softmax

```python
  
    def RankAE(self,head,relation,tail,mode):
    
        relation = relation.squeeze(1)
 

        re_head,re_tail = torch.chunk(relation,2,2)

        score = torch.matmul(tail, re_tail) - torch.matmul(head, re_head)

        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
 
        return score
'''
2022-04-11 19:41:14,405 INFO     {'MRR': 0.12742918680533108, 'MR': 539.9190761334474, 'HITS@1': 0.06084972911320217, 'HITS@3': 0.12751639577986884, 'HITS@10': 0.27385229540918166}
'''
```

2. 对关系归一化

```python
  
    def RankAE(self,head,relation,tail,mode):
    
        relation = relation.squeeze(1)
 

        re_head,re_tail = torch.chunk(relation,2,2)
      	re_head = F.softmax(re_head,dim=2)
        re_tail = F.softmax(re_tail,dim=2)

        score = torch.matmul(tail, re_tail) - torch.matmul(head, re_head)

        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
 
        return score
# 2022-04-11 20:41:11,186 INFO     {'MRR': 0.08941029407268916, 'MR': 658.3790419161677, 'HITS@1': 0.03635585970915312, 'HITS@3': 0.08696891930424865, 'HITS@10': 0.20034217279726263}

```



3. 对实体归一化（不对关系使用softmax）

```python
def RankAE(self,head,relation,tail,mode):
    
        relation = relation.squeeze(1)
 

        re_head,re_tail = torch.chunk(relation,2,2)

        score = torch.matmul(tail, re_tail) - torch.matmul(head, re_head)

        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
 
        return score
# 2022-04-11 21:31:41 INFO     {'MRR': 0.09695638542662927, 'MR': 452.92711719418304, 'HITS@1': 0.037496435700028516, 'HITS@3': 0.0902480752780154, 'HITS@10': 0.23008269175933846}

```



损失函数反映的是预测值与真实值的差距，设计损失函数应考虑最理想的情况下预测值能否收敛

**向量与矩阵相乘的几何意义为：矩阵是对向量的拉伸和移动**

#### 4-12

**在理想情况下，正例打分为正数，负例打分为负数**

训练好的TransE模型打分示例：

```
#  负例打分矩阵  batch_size * negative_sample_size
tensor([[-7.4683, -7.4305, -4.6976,  ..., -6.5150, -8.1777, -6.0501],
        [-7.2879, -6.6218, -7.1236,  ..., -6.2508, -5.7582, -7.7917],
        [-4.2875, -6.5976, -5.8334,  ..., -7.6899, -7.8039, -8.5617],
        ...,
        [-5.3396, -4.3990, -5.6307,  ..., -5.3808, -5.6939, -6.0155],
        [-7.5595, -5.9336, -8.3219,  ..., -7.0672, -7.7131, -7.3514],
        [-7.3159, -6.3907, -8.1448,  ..., -5.9525, -5.6369, -6.2292]],
       device='cuda:0', grad_fn=<RsubBackward1>)
# 正例打分    batch_size * 1
tensor([[2.8526],
        [1.4609],
        [1.9451],
        ...,
        [3.1919],
        [3.9446],
        [1.5595]], device='cuda:0', grad_fn=<RsubBackward1>)
```

这里的score = gamma - score # gamma=12

**参考上述，希望模型在经过训练之后，能够给正例打出一个【0，12】的分数，给负例打出一个【12,24】的分数，这样能尽量保证正例打分为正数，负例打分为负数**

##### 分析现有的打分函数

```
if mode == 'head-batch':
	score =torch.matmul(tail,relation)-head
else:
	score = torch.matmul(head,relation)-tail
```

其中

* `head,tail∈[-14,14]`

* `relation∈[-14,14]`

一个新的基于翻译的模型，将旋转与拉伸抽象成矩阵（用来表示关系）。参考PairRE，将关系分成两块，分别为re_head,re_tail，用来移动头实体和尾实体，希望经过平移之后的头实体和尾实体无限接进

```python
def RankAE(self,head,relation,tail,mode):

    # 将relation由四维压缩至三维
    head = F.normalize(head,p=1,dim=-1)
    tail = F.normalize(tail,p=1,dim=-1)
    relation = relation.squeeze(1)

    re_head,re_tail = torch.chunk(relation,2,2)

    score = torch.matmul(tail, re_tail) - torch.matmul(head, re_head)

    score = self.gamma.item() - torch.norm(score, p=1, dim=2)
    print(score)
    return score
        
# 2022-04-12 16:38:13,041 INFO     {'MRR': 0.12789119747259764, 'MR': 541.6890504704876, 'HITS@1': 0.06880524664955803, 'HITS@3': 0.12905617336755063, 'HITS@10': 0.25258055317935557}

```

```
# 可以看出打分不合理，正例打分中有部分负数，如何消除这部分负数
tensor([[-10.9566,  -3.6015, -11.4388,  ..., -12.3888, -13.1086,  -7.7097],
        [-25.7448, -61.4515, -63.8659,  ..., -14.2013, -41.9479, -21.8161],
        [-27.8123, -15.2230, -38.4539,  ...,  -9.6707,  -3.8032, -16.4933],
        ...,
        [-60.4790, -32.5939, -19.0900,  ...,  -8.2419, -21.4968, -19.9968],
        [-16.6250, -48.4138,  -6.2616,  ...,  -5.6204, -26.1848, -74.8652],
        [ -0.1679, -11.6307, -37.0040,  ...,  -4.2072, -21.2659,  -5.6354]],
       device='cuda:0', grad_fn=<RsubBackward1>)
tensor([[ -2.9736],
        [  4.9435],
        [  3.7772],
        [  4.9001],
        [  1.6094],
        [  4.4326],
...
    
        [  2.1520],
        [ -4.8825],
        [  0.6641],
        [  4.6088],
        [  2.7532]], device='cuda:0', grad_fn=<RsubBackward1>)

```



##### logsigmoid函数表

| x    | -100 | -10  | -1      | 0       | 1       | 5       | 2       | 3       |      |
| ---- | ---- | ---- | ------- | ------- | ------- | ------- | ------- | ------- | ---- |
| y    | -100 | -10  | -1.3133 | -0.6931 | -0.3133 | -0.0067 | -0.1269 | -0.0486 |      |

![image-20220412195455274](https://gitee.com/modige/modige/raw/master/_posts/imgs/image-20220412195455274.png)

##### sigmoid

![image-20220412195653346](https://gitee.com/modige/modige/raw/master/_posts/imgs/image-20220412195653346.png)

**使用Rotate得到了最佳结果，考虑用Rotate作为基础方法**





### 4-13

##### 良好的打分样式

```python
score = self.gamma.item() - torch.norm(score, p=1, dim=2)
print(score)
       [ 3.3332],        [ 1.3067],        [ 1.8432],        [ 1.0748],        [ 1.4002],        [ 2.4571],
        [ 0.7624],        [ 0.7899],        [ 1.5524],        [ 2.2567],        [ 1.4106],        [ 1.3078]], device='cuda:1', grad_fn=<RsubBackward1>)
tensor([[-3.7442, -4.1623, -2.8789,  ..., -4.7101, -5.2244, -4.0328],
        [-4.0690, -3.7655, -4.6183,  ..., -4.3026, -5.0975, -4.8184],
        [-4.4432, -3.6948, -2.6504,  ..., -3.0819, -2.3491, -2.9527],
        ...,
        [-5.0083, -4.0561, -4.3843,  ..., -1.9999, -4.6484, -6.0739],
        [-1.1330, -0.9614, -1.5570,  ..., -0.5390, -0.5548, -1.2421],
        [-4.9541, -5.2875, -4.1234,  ..., -5.1638, -4.5297, -3.9279]],
################################################
print(torch.norm(score, p=1, dim=2))
score = self.gamma.item() - torch.norm(score, p=1, dim=2)
        [[8.8864],        [7.9080],        [7.3497],        [7.9685],        [7.5659],        [7.0571],        [8.7359],        [6.5707],        [6.7036],        [7.7195],        [7.2954],        [8.0465]], device='cuda:1', grad_fn=<NormBackward1>)
tensor([[ 9.0104, 12.8179,  8.9633,  ..., 12.5459, 13.0442, 12.1168],
        [12.3748, 12.1959, 11.3094,  ..., 11.6008, 11.4025, 11.8258],
        [12.4585, 13.0531, 12.8909,  ..., 13.8334, 13.9281, 12.2755],
        ...,
        [13.1048, 13.1762, 10.2166,  ..., 13.3113, 13.6677, 14.1570],
        [13.7096, 15.2799, 14.0825,  ..., 14.6016, 14.9083, 15.6160],
        [13.1607, 13.7680, 10.6684,  ..., 12.2305, 12.9776, 12.6764]],

```

当前的打分样式

```python
score = self.gamma.item() - torch.norm(score, p=1, dim=2)
print(score)
[2.9314],
        [2.7933],
        [2.2988],
        [5.2664],
        [2.5510],
        [2.2088],
        [7.1266],
        [2.4845],
        [2.1402],
        [2.8471],
        [3.0943],
        [2.8239]], device='cuda:1', grad_fn=<RsubBackward1>)
tensor([[-10.0999, -12.5759, -12.0622,  ..., -12.0303, -11.9649, -12.0259],
        [ -6.0398,  -3.6760,  -4.9237,  ...,  -6.9881,  -4.1971,  -7.3226],
        [ -6.9591,  -8.1318,  -5.8662,  ...,  -7.7970,  -5.4209,  -7.3854],
        ...,
        [ -7.6281,  -7.9432,  -5.7316,  ...,  -7.5425,  -6.6481,  -6.3708],
        [ -5.9159,  -5.7833,  -7.8895,  ...,  -6.6372,  -5.6457,  -6.6715],
        [ -7.7477,  -6.1005,  -6.9843,  ...,  -6.6246,  -7.0557,  -6.2593]],
       device='cuda:1', grad_fn=<RsubBackward1>)

#######################################################################
print(torch.norm(score, p=1, dim=2))
score = self.gamma.item() - torch.norm(score, p=1, dim=2)   
    [7.7165],        [7.3851],        [6.1717],        [7.6597],        [7.5797],        [4.6369],        [6.3454],        [7.3008],        [1.4445],        [7.6685],        [6.8288],        [6.0709],        [6.9473],        [7.4901],        [7.4056],        [7.0302],        [6.8369],        [6.1712],        [6.5701],        [6.7926],        [5.9160],        [7.0610],        [5.1716],        [8.1693],
        [1.0003],
        [7.2113],
        [7.0156],
        [7.0149],
        [2.6214],
        [6.1943],
        [7.5892],
        [6.4955],
        [7.2864]], device='cuda:1', grad_fn=<NormBackward1>)
  0%|                                       | 84/100000 [00:02<32:18, 51.55it/s]tensor([[15.2632, 15.2638, 13.6254,  ..., 13.4831, 14.7855, 15.5960],
        [14.0831, 12.7506, 13.6387,  ..., 13.4232, 13.8130, 14.1605],
        [16.6977, 16.9461, 16.2614,  ..., 17.6516, 18.0486, 15.9559],
        ...,
        [14.2883, 10.0355, 13.9412,  ..., 13.4516, 14.4568, 12.3324],
        [16.6906, 19.0858, 15.4644,  ..., 14.5955, 16.1259, 13.8400],
        [15.1037,  9.8752, 14.4317,  ..., 10.4137, 13.4147, 13.2833]],
       device='cuda:1', grad_fn=<NormBackward1>)
```

### 4-14

* 对head tail正则化
* head* relation矩阵 ，经过sigmoid映射到【0,1】
* 与同为0,1的tail求距离
* 方案不可行

##### 查看filter_bias对测试结果的影响

测试流程

测试样本生成流程

**头批（head-batch）**

​    对于每个正例三元组，全部nentity个实体都作为假head，这意味着negative_sample_size = nentity,需要注意的是，在负例中已经包含了正例，可以根据正例三元组定位到真head的位置

距离说明

```txt
对于测试集中的一个三元组
（1,0,2）
如果是头批（head-batch）,那么生成负例的样式如下
0,1,2.......nentity
```

由此可知，对于一个拥有100个正例三元组的head-batch，为它生成的负例是一个形状为100*nentity的矩阵

**需要注意的是，由于在测试时的负采样是采集所有实体，那么不可避免地负例可能也是实际存在于训练集中的三元组，那么这种负例的打分也是较高的，甚至可能高于当前正例的打分，因此需要引入一个filter_bias矩阵来手动将这部分实体的打分拉低**

在当前的实验设置中，filter_bias是一个形状与负例相同的矩阵，它存放着0或-1,

* 存放0表示对应位置的假头实体与正例中关系和尾实体组成的三元组没有在训练集中出现过，所以无需操作
* 存放-1表示出现过，在测试时需要把这个实体的打分手动降低以免影响正例打分的排名

**问题是，filter_bias的值是否有更加合理的设置方法**？

* -1方案

{'MRR': 0.29997343198650755, 'MR': 215.6084970194469, 'HITS@1': 0.20578031857715234, 'HITS@3': 0.3371933939216261, 'HITS@10': 0.4842177269617903}

* 0.5方案

{'MRR': 0.29997343198650755, 'MR': 215.6084970194469, 'HITS@1': 0.20578031857715234, 'HITS@3': 0.3371933939216261, 'HITS@10': 0.4842177269617903}

* 0.0方案

{'MRR': 0.17780275302794976, 'MR': 332.29224078960226, 'HITS@1': 0.1034154206977426, 'HITS@3': 0.18716407700576565, 'HITS@10': 0.3373399785009284}

* -2方案

{'MRR': 0.29997343198650755, 'MR': 215.6084970194469, 'HITS@1': 0.20578031857715234, 'HITS@3': 0.3371933939216261, 'HITS@10': 0.4842177269617903}



1. 根据测试集生成负例
2. 正例负例同时送入前馈函数求打分
3. score += filter_bias
4. 求正例打分在所有正负例中的排行
5. 统计结果

#### 4-15

自从清明节之前想到用矩阵表示关系到现在两周毫无进展

##### 另辟蹊径，明天测试filter_bias的影响

仿照TransH+PairRE 设计的模型

训练20万步之后的结果

```
{'MRR': 0.14010179454943977, 'MR': 707.5311980846282, 'HITS@1': 0.08218508746213232, 'HITS@3': 0.1411120883416398, 'HITS@10': 0.26573341151177565}

```

这个结果说明这个方法是有训练潜力的，考虑怎么修改、优化模型，比如损失函数是否合理，打分函数是否合理

在次基础上再训练20万步

```
2022-04-15 15:00:04,569 INFO     Train positive_sample_loss at step 180000: 0.052227
2022-04-15 15:00:04,569 INFO     Train negative_sample_loss at step 180000: 0.052145
2022-04-15 15:00:04,569 INFO     Train loss at step 180000: 0.052186
2022-04-15 15:00:04,568 INFO     {'MRR': 0.14892060035998197, 'MR': 599.3613344739093, 'HITS@1': 0.0879954376960365, 'HITS@3': 0.1493584260051326, 'HITS@10': 0.2806387225548902}
2022-04-15 15:02:43,662 INFO     test MRR at step 199999: 0.145878
2022-04-15 15:02:43,662 INFO     test MR at step 199999: 606.968655
2022-04-15 15:02:43,662 INFO     test HITS@1 at step 199999: 0.085752
2022-04-15 15:02:43,662 INFO     test HITS@3 at step 199999: 0.145876
2022-04-15 15:02:43,662 INFO     test HITS@10 at step 199999: 0.276190
```

打分样式

```
[[52.6235, 36.2686, 39.0549,  ..., 36.6894, 47.0572, 44.1970],
        [31.2052, 26.8875, 32.4208,  ..., 31.4106, 28.5969, 29.9038],
        [25.7883, 38.7613, 33.2754,  ..., 27.1134, 28.2509, 35.0780],
        ...,
        [31.5359, 37.6847, 37.8647,  ..., 40.0903, 31.3638, 35.9607],
        [49.6850, 47.8009, 47.7147,  ..., 52.5379, 54.4053, 57.5641],
        [37.3791, 38.7685, 41.7006,  ..., 40.8949, 40.0877, 41.8958]],
       device='cuda:0', grad_fn=<NormBackward1>)
tensor([[20.8025],
        [19.8672],
        [21.8376],
        [17.6620],
        [18.9528],
        [21.0433],
        [21.3922],
        [21.7479],
        [15.8670],
        [20.8986],
        [21.8148],
        [15.5415],
        [18.6285],
        [21.4022],
        [22.2362]
        
```

可以看出正例打分普遍偏低，负例分布在30-50，正例分布在20左右

#### 4-17

正例打分排名详情

```python
rank_detail = dict()
rank_detail['head-batch'] = []
rank_detail['tail-batch'] = []

rank_detail[mode].append([positive_sample[i].tolist(),ranking.item()])
json_str = json.dumps(rank_detail, indent=4)

with open('rank_detail.json', 'a') as fin:
    fin.write(json_str)
```

根据结果看得出n-n关系确实影响正例排名，但分数差距大多在1以内，因此使用1作为filter_bias的值是合理的

#### 4-18

设置一个类似filter_bias的矩阵用来手动拉低假三元组的打分，

原始模型效果

```
{'MRR': 0.29770578137660036, 'MR': 204.9627186553308, 'HITS@1': 0.19935502785107007, 'HITS@3': 0.3363871787354637, 'HITS@10': 0.4916935405062054}
16万步


```

添加filter_score

**发现在训练时并没有生成假正例，也就是说在训练时生成的负样本都是训练集中不存在的三元组**，也就是说训练并没有针对一对多关系进行处理，而是单方面

之前的损失只考虑了negative_score,positive_score,现在添加一个考虑1-n关系的filter_entity专门用来存放那些与正例实体位置相同的实体，要求   scroe(**filter_entity**)比score(entity)小

重新生成代码

```python
def collate_fn(data):
    positive_sample = torch.stack([_[0] for _ in data], dim=0)
    negative_sample = torch.stack([_[1] for _ in data], dim=0)
    max = 0
    for filter in data:
        max = filter[2].size(0) if filter[2].size(0) > max else max
        filter_entity = torch.zeros(len(data),max)

        for i in range(len(data)):
            line = data[i][2]
            for col in range(len(line)):
                filter_entity[i][col] = line[col]
                mode = data[0][3]
                return positive_sample, negative_sample, filter_entity, mode

```

#### 4-19

新加filter_entity针对性训练后结果



两万步

```
2
{'MRR': 0.2833120894184372, 'MR': 257.48507280367437, 'HITS@1': 0.1927098602560344, 'HITS@3': 0.31757549105834065, 'HITS@10': 0.4613993941170722}
16
{'MRR': 0.3083353917006644, 'MR': 187.84984852926806, 'HITS@1': 0.2085409948206782, 'HITS@3': 0.3484804065278999, 'HITS@10': 0.5062054138571289}
18w
{'MRR': 0.30821611849426256, 'MR': 187.02435747092738, 'HITS@1': 0.20815010260920552, 'HITS@3': 0.34884686797615555, 'HITS@10': 0.5058633831720903}


```

##### 相同超参TransE模型

* learning_rate = 0.0001

* batch_size = 256

* hidden_dim  = 200

* gamma = 12.0

```
2
2022-04-19 12:43:44,067 INFO     {'MRR': 0.26080223434209077, 'MR': 311.493134955536, 'HITS@1': 0.17384931105247728, 'HITS@3': 0.29101925144141505, 'HITS@10': 0.4321802013094889}
4
2022-04-19 12:46:25,259 INFO     {'MRR': 0.28532514907390977, 'MR': 232.10380631290923, 'HITS@1': 0.19085312225153914, 'HITS@3': 0.31977425974787455, 'HITS@10': 0.4744209909117561}
6
2022-04-19 12:49:08,590 INFO     {'MRR': 0.2918359385955645, 'MR': 216.06332453825857, 'HITS@1': 0.19495749047200234, 'HITS@3': 0.3287647806117463, 'HITS@10': 0.48265415811589957}
8
2022-04-19 12:51:50,647 INFO     {'MRR': 0.2949314531092189, 'MR': 208.5749780123131, 'HITS@1': 0.19769373595231116, 'HITS@3': 0.3295709957979087, 'HITS@10': 0.49076517150395776}
10
{'MRR': 0.29494395164360465, 'MR': 204.95018567380046, 'HITS@1': 0.19759601289944298, 'HITS@3': 0.3308169647219779, 'HITS@10': 0.49137594058438383}

```

##### 最佳超参TransE模型

* learning_rate = 0.0001

* batch_size = 1024

* hidden_dim  =1000

* gamma = 12.0

  ```
  2022-04-19 13:11:26,398 INFO     {'MRR': 0.3060195800136327, 'MR': 178.97166031466824, 'HITS@1': 0.2038258575197889, 'HITS@3': 0.3462816378383661, 'HITS@10': 0.511946643213134}
  2022-04-19 13:20:42,236 INFO     {'MRR': 0.3068412552901708, 'MR': 177.48531711130656, 'HITS@1': 0.20429004202091272, 'HITS@3': 0.3457197302843741, 'HITS@10': 0.5126062738199941}
  
  ```

##### 最佳超参修改后模型

```
{'MRR': 0.31815919076014393, 'MR': 168.59623277631192, 'HITS@1': 0.2158946545490081, 'HITS@3': 0.35964526531808855, 'HITS@10': 0.5210348871298739}

{'MRR': 0.31791170469455077, 'MR': 169.31369099970684, 'HITS@1': 0.21601680836509332, 'HITS@3': 0.3586436040261898, 'HITS@10': 0.5190804260725105}


{'MRR': 0.3186289703412755, 'MR': 174.0854588097332, 'HITS@1': 0.22263754519691195, 'HITS@3': 0.3570067428906479, 'HITS@10': 0.50908824391674}
{'MRR': 0.32092076531217795, 'MR': 171.1786133098798, 'HITS@1': 0.22429883709567086, 'HITS@3': 0.35908335776409656, 'HITS@10': 0.5128994429785987}
{'MRR': 0.3206658885869227, 'MR': 168.35353757451384, 'HITS@1': 0.22293071435551648, 'HITS@3': 0.3581549887618489, 'HITS@10': 0.5165884882243721}
{'MRR': 0.3208306776118587, 'MR': 168.67265220365485, 'HITS@1': 0.22288185282908238, 'HITS@3': 0.3587413270790579, 'HITS@10': 0.5173214111208834}


```





##### 张量拼接

有一个短的一维张量，想要将它补全到指定长度并使用自身元素扩充

1 2 3    --------> 1 2 3 1 2 3 1 2

两种方法

1. numpy.tile()

```pyhton
# 将数组a扩充到长度length3
a = np.tile(a, length1 //length2 + 1)[:length3]

```

2. numpy.repeat()
3. torch.repeat()

##### 今日问题

对于1-n关系

head + relation = t1,t2,t3...

t1,t2,t3打分由相同的方式得到，总有高分低分，不管谁领先都对最后的平均排名影响不大。

所以要给打分函数引入别的变量，t1,t2,t3的打分不止依赖于head和relation

考虑让出现次数较多的实体获得更低的打分，这样如果它的排名能靠后，不至于拉低太多的实体的排名

1. 计算每个实体在训练集中的出现频率
2. 根据频率限制打分
3. 对于出现频率高的实体，使它的打分虚高，在调整的时候尽量将它的打分调低

坑：

整数转换tensor的错误方法

entity_count = torch.LongTensor(entity_count)

已经求出各个实体出现频率，怎么根据实体频率考虑实体的加权打分

## 4-20 

一个新的模型效果

```
Validation:
Number of data points: 35070
Hits @10: 0.5407185628742515
Hits @3: 0.39215854006273165
Hits @1: 0.2680068434559453
Mean rank: 156.19994297120044
Mean reciprocal rank: 0.3586653737639106
Test:
Number of data points: 40932
Hits @10: 0.5389914980944005
Hits @3: 0.38774064301768785
Hits @1: 0.2604319358936773
Mean rank: 164.51065181276263
Mean reciprocal rank: 0.35338669984208015
```

## 4-21

##### 实体出现频率权重对打分排名的影响

* 处理前效果

```
2022-04-21 19:18:00,057 INFO     {'MRR': 0.304350215523473, 'MR': 210.32106909019836, 'HITS@1': 0.20934721000684062, 'HITS@3': 0.340564839245578, 'HITS@10': 0.49423433988077786}

```

* 给出现频率高的实体加分

方法

```python
entity_count =(entity_count/float(entity_count.size(1))) + 1
score *= entity_count
```

效果

```
第一次
2022-04-21 19:12:27,736 INFO     {'MRR': 0.30457662596450275, 'MR': 210.57732336558195, 'HITS@1': 0.21003127137691782, 'HITS@3': 0.3404915469559269, 'HITS@10': 0.49352584774748365}
第二次
2022-04-21 19:15:28,087 INFO     {'MRR': 0.30457662596450275, 'MR': 210.57732336558195, 'HITS@1': 0.21003127137691782, 'HITS@3': 0.3404915469559269, 'HITS@10': 0.49352584774748365}
```

* sigmoid

2022-04-21 19:27:19,849 INFO     {'MRR': 0.3043170749883222, 'MR': 209.7684940877553, 'HITS@1': 0.20929834848040652, 'HITS@3': 0.340540408482361, 'HITS@10': 0.49423433988077786}



* 给出现次数多的实体扣分

```
2022-04-21 19:20:21,398 INFO     {'MRR': 0.3034287237919154, 'MR': 210.53579106811296, 'HITS@1': 0.20795465650346917, 'HITS@3': 0.3402716700869735, 'HITS@10': 0.49433206293364607}
```

## 4-22

* 跑通TuckRE代码

## 4-23

* 



## 4-27

* 测试RotatE训练好的样本，将测试集中打分超过正样本的实体存放进列表写入json文件
* 超过正样本的实体与正样本组成的三元组基本没在训练集中出现过

## 4-28

* 特征张量、特征池
* 构建一个entity_dim×entity_dim×relation_dim的三维张量，张量中存放相应位置特征的交互程度
* 两种想法：
* * 在训练实体嵌入、关系嵌入时从特征池中选取相应的特征组成其向量
  * 将实体、关系嵌入投影到相应的平面

# 4-29 TuckER复现

## dataloader

### 函数

#### load_data

形参：

| 参数      | 参数类型 | 解释                                                         |
| --------- | -------- | ------------------------------------------------------------ |
| data_dir  | str      | 文件路径                                                     |
| data_type | str      | train test valid 对应训练集、测试集、验证集                  |
| reverse   | bool     | 是否添加逆置三元组，逆置三元组调转头尾实体位置并在关系尾部拼接_reverse |

返回值

* 类型：列表
* 元素：数据集中的三元组，三元组也以列表的形式存储，实体、关系以字符串存储

#### get_relations

* 参数：load_data返回的三元组列表

* 返回值：有序关系列表

#### get_entities

* 参数：load_data返回的三元组列表

* ```

* 返回值：有序实体列表

## model

### 函数

#### _init_

* [torch](https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020).nn.Embedding:在使用[pytorch](https://so.csdn.net/so/search?q=pytorch&spm=1001.2101.3001.7020)进行词嵌入使用torch.nn.Embedding（）就可以做到

```
self.E = torch.nn.Embedding(len(d.entities), d1)
```

* torch.nn.Parameter: 默认可被训练的参数
* 





#### torch.nn.Dropout

**torch.nn.Dropout(p=0.5, inplace=False)**

其作用是，在 training 模式下，基于伯努利分布抽样，以概率 p 对张量 input 的值随机置0；

training 模式中，对输出以 1/(1-p) 进行 scaling，而 evaluation 模式中，使用恒等函数；

p：默认 0.5，张量元素被置0的概率；

inplace：默认 False，是否原地执行；

self.dropout_layer = torch.nn.Dropout()

#### torch.nn.BCELoss

交叉熵损失

![img](https://img-blog.csdnimg.cn/fd61564f804241b486e2569e103c8a29.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5rKZ5a2Q5piv5rKZ5a2Q,size_20,color_FFFFFF,t_70,g_se,x_16)



### 4-30 张量乘法

```python
# 特征池张量
w = torch.rand(8,4,4) 

head = torch.range(1,5,1)
tail = torch.range(1,5,1)
relation = torch.range(1,8,1)

head_new = w *head
tail_new = w.transpose(1,-1) * tail
relation_new = w.transpose(0,2) * relation
```

#### forward函数

```python
def forward(self, e1_idx, r_idx):
    
    e1 = self.E(e1_idx) # 一批头实体  batch_size * entity_dim
    x = self.bn0(e1)     # 归一化  batch_size * entity_dim
    x = self.input_dropout(x)   # 以概率 p 对x随机置0   batch_size * entity_dim
    x = x.view(-1, 1, e1.size(1)) # 变换x形状 batch_size * 1 * entity_dim


    r = self.R(r_idx) # 一批关系  batch_size *relation_dim

    W_mat = torch.mm(r, self.W.view(r.size(1), -1)) # 关系向量与核张量相乘，batch_size * (e_d*e_d)
    # W.view(r.size(1), -1)：将1,2轴对应的矩阵展开成一维，三维张量变为二维，形状为ralation_dim *(e_d*e_d)
    W_mat = W_mat.view(-1, e1.size(1), e1.size(1))  # 将张量重新展开为三维 batch_size * e_d * e_d
    W_mat = self.hidden_dropout1(W_mat) # 对张量随机填充0

    x = torch.bmm(x, W_mat)  # 三维张量中的第1、2维组成的矩阵相乘 batch_size * 1 * entity_dim
    x = x.view(-1, e1.size(1))    # 三维压缩成二维   batch_size * entity_dim  
    x = self.bn1(x)     # 归一化
    x = self.hidden_dropout2(x) # 随机填充 0
    x = torch.mm(x, self.E.weight.transpose(1,0))  #批数据与转置实体矩阵相乘 batch_size * nentity
    pred = torch.sigmoid(x) # 西格玛激活 转变为nentity分类  求交叉熵损失
    return pred



```





##  负采样

损失计算方法不科学

l  =  (positive_score + negative_score)/2

由于正负样本数量未必相同，直接采用这种算术平均的方法未必合适

修改为样本数量加权平均和

sum = |positive_samples + negative_samples |

l = positive_score * (positive_samples/sum)  + negative_score * (|negative_samples|/sum)

```python
loss = (positive_sample_loss + negative_sample_loss)/2
```

改为

```python
loss = positive_sample_loss * 1.0/ + negative_sample_loss)/2
```

* * 



# FB15k

###  transE

* 2022-04-20 14:03:12,344 INFO     {'MRR': 0.5578831662289234, 'MR': 78.79085337983105, 'HITS@1': 0.43016031555247075, 'HITS@3': 0.6478982918860354, 'HITS@10': 0.7700479084491544}    200d  256bs

#### 4-20

* Rotate最佳参数

```
Valid MRR at step 50000: 0.744018
2022-04-20 21:47:11 INFO     Valid MR at step 50000: 48.679470
2022-04-20 21:47:11 INFO     Valid HITS@1 at step 50000: 0.680830
2022-04-20 21:47:11 INFO     Valid HITS@3 at step 50000: 0.783640
2022-04-20 21:47:11 INFO     Valid HITS@10 at step 50000: 0.855150

```



# GOT数据集

| Model         | 结束时损失 | MRR                | Hits@1   | Hits@3   | Hits@10    | epoch | dim  | lr     |
| ------------- | ---------- | ------------------ | -------- | -------- | ---------- | ----- | ---- | ------ |
| TransE(tc181) | 0.014485   | 0.278463           | 0.193638 | 0.322268 | 0.43637621 | 10000 | 400  | 0.0001 |
| TransE(ampli) | 0.024855   | 0.25               | 0.33     | 0.27     | 0.20       | 2000  | 150  | 0.0001 |
| TransE(tc181) | 0.010470   | 0.3895(过拟合？)   | 0.267    | 0.463    | 0.599      | 10000 | 200  | 0.0001 |
| TransE(tc181) | 0.0152583  | 0.393020(过拟合？) | 0.273    | 0.467    | 0.61       | 10000 | 200  | 0.0001 |
| PairRE(tc181) | 0.0029190  | 0.3747             | 0.299    | 0.406    | 0.515      | 10000 | 200  | 0.001  |

{'MRR': 0.3932411154840647, 'MR': 141.306, 'HITS@1': 0.275, 'HITS@3': 0.462, 'HITS@10': 0.598}

##  pairRE复现

将PairRE中的打分函数用在TransE中查看效果

涉及问题：torch到tensorflow的改变



```python
# torch版本中PairRE的打分函数
def PairRE(self, head, relation, tail, mode):
        re_head, re_tail = torch.chunk(relation, 2, dim=2)
        
        head = F.normalize(head, 2, -1)
        tail = F.normalize(tail, 2, -1)

        score = head * re_head - tail * re_tail
        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
        return score
    
    
# tensorflow版TransE的打分函数    
def _fn(self, e_s, e_p, e_o):


    return tf.negative(
            tf.norm(e_s + e_p - e_o, ord=self.embedding_model_params.get('norm', constants.DEFAULT_NORM_TRANSE),
                    axis=1))
```

### torch.chunk

```
torch.chunk(tensor, chunks, dim=0)
```

在给定维度(轴)上将输入张量进行分块儿。

参数:

- tensor (Tensor) – 待分块的输入张量
- chunks (int) – 分块的个数
- dim (int) – 沿着此维度进行分块

###  tensortflow.chunk

与torch.chunk效果相同

## 评估函数

* 选取正例
* 生成负例

最后计算的排名并不是所有实体的打分排名，而是选取一定数量（128,256.。。。）的实体，计算正确答案在这些实体中的排名

# 知识点

## 学习率

**1) 要理解学习率是什么, 首先得弄明白神经网络参数更新的机制 - “梯度下降+反向传播”.**

总结一句话: 将输出误差反向传播给网络参数, 以此来拟合样本的输出. 本质上是最优化的一个过程, 逐步趋向于最优解. 

但是每一次更新参数利用多少误差, 就需要通过一个参数来控制, 这个参数就是学习率 (Learning rate), 也称为步长.

**2) 学习率对模型的影响**

学习率越大, 输出误差对参数的影响就越大, 参数更新的就越快, 但同时受到异常数据的影响也就越大, 很容易发散.

学习率 (Learning rate，η) 作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。

合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。

运用梯度下降算法进行优化时，权重的更新规则中，在梯度项前会乘以一个系数，这个系数就叫学习速率 α。

学习率是指导我们，在梯度下降法中，如何使用损失函数的梯度调整网络权重的超参数。

```python
new_weight = old_weight - learning_rate * gradient
```

### 学习率对损失值甚至深度网络的影响？

- 学习率如果过大，可能会使损失函数直接越过全局最优点，容易发生梯度爆炸，loss 振动幅度较大，模型难以收敛。
- 学习率如果过小，损失函数的变化速度很慢，容易过拟合。会大大增加网络的收敛复杂度. 虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在局部最优点的时候。

### 学习率的作用

学习率 (learning rate)，控制 模型的 学习进度.

由以上可以看出，为深度网络选择一个良好的学习率更新策略，可以抽象为以下两点好处：

- 更快地达到 loss 的最小值
- 保证收敛的 loss 值是神经网络的全局最优解

### 学习率设置

最理想的学习率不是固定值, 而是一个随着训练次数衰减的变化的值, 也就是在训练初期, 学习率比较大, 随着训练的进行, 学习率不断减小, 直到模型收敛.

在训练过程中，一般根据训练轮数设置动态变化的学习率。

- 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。
- 一定轮数过后：逐渐减缓。
- 接近训练结束：学习速率的衰减应该在 100 倍以上。

现阶段研究中，共同认同的学习率设置标准为：首先设置一个较大的学习率，使网络的损失值快速下降，然后随着迭代次数的增加一点点减少学习率，防止越过全局最优解。

那么我们现在面临两个两个问题：

- 如何选取初始的学习率

- - 大多数的网络的学习率的初始值设置为 0.01 和 0.001 为宜
  - 较为科学的设置方法: 首先设置一个十分小的学习率，在每个 epoch 之后增大学习率，并记录好每个 epoch 的 loss 或者 acc，迭代的 epoch 越多，那被检验的学习率就越多，最后将不同学习率对应的 loss 或 acc 进行对比。

- 如何根据迭代次数更新学习率（即衰减学习率策略）

### 学习率大小

|            | 学习率 大                  | 学习率 小                  |
| :--------- | :------------------------- | :------------------------- |
| 学习速度   | 快                         | 慢                         |
| 使用时间点 | 刚开始训练时               | 一定轮数过后               |
| 副作用     | 1.易损失值爆炸；2.易振荡。 | 1.易过拟合；2.收敛速度慢。 |
